BEGIN;

-- Inserts for user table
INSERT INTO "user" ("id", "name") VALUES ('alexhking', 'Alexander King');
INSERT INTO "user" ("id", "name") VALUES ('arv', 'Erik Arvidsson');
INSERT INTO "user" ("id", "name") VALUES ('aboodman', 'Aaron Boodman');
INSERT INTO "user" ("id", "name") VALUES ('tristanls', 'Tristan Slominski');
INSERT INTO "user" ("id", "name") VALUES ('cesara', 'Cesar Alaestante');
INSERT INTO "user" ("id", "name") VALUES ('tantaman', 'Matt Wonlaw');
INSERT INTO "user" ("id", "name") VALUES ('arielpollack', 'Ariel Pollack');
INSERT INTO "user" ("id", "name") VALUES ('ocooper', 'Ofir Cooper');
INSERT INTO "user" ("id", "name") VALUES ('grgbkr', 'Greg Baker');
INSERT INTO "user" ("id", "name") VALUES ('sboodman', 'Susan Boodman');
INSERT INTO "user" ("id", "name") VALUES ('noamackerman', 'noamackerman');
INSERT INTO "user" ("id", "name") VALUES ('tom-ka', 'tom katz');
INSERT INTO "user" ("id", "name") VALUES ('darkgnotic', 'Darick Tong');
INSERT INTO "user" ("id", "name") VALUES ('rocicorp-reflect-services', 'rocicorp-reflect-services');
INSERT INTO "user" ("id", "name") VALUES ('jesseditson', 'Jesse Ditson');
INSERT INTO "user" ("id", "name") VALUES ('phritz', 'Fritz Schneider');
INSERT INTO "user" ("id", "name") VALUES ('ingar', 'Ingar Shu');
INSERT INTO "user" ("id", "name") VALUES ('jamischarles', 'Jamis Charles');
INSERT INTO "user" ("id", "name") VALUES ('linear[bot]', 'Linear Bot');
INSERT INTO "user" ("id", "name") VALUES ('pcorpet', 'Pascal Corpet');
INSERT INTO "user" ("id", "name") VALUES ('KeKs0r', 'Marc HÃ¶ffl');


-- Inserts for issue table
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('AZKelPxNxWMM8Tes9AUSr', 'Sort and remove duplicates from refs', TRUE, 1722450720000.0, 1722450712000.0, 'arv', 'We have duplicates in the `c/<HASH>/m` keys in the kv store. These keys are also not sorted.

Removing the duplicates makes the data slightly smaller and we have less work to do even though we do cache things so we do not end up going to IDB twice.

Sorting could also potentially help with the lookup speed if IDB uses a cursor internally.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('QkF2LYzuk8cu9CdFKm2th', 'UnknownError: Internal error opening backing store for indexedDB.open.', TRUE, 1722332514000.0, 1722332514000.0, 'arv', 'https://discord.com/channels/830183651022471199/1267770276972593162

This was a corrupted chrome user profile

But maybe we should expose these kind of errors on the Replicache instance so that the app can tell the user what is going on.

We could also fallback to MemStore in this case.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('1pMSyLCAy9OEz-khbYcIE', 'Presence keys not removed when room is invalidated', TRUE, 1719261305000.0, 1719261305000.0, 'arv', '> another issue I''ve found with the presence key - when I invalidate the room using connections/rooms:invalidate the presence keys of the disconnected users are still there

Seems like we need to go through the REST endpoints and see how they impact the presence keys.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('1MHR9pRvoHHlJ9V8k3RXS', 'Buffer tail messages during authHandler and roomStartHandler', TRUE, 1718613057000.0, 1718613057000.0, 'arv', 'We do not currently have a way for the `reflect tail` log to receive console log messages during the authHandler and roomStartHandler (I believe)

https://discord.com/channels/830183651022471199/1020392595450507304/1251704780196151317

I think we would need to buffer the `console.log` calls in auth and room start handler to support these to be sent to the tail client.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('wEC9GXLIWa57UKHNFnUyj', 'TODO: Add unit tests for statement', TRUE, 1716997963000.0, 1716997960000.0, 'arv', '              TODO: Add unit tests for statement

_Originally posted by @arv in https://github.com/rocicorp/mono/pull/1959#discussion_r1619128486_
            ', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('ozDbAz2yztkwQFjKtX1YX', 'zql: Limit needs to pull more data', TRUE, 1716811440000.0, 1716811440000.0, 'arv', 'When we have a query with a limit. If a row is removed or changed so that it no longer is in the view we should pull for more data from the source.

#1866 has tests for this case and these tests fail

The problem is that we do not know that we changed the min/max until we are in the commit phase and pullHistoricalData does not work in the commit phase.

', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('kIClj-c-FMalyG1_1jU6E', 'ZQL: Write test for remove all', TRUE, 1715968725000.0, 1715968664000.0, 'arv', 'I should really have written a test for this... TODO

_Originally posted by @arv in https://github.com/rocicorp/mono/pull/1848#issuecomment-2117936187_', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('BqHDX7sPYX2E-m-JRyuGA', 'View Syncer b0rked with invalid snapshot identifier', FALSE, 1715906651000.0, 1715877820000.0, 'darkgnotic', '<img width="2503" alt="Screenshot 2024-05-16 at 09 41 53" src="https://github.com/rocicorp/mono/assets/132324914/8c161d0a-3dd5-4b59-adb9-cbba6c9a43c1">

I bet this has to do with the TransactionPool caching. I''ll revert that to get things working and study the logs post-mortem.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('fSzfZOsgffkU8XWuYie6g', 'Zeppliear: Exception because issue missing properties', TRUE, 1715699188000.0, 1715676220000.0, 'arv', 'To reproduce scroll down in Zeppliear.

![image](https://github.com/rocicorp/mono/assets/45845/5b80ca74-1600-438b-b40d-18e7f9f35d7c)

`row` is 

```
{
    "id": "_0kcprVNTV",
    "issue": {
        "id": "_0kcprVNTV",
        "kanbanOrder": "0"
    },
    "labels": []
}
```

but the type of `row` is supposed to be `{issue: Issue; labels: string[]};`', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('EhVaY0SmM1SFxLt43Xq6l', 'Zeppliear: limit is broken ', FALSE, 1716811486000.0, 1715675567000.0, 'arv', 'I was hitting this when testing Zeppliear

In Zeppliear we have a limit of 200 but we end up with a case where we get to `#limitedAddAll` where  the size of the BTree is 201 (changing the limit to 10 hits a case where the data.size is 11):

https://github.com/rocicorp/mono/blob/baff399d78101ac77813803516b06a0c84fa4209/packages/zql/src/zql/ivm/view/tree-view.ts#L142-L144


I did some debugging and the problem seems to be that the we call tree `set` without going through the _limit function_ so the tree size is larger than we expect.

https://github.com/rocicorp/mono/blob/baff399d78101ac77813803516b06a0c84fa4209/packages/zql/src/zql/ivm/source/set-source.ts#L173-L175
', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('KAJMDvkPkNX7ssqR6ijhZ', 'Type Generation for Client API', TRUE, 1715671242000.0, 1715671153000.0, 'aboodman', NULL, NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('l4d3GWQ4feLSHJ3MHEoM2', 'Auth', TRUE, 1715671253000.0, 1715671048000.0, 'aboodman', 'Right now we have a few paragraphs of text and a code block. We need to design and implement both authentication and authorization.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('TEV8ldKCbizgc7IAJurcD', 'test', FALSE, 1715671278000.0, 1715670370000.0, 'aboodman', 'test', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('2oVDh-xJvsIX5UFda_Qa2', 'AST `field` should be split into `table` and `column` parts', FALSE, 1716564468000.0, 1715198543000.0, 'tantaman', 'All `fields` should be tuples of `table`, `column` -- https://github.com/rocicorp/mono/blob/main/packages/zql/src/zql/ast/ast.ts#L19

Right now we encode table + column via a dot in the string.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('ddfkR2NYg3cpPkOYQNtRD', 'Implement sub-queries / move to north star query API', TRUE, 1715196481000.0, 1715196474000.0, 'tantaman', 'To get us to our aspirational API and remove the awkward:

```
issue
  .join(issueLabel)
  .join(label)
  .groupBy(issue.id)
  .select(issue.*, agg.array(label))
```

to:

```
issue.select(issue.*, nest(''labels'', q => q.queryLabels()))
```

- Requires #1779 for the nice `q.queryLabels` sort of syntax', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('BXsJDhkKdYHo7huZgUSlY', 'Simplify `join` by using client side schema information', TRUE, 1715196272000.0, 1715196272000.0, 'tantaman', '- #1775 

Once we know available foreign keys the `Join` API can go from:

```
issue.join(issueLabel, ''issueLabel'', ''id'', ''issueID'')
```

to:

```
issue.join(issueLabel)
```', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('IYf9vNEklpPOgj-tG9FnX', 'AST and QueryBuilder type divergence', TRUE, 1715196166000.0, 1715196166000.0, 'tantaman', '- where conditions definitely have duplicative structures
- order by too
- others?', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('clYs8ovjAJjlVSuIGKlL3', 'typescript types for `join` selectors on the right table are incorrect', TRUE, 1716564397000.0, 1715192998000.0, 'tantaman', 'If you alias a table that you join, TypeScript tells you that the selector in un-aliased. This is wrong. The selector needs to use the alias name.

```ts
issue.join(label, ''stuff'', ''id'', ''label.id'')
```

''label.id'' should be ''stuff.id''', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('x2N4dx34-OrrxDScWpkI0', 'Need schema information on the client', TRUE, 1716564478000.0, 1715192910000.0, 'tantaman', 'for:

1. compound primary keys
2. #1700 
3. better ergonomics for foreign key traversal', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('6l1zLfrROTr1NdeF-L_wl', 'Zeppliear: Back button no longer works', TRUE, 1715085530000.0, 1715085530000.0, 'arv', 'We used to be able to go back and forth using the browser back and forward buttons', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('69hisURAjfAph8bLIhLTl', 'Zero/Reflect: Consider using websocket ping frames?', FALSE, 1715066131000.0, 1714987020000.0, 'arv', 'The websocket protocol has built in ping support.

https://developer.mozilla.org/en-US/docs/Web/API/WebSockets_API/Writing_WebSocket_servers#pings_and_pongs_the_heartbeat_of_websockets

https://www.rfc-editor.org/rfc/rfc6455

Do we really need application level support for ping/pong?', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('cXM1rl-5nWvpAmqR2oTU5', 'ZQL: need to ignore incomplete rows', FALSE, 1718868285000.0, 1714601979000.0, 'aboodman', 'Say we have a row cached with columns {a, b, c}. If we select the same row with columns {a, b, c, d}, we shouldn''t return the cached row.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('qFXKhttg4GG6UDqNs2Enf', 'ZQL - double check operators are all lazy where possible', TRUE, 1715649261000.0, 1714484962000.0, 'tantaman', 'map & filter & the newest distinct operator are lazy & 0-copy in terms of making new collections. All operators should be lazy if possible.

- can join be more lazy?
- reduce?
- agg & full agg operators?', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('6AfgoH5ICMJhe8g7Q93Nc', '[notes] ZQL Join & Sub-query Optimization', TRUE, 1714760950000.0, 1714155182000.0, 'tantaman', '- If/when we need to optimize joins, there are some thoughts here in the description: https://github.com/rocicorp/mono/pull/1644

One item not mentioned there is that we can also re-order where `join` occurs in the data flow graph in order to:
1. Compute a join once and share it among all queries
2. Constrain a join to a subset of the table being joined

These same optimizations apply to correlated sub-queries.

An interesting little bit about sub-queries is that if the sub-query is not used in a `where` or `order-by`, we can completely skip evaluating it until the limited view returns it.

E.g.,
```ts
issue.select(user.select..., title, id).limit(10)
```

We can wait to evaluate the `user.select` until we know the final 10 rows being returned.


related docs:
- https://www.notion.so/replicache/No-Memory-IVM-4ff5c1746fc14cf6a4a8b283cfe48e0e
- https://www.notion.so/replicache/Sub-Query-Implementation-Options-50ecab0b78b44c7aa04114d440e5a046', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('zA09ylXAJ5afxFeiwwXWF', 'zql devx issues', TRUE, 1714157742000.0, 1714130250000.0, 'tantaman', '1. The developer having to provide `FromSet` is awkward. Can this just be a list of `Entity` types instead? E.g., `EntityQuery<FromSet>` -> `EntityQuery<...Entities>`
2. join changing the nesting level of the results of a query could be problematic. E.g., from `Issue[]` to `{issue: Issue, label: Label}[]` See commit: https://github.com/rocicorp/mono/pull/1640/commits/ac0deea5b64f3a4c50565c1ad11285eabaebf7a7 where the query is changed to join labels and the impact ripples through the app. The commit comment documents a few options.
3. Select changing the return type of a query makes it impossible to re-assign to the same variable after applying select. `q = query; q = q.select(''foo'') // type error`
4. `Return` being on a type param in the EntityQuery makes it hard to take `EntityQuery` as an arg to a function and error prone when defining filters.
   1. Specifically, if someone fumbles `Return` and/or `FromSet` then they will write unqualified `where` when the `where` requires qualification. The root of the problem is that `{a: any}` is wider than `{a: any, b: any}` and we allow unqalified `where` if the `FromSet` only has a single prop.
5. Because `join` changes generics on the query, having a function that updates the query with new `HAVING` filters (and no selects) can produce an incompatible query. Specifically: https://github.com/rocicorp/mono/pull/1640/commits/183979c6cea435e11d4ab9fca977b412c312fa0c#diff-a275f49b14e8e300928bee76064f80e20e5cba32e36975fda20caa65a395075cR383
   1. We can remove most of this problem by introducing `whereExists`
   2. Maybe a non-joined query should be wider than a joined query in a type sense? So the one with joins can be assigned to the one without?', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('M3hzZU1ymOTIVJRRrOiep', '`where` applied prior to `join` does not set a qualified selector in the `ast`', FALSE, 1714429076000.0, 1714066521000.0, 'tantaman', 'Zeppliear does:

```ts
query = issue.where(''priority'', ''in'', x);
```

then

```
query.join(''issueLabels'', ...)
```

but the first `where` doesn''t write itself into the AST as a qualified selector. This means that after the `join` we don''t know what to compare against.

We should always use fully qualified selectors under the hood, even if the user does not provide it.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('5FhK0qX7Gvsh24jGA0EtP', 'zql subscribe callback called with wrong arguments', TRUE, 1714034881000.0, 1714034881000.0, 'arv', 'If I do 


```js
query.subscribe((...args) => {
  console.log(...args);
})
```

It will log the value but also the internal zql tx version.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('QkIQSSr1UMxdQ1XvCXRLL', 'Zeppliear `count` & `reduce` are incorrect by a constant factor', FALSE, 1715193127000.0, 1714004632000.0, 'tantaman', 'For some reason the Zeppliear issue count is sometimes 2 and sometimes 3 times larger than expected ðŸ¤”
', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('Hevl4rWrSKe8t4P99CDZn', 'ZQL: Optimization in SetSource?', FALSE, 1713202888000.0, 1713187276000.0, 'arv', 'We have the following optimization:

```ts
      onCommitEnqueue: (version: Version) => {
        for (let i = 0; i < this.#pending.length; i++) {
          const [val, mult] = must(this.#pending[i]);
          // small optimization to reduce operations for replace
          if (i + 1 < this.#pending.length) {
            const [nextVal, nextMult] = must(this.#pending[i + 1]);
            if (
              Math.abs(mult) === 1 &&
              mult === -nextMult &&
              comparator(val, nextVal) === 0
            ) {
              // The tree doesn''t allow dupes -- so this is a replace.
              this.#tree = this.#tree.add(nextMult > 0 ? nextVal : val);
              ++i;
              continue;
            }
          }
          if (mult < 0) {
            this.#tree = this.#tree.delete(val);
          } else if (mult > 0) {
            this.#tree = this.#tree.add(val);
          }
        }

        this.#stream.newDifference(version, this.#pending);
        this.#pending = [];
      },
```

However, how does the old value get deleted from the Set?', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('8MRH9mJz-XDup7tfOKpZm', 'rolling back to previous version of reflect causes rooms to break ', TRUE, 1712015712000.0, 1712015712000.0, 'cesara', 'if a user installs a version of Reflect past "0.39.202402230127" and rolls back to "0.39.202402230127" their rooms will encounter this issue.


```text
[Unhandled exception in #processNext, {stack=TypeError: Unexpected property deleted
    at parse (index.js:5388:11)
    at validateOrNormalize (index.js:7183:20)
    at listEntries (index.js:7178:10)
    at async listClientRecords (index.js:8004:19)
    at async fastForwardRoom (index.js:8130:29)
    at async processRoom (index.js:9071:23)
    at async processPending (index.js:9217:27)
    at async #processNextInLock (index.js:10781:65)
    at async index.js:10770:11
    at async index.js:9436:16, name=TypeError, message=Unexpected property deleted}]
``` ', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('OmxMund3wGkn22MYzeFrE', 'Cloudflare returning `null` from DNSRecords.list()', TRUE, 1711992914000.0, 1711992914000.0, 'darkgnotic', 'We''re seeing an error in our `setCustomHostnames()` method for both publishes and deletes.


https://console.cloud.google.com/monitoring/alerting/incidents/0.nb7hvre49xxg?channelType=slack&project=reflect-mirror-prod


```
TypeError: Cannot read properties of null (reading ''forEach'')
    at setCustomHostnames (file:///workspace/out/index.js:14639:18)
    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)
    at async publishCustomHostname (file:///workspace/out/index.js:14616:20)
    at async NamespacedScriptHandler._doPublish (file:///workspace/out/index.js:15259:22)
    at async NamespacedScriptHandler.publish (file:///workspace/out/index.js:15190:22)
    at async runDeployment (file:///workspace/out/index.js:15487:22)
    at async file:///workspace/out/index.js:15393:5
```

```
TypeError: Cannot read properties of null (reading ''forEach'')
    at setCustomHostnames (file:///workspace/out/index.js:14639:18)
    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)
    at async deleteCustomHostnames (file:///workspace/out/index.js:14621:20)
    at async NamespacedScriptHandler.delete (file:///workspace/out/index.js:15271:7)
    at async runDeployment (file:///workspace/out/index.js:15465:7)
    at async file:///workspace/out/index.js:15393:5
```



In the [code](https://github.com/rocicorp/mono/blob/c6020d36b62935fec5459d06e969d44590d09143/mirror/mirror-server/src/cloudflare/publish-custom-hostnames.ts#L34), the only time the target of "forEach" can be null is when it is called on the return value of DNSRecords.list():

<img width="611" alt="Screenshot 2024-04-01 at 10 22 38" src="https://github.com/rocicorp/mono/assets/132324914/d395c254-2fc0-4d8b-82cb-bd6dbc6f6782">

And indeed the errors always follow a call to `GET /zones/1b044253688b6ddb8e67738539a2b6d0/dns_records`

<img width="1202" alt="Screenshot 2024-04-01 at 10 24 53" src="https://github.com/rocicorp/mono/assets/132324914/8dff045c-e99a-49bc-b0c0-c956c56acef5">

So evidence points to the GET DNSRecords API call returning `null` (presumably instead of an empty list). 

I can''t yet tell whether this is new behavior that we have to adjust to, or a bug that they''ve since rolled back. I will keep an eye on this.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('tx8zFWtt1fTUaUXehja7C', 'frequent error in logs around WebSocket send() after close()', TRUE, 1710962933000.0, 1710962932000.0, 'cesara', '`
[Unhandled exception in handleMessage, {stack=TypeError: Can''t call WebSocket send() after close().     at sendErrorInternal (reflect-server.js:3351:6)     at closeWithErrorInternal (reflect-server.js:3334:3)     at closeWithError (reflect-server.js:3331:3)     at handleMessage (reflect-server.js:5920:5)     at reflect-server.js:6649:15     at reflect-server.js:5368:22     at run (reflect-server.js:1179:18)     at async LoggingLock.withLock (reflect-server.js:5357:20)     at async #handleMessageInner (reflect-server.js:6648:7), name=TypeError, message=Can''t call WebSocket send() after close().}]
`


[datadog url](https://app.datadoghq.com/logs?query=-source%3A%28heroku%20OR%20client%20OR%20cloudflare%29%20status%3Aerror&agg_q=status%2Cservice&agg_q_source=base%2Cbase&cols=host%2Cservice&fromUser=true&index=%2A&messageDisplay=inline&refresh_mode=sliding&sort_m=%2C&sort_t=%2C&storage=hot&stream_sort=desc&top_n=10%2C10&top_o=top%2Ctop&viz=pattern&x_missing=true%2Ctrue&from_ts=1710876290597&to_ts=1710962690597&live=true)', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('fXWBarBsdlyZPdGy6jneN', 'Simplify IDB handling for Firefox 115', TRUE, 1710489884000.0, 1710488861000.0, 'arv', 'https://www.mozilla.org/en-US/firefox/115.0/releasenotes/

> [IndexedDB](https://w3c.github.io/IndexedDB/) is now also supported in [private browsing](https://bugzilla.mozilla.org/show_bug.cgi?id=1639542) without memory limits thanks to encrypted storage on disk. The temporary keys to decrypt the information are held in RAM only and all stored information is purged at the normal end of a private browsing session from disk.

They way Replicache deals with this is that it catches an exception and switches to an in memory store. With Firefox 115 this exception is no longer triggered. This means that we already use IDB in Firefox private browsing but there is room to simplify the code to remove this fallback.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('BiD4rZEcAwBRdDia8sVcN', 'Watch with empty result not working as intended?', FALSE, 1710452110000.0, 1710447788000.0, 'arv', 'Looks like watch has logic for this case, but evidently not working as intended
https://github.com/rocicorp/mono/blob/4aa964038da9aff98ba0d8a43d5032fcadb072ac/packages/replicache/src/subscriptions.ts#L267
cc @arv

_Originally posted by @grgbkr in https://github.com/rocicorp/rails/pull/33#discussion_r1525273482_
            ', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('5KQHlQsv-6j1BOXjh80ft', 'Rename authHandler and roomStartHandler', TRUE, 1708950746000.0, 1708948359000.0, 'arv', 'We should rename these to `onAuth` and `onRoomStart`.

Regarding naming for callbacks we still have two of the ''Handler'' suffix style (`authHandler` and `roomStartHandler`).   What is your thinking on when a callback option should have the ''Handler'' suffix vs the ''on'' prefix?

_Originally posted by @grgbkr in https://github.com/rocicorp/mono/issues/1431#issuecomment-1954824651_
            ', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('F0B_SJmTcXhGgGY9hZvk0', 'Fix update version in disconnect handler etc', TRUE, 1708940240000.0, 1708940235000.0, 'arv', 'I''m finding the version updating a bit hard to follow.  

I believe there are some cases where collectClientIfDeleted may modify the user values but version does not get updated.

From my reading collectClientIfDeleted, does not itself call putVersion, and prior to this patch relied on its caller to call putVersion.  It does now call callCloseHandler which sometimes calls putVersion, but only if the closeHandler actually modified user values.

So in the case that the above disconnectHandler throws an exception (and thus does not putVersion), the closeHandler does not modify user values, and  collectOldUserSpaceClientKeys is run and deletes some user values, I believe we end up with user values modified but version not updated.

_Originally posted by @grgbkr in https://github.com/rocicorp/mono/pull/1418#discussion_r1496149854_
            ', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('tfrqvYwlGDe4LVR0BvS63', 'Check for client tombstone in connect', TRUE, 1709537138000.0, 1708938736000.0, 'arv', 'That reminds me. I don''t think that check is in place.

_Originally posted by @arv in https://github.com/rocicorp/mono/pull/1420#discussion_r1485232520_
            ', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('X1SpFrTBwT5wTa4_x6LNL', 'Add comment to code', TRUE, 1709537152000.0, 1708938663000.0, 'arv', 'We should add something to the code that covers this:


Regarding 

"Another gotcha here is that when we process disconnect we collect the client even if lastMutationId is larger than lastMutationIdAtClose. Intuitively this seems correct but this could use some more thought."

I think this is correct.  I think the initial assumption was that there can be no mutations from the client with a lastMutationID larger than the one sent it sent in the close beacon, but then you discovered that is not quite true, a client can sneak mutations over the websocket as the page is unloading. If it manages to do this its still correct to collect it when it disconnect.

What would be problematic is if the client:
1. manages to execute mutations after the close beacon
2. does not manage to send them over the websocket
3. but does manage to persist them to idb (via persists)

In this case the server may collect the client, but the client group may later push mutations from that client.  I think the client group might get wedged in this case.  Do you think this is possible?

_Originally posted by @grgbkr in https://github.com/rocicorp/mono/issues/1407#issuecomment-1936865370_
            ', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('PlfYFT5Cf3igeiSPgvlQ9', 'Better message when calling `npx reflect keys` for users with no apps?', FALSE, 1709681779000.0, 1708746279000.0, 'darkgnotic', 'The error alerts have reported a cli error that we''re displaying when a user with no apps (and thus no teams) calls `npx reflect keys`:

```
Error: You are not an admin of any teams
```

Note that we''ll also display this message for `npx reflect apps list` and anything else that looks up the team.

Not an urgent priority, but perhaps we can tailor the messages better.

@aboodman ?

<img width="1262" alt="Screenshot 2024-02-23 at 19 41 18" src="https://github.com/rocicorp/mono/assets/132324914/093abf3e-916e-4c97-bc6c-2a5097fddd78">
', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('qo-ny8b0TQ4Q8HMzngZ5E', 'DNS Validation Error from Cloudflare', FALSE, 1708746207000.0, 1708635824000.0, 'darkgnotic', 'Need to look into when this happens.

@aboodman 

<img width="1927" alt="Screenshot 2024-02-22 at 13 02 41" src="https://github.com/rocicorp/mono/assets/132324914/f7c1b6da-6432-4dec-92e9-cedb4fe24a2b">
', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('F2FaYE6GlVnOvb18-4_S3', 'User invalidations return `410: Gone` for deleted rooms', FALSE, 1708109133000.0, 1707802650000.0, 'darkgnotic', 'We''re returning `410: Gone` for requests of the form:

```
https://mapkeep-preproduction-20231217-tristanls.reflect-server.net/api/v1/connections/users/c9be64ea-0802-4a0e-85a9-d16f6cbe6e53_deleteme:invalidate
```

I think the AuthDO is forwarding invalidate requests to rooms that have been deleted.

@grgbkr, is this a situation that is expected?
* If not, what''s the fix?
* If so, perhaps we can treat 410: Gone responses from rooms as OK in the case of invalidations?

I''m happy to make a fix if you''d like, but I''d like to confirm whether this is exposing some kind of bookkeeping bug.

<img width="1495" alt="Screenshot 2024-02-12 at 21 32 06" src="https://github.com/rocicorp/mono/assets/132324914/be15c743-5e33-45c0-b3b3-7b2027d26e68">
', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('1fsM2YJIam46LHJSXHhqN', '`410: Gone` responses are counted as warnings', FALSE, 1707802349000.0, 1707802201000.0, 'darkgnotic', 'When an API room:delete is called on a deleted room, the `reflect-server` returns `410: Gone`.

This is the intended behavior, but the error reporting system is flagging these as warnings (to us) and triggering alerts above a certain threshold. 

Perhaps we should return a different error for `410: Gone`. A delete of a deleted room should probably be a 2xx response? Maybe `204: No content`?

@grgbkr, any opinions?', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('E8dSj9b47wGSDgzsu0DMI', 'Rest invalidate-by-user API does not properly handle user ID''s with colons in them', FALSE, 1707963679000.0, 1707801918000.0, 'darkgnotic', 'API calls to urls that look like:

```
https://api-apps-emelgxcryq-uc.a.run.app/v1/apps/lq9xwmf6/connections/users/00969251-c431-457a-b581-b3b3a372c1e2:villagermapper:invalidate
```

are resulting in an error at the parsing stage:

```
HttpsError: Invalid resource or command "connections:villagermapper:invalidate" 
```

<img width="1503" alt="Screenshot 2024-02-12 at 21 23 27" src="https://github.com/rocicorp/mono/assets/132324914/eacce58f-cfa0-4171-9cb2-cd376dbebfe5">
', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('F16yIS2tLzwNqDN4Cznok', 'Clean up and complete Reflect server client hooks', TRUE, 1707388515000.0, 1707388515000.0, 'arv', 'We now have

- `disconnectHandler` which gets called when a client is disconnected (network dropped).
- `closeHandler` which gets called when a client (`clientID`) sends a close beacon or it gets collected when the client has not been seen for 2 weeks.
It seems like we might want to introduce `connectHandler` and `openHandler` for symmetry.

Maybe the API names should be?

- `onClientCreate`
- `onClientConnect`
- `onClientDisconnect`
- `onClientDelete`
', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('ZfwObbfT8wNRzALaTF39a', 'docs needed: closeHandler', TRUE, 1707388353000.0, 1707388352000.0, 'arv', '#1418 added a closeHandler to reflect server options.

This is called when the client is GC''d on the server.

This needs to be documented.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('gZ68_v0OSgG5iODXi8BWJ', 'Check isValidAppName() on `--app` so show the user a better error message', FALSE, 1707409113000.0, 1707334887000.0, 'darkgnotic', 'I think right now we''re showing them some Firestore error.

<img width="1277" alt="Screenshot 2024-02-07 at 11 38 20" src="https://github.com/rocicorp/mono/assets/132324914/034cfc69-5e0e-446c-ac13-aa6b6ec3bd99">

We can instead check `isValidAppName()` and show them a more informative error instead.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('NJstzvXt-8vhzO9ZirTXR', 'Suppress (new) `reflect dev` errors that should be reported as warnings', FALSE, 1706916902000.0, 1706915844000.0, 'darkgnotic', '@grgbkr @arv 

https://console.cloud.google.com/monitoring/alerting/incidents/0.n8v6wwmzyyvn?channelType=slack&project=reflect-mirror-prod&pageState=(%22interval%22:(%22i%22:(%22s%22:%222024-02-02T20:43:20.000Z%22,%22e%22:%222024-02-02T21:58:27.000Z%22)))

<img width="1119" alt="Screenshot 2024-02-02 at 15 16 11" src="https://github.com/rocicorp/mono/assets/132324914/da1e208a-5a28-474c-b111-925358fb6251">
', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('R9O4DN_T0uY-NcGNVRzah', 'Increase Custom Hostname quota in Cloudflare', FALSE, 1707857509000.0, 1706895951000.0, 'darkgnotic', 'For tracking: https://console.cloud.google.com/errors/detail/COn4rtn636HXuwE;time=P30D?project=reflect-mirror-prod&utm_source=error-reporting-notification&utm_medium=slack&utm_content=resolved-error

```
_FetchResultError: POST /zones/1b044253688b6ddb8e67738539a2b6d0/custom_hostnames: {"result":{"ssl":null},"success":false,"errors":[{"code":1405,"message":"Quota exceeded. If you''re already a paid SSL for SaaS customer, please contact your Customer Success Manager for additional provisioning. If you''re not yet enrolled, please fill out this form and someone from our sales team will contact you: https://www.cloudflare.com/plans/enterprise/contact/."}],"messages":[]}
  at Quota exceeded. If you''re already a paid SSL for SaaS customer, please contact your Customer Success Manager for additional provisioning. If you''re not yet enrolled, please fill out this form and someone from our sales team will contact you: https://www.cloudflare.com/plans/enterprise/contact/. (cloudflare:1405)
```

<img width="1040" alt="Screenshot 2024-02-02 at 09 43 32" src="https://github.com/rocicorp/mono/assets/132324914/f0919d59-53d7-47a3-a962-5a5bdae45c6a">
', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('tCNDwi5uFtMpq0wNi4snY', 'reflect.net fails the close beacon', FALSE, 1707395741000.0, 1706622320000.0, 'arv', 'Something is going on with the auth headers.

<img width="788" alt="Screenshot 2024-01-29 at 22 36 18" src="https://github.com/rocicorp/mono/assets/45845/c96436cb-62e5-4467-826d-6115b9cd7e43">
', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('l_0enidPsmtAhXGyA2VKR', 'Prolonged Cloudflare Analytics outages can result in orphaning data', FALSE, 1706236737000.0, 1705951016000.0, 'darkgnotic', 'Our metrics aggregation process received an error: `429: Please wait and consider throttling your request speed` (even with just one request) for what looks like at least 5 minutes:

<img width="1692" alt="Screenshot 2024-01-22 at 11 11 56â€¯AM" src="https://github.com/rocicorp/mono/assets/132324914/c13e8951-ccd3-471d-abde-5d10c1f70e5a">

Our Error Reporting shows that this isn''t the first or only time that it''s happened:

https://console.cloud.google.com/errors/detail/CPTJ2fj3s5D8zwE;time=P30D?project=reflect-mirror-prod&utm_source=error-reporting-notification&utm_medium=slack&utm_content=resolved-error

<img width="708" alt="Screenshot 2024-01-22 at 11 14 54â€¯AM" src="https://github.com/rocicorp/mono/assets/132324914/08788984-5d69-4fb6-907a-ca6c89491a92">

We might get lucky because we run the aggregation process 3 times per hour, at minutes 1, 5, and 30 (for the metrics of the previous hour).

However, this is not a reliable guarantee. We should add reliability to this archiving process so that we will theoretically go back to previous hour windows that were not successfully archived.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('ylT7g4Kd3elJzhArCrAGg', 'Reduce the latency of cold starts of Cloud Functions', FALSE, 1705080101000.0, 1705017847000.0, 'darkgnotic', 'The brute-force method of avoiding cold starts is to run our functions with a `minInstances: 1` configuration. However, this results in incurring a non-negligible cost:

![Screenshot 2024-01-11 at 3 18 03â€¯PM](https://github.com/rocicorp/mono/assets/132324914/cc6fc8b6-9d54-4364-b5a2-cc3d29d101bd)

A more cost effective method is to amortize the cost of the cold start by pre-initializing the function while we''re waiting for other things to initialize:

1. For the login page, we can ping `user-ensure` when we show the login form.
2. For cli commands, we can ping the destination function while we''re waiting for firebase authentication (creds from `~/.reflect/config/default.json`

An initial POC indicates that this will work:

![Screenshot 2024-01-11 at 3 44 25â€¯PM](https://github.com/rocicorp/mono/assets/132324914/5f83bef3-d093-4b3a-9a33-aa39b175cf0f)

![Screenshot 2024-01-11 at 3 44 51â€¯PM](https://github.com/rocicorp/mono/assets/132324914/6b60704d-e15b-4324-a24b-927afa6b1128)

It shouldn''t take too much work to make this a little cleaner so that we''re not littering our server logs with 401 requests.

@aboodman @cesara ', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('U0LjvDThCIoDEYqkjx3gv', 'Move Firestore writes out of the critical API serving path', FALSE, 1705008876000.0, 1705002688000.0, 'darkgnotic', 'Every time an app key is used, we update the `lastUsed` timestamp of the key, which is surfaced to the user when they list their keys. This is useful, for example, for knowing when keys are used and which keys are no longer used.

![Screenshot 2024-01-11 at 11 41 49â€¯AM](https://github.com/rocicorp/mono/assets/132324914/f69610ba-95b7-459c-b6c2-e4d400c53dc6)

The initial implementation of key authorization performs this Firestore write while authorizing the request. This is fine for manual developer actions like `reflect publish` and `reflect env set|delete`. However, it is less acceptable for actions that affect the actual app runtime, which as API calls.

For example, if we get simultaneous API calls to create or modify (close, delete) 1000 rooms, the calls will bottleneck/serialize at the step where we perform the transactional update of the app key. (And it will incur 1000 Firestore writes when we really only needed one).

Ideally, the update should be done off of the critical path. In fact, the code was written when a [TODO to address this](https://github.com/rocicorp/mono/blob/f5f8334dc50c3b9cd9060379aa0102eec700ba25/mirror/mirror-server/src/functions/validators/auth.ts#L257):

```ts
        txn.update(appKeyDocRef, {lastUsed: FieldValue.serverTimestamp()});
        return {app};
      },
      // TODO(darick): Add a mechanism for initiating writes (like the `lastUsed` timestamp update)
      // in the background so as not to delay request processing. Then this Transaction can be readOnly.
      // {readOnly: true},
    );
```

Note that simply delaying the write to happen later in the function is not an option, as cloud functions do not support "background" processing; the response is only sent after the returned Promise is resolved, and any unresolved Promises are not guaranteed to complete as the function may be hibernated before hand.

https://firebase.google.com/docs/functions/terminate-functions

So to correctly move logic off of the critical path, the logic must be handed off to another process (such as another function).
', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('e769BGw1cVWLswvZrrLn9', 'Active serving user dashboard Firestore queries', TRUE, 1709537194000.0, 1704937088000.0, 'darkgnotic', '## Motivation

[We need an â€œactive serving usersâ€ metric](https://www.notion.so/replicache/Reflect-Roadmap-d5a5dfbbf67f4a3a8077804e8250dc06?p=b86657b1edbc49c28e6b984b79563b58&pm=s)

@aboodman 

## Definitions
* An **active team** for a given period (month, day, hour) is one for which there are active room seconds recorded for that team.
* An **active app** can be similarly defined at the scope of an app instead of a team

## Metrics

Room seconds are stored as the `rs` metric in Firestore ([schema](https://github.com/rocicorp/mono/blob/7e70ce828b59bce7941bcae94ad293fe110f466e/mirror/mirror-schema/src/metrics.ts#L10)). An example of what these look like can be seen in [ledger.test.ts](https://github.com/rocicorp/mono/blob/7e70ce828b59bce7941bcae94ad293fe110f466e/mirror/mirror-server/src/metrics/ledger.test.ts#L364)

## Queries

* For yearly or monthly team totals, query the total metrics docs:

```ts
const query = await getFirestore()
    .collectionGroup(''metrics'')
    .where(''yearMonth'', ''=='', null)
    .where(''appID'', ''=='', null)
    .get();
```

From the results you can count the number of documents (i.e. teams) in which there are non-zero `rs` values for each month.

* For yearly or monthly app totals, similarly query:

```ts
const query = await getFirestore()
    .collectionGroup(''metrics'')
    .where(''yearMonth'', ''=='', null)
    .where(''appID'', ''!='', null)
    .get();
```
 
* For daily or hourly team totals, query the monthly metrics docs for the appropriate `yearMonth` value:

```ts
const query = await getFirestore()
    .collectionGroup(''metrics'')
    .where(''yearMonth'', ''=='', 202401)
    .where(''appID'', ''=='', null)
    .get();
```

This is structured similarly to the total metrics docs, but instead of a year/month breakdown, the docs will have day/hour breakdowns for the associated month.

* For the per-app equivalent, similarly change the condition of the `appID`:
 
```ts
const query = await getFirestore()
    .collectionGroup(''metrics'')
    .where(''yearMonth'', ''=='', 202401)
    .where(''appID'', ''!='', null)
    .get();
```

## Permissions / setup

The service account `metrics-dashboard@reflect-mirror-prod.iam.gserviceaccount.com` has been created with Firestore read-access. [Create a key](https://console.cloud.google.com/iam-admin/serviceaccounts/details/116401512738751249544/keys?project=reflect-mirror-prod) for this service account (JSON), download it, and use that to initialize the Firestore client in the dashboard backend.

Note that this key grants read access to all of Firestore and should be considered sensitive. DO NOT check it in to source control. ', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('vN2gzMHjlwjK3S8gNYREg', '`app-autoDeploy` -> `app-deploy` delayed for almost a minute', FALSE, 1704914930000.0, 1704845422000.0, 'darkgnotic', 'An error was triggered by a 500 response for an `app-deploy` run:

https://console.cloud.google.com/monitoring/alerting/incidents/0.n7y0k7cssgsw?project=reflect-mirror-prod&pageState=(%22interval%22:(%22i%22:(%22s%22:%222024-01-09T22:52:09.000Z%22,%22e%22:%222024-01-09T23:46:01.464Z%22)))

But as far as I can tell, the function succeeded.

![Screenshot 2024-01-09 at 4 01 28â€¯PM](https://github.com/rocicorp/mono/assets/132324914/0f313bd1-5bf6-4ec3-a065-9ccc57a71a75)

A closer look suggests that the error was because it took 1 minute for the command to succeed, which is [default timeout for v2 functions](https://cloud.google.com/functions/docs/configuring/timeout).

Why it took so long is an interesting question. I will increase timeouts and add logging to see if there''s some kind of transaction contention that''s causing a delay.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('mlp0pI11UeulsZNpVENGN', 'Suppress error (as warning) when the cli is run from a non-git directory', TRUE, 1703267147000.0, 1703267147000.0, 'darkgnotic', 'These aren''t actionable errors on our side.

![Screenshot 2023-12-22 at 9 43 42â€¯AM](https://github.com/rocicorp/mono/assets/132324914/3c426e7e-f657-4e43-8145-b649afbea4f8)
', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('62hTFrMd8ZzbjMXxJW9-5', 'authHandler custom error codes', TRUE, 1702663040000.0, 1702663001000.0, 'grgbkr', '<img width="993" alt="image" src="https://github.com/rocicorp/mono/assets/19158916/a8b853ed-316d-4f46-9b48-57599a3c3e71">
', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('Crni_yCBuJnmTXyO0RAgy', 'network checks should not be done in dev mode', FALSE, 1705337488000.0, 1702662369000.0, 'grgbkr', 'We already disable analytics and remote logging we should also disable the network check calls.

<img width="727" alt="image" src="https://github.com/rocicorp/mono/assets/19158916/7c38d5bc-f4fc-4650-9c4a-992c5fb349cc">
', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('WAwQvGFjwY_dvk4clQ_vN', 'reflect .d.ts references DurableObject', TRUE, 1709537218000.0, 1702637807000.0, 'arv', 'We are "exporting" something that depends on DurableObject (and related types) in our public API.

DOs are not supposed to be part of the public API.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('WtZFBqJUJ7tXABH8qBoJL', 'Suppress errors due to missing template file', FALSE, 1701886435000.0, 1701886304000.0, 'darkgnotic', 'As reported by @arv in https://rocicorp.slack.com/archives/C05TT7N4Z09/p1701780314529909

![image (16)](https://github.com/rocicorp/mono/assets/132324914/6d58bed2-767a-4c64-9186-4d628c9f3585)
', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('R3POoebZhdEd2uoAXrTjP', 'Intercept Miniflare stdout/stderr', TRUE, 1709537245000.0, 1701431420000.0, 'arv', 'Details here?

https://github.com/cloudflare/workers-sdk/releases/tag/miniflare%403.20231030.1', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('7baXcO5BCdpFSyo3hY4Wn', '`app-deploy` runs out of memory when all of prod is migrated / republished', FALSE, 1701397669000.0, 1701396616000.0, 'darkgnotic', 'I just ran a migration that renames an env var resulting in a simultaneous publish of all 57 apps. This resulted in `app-deploy` running out of memory:

![Screenshot 2023-11-30 at 6 06 22â€¯PM](https://github.com/rocicorp/mono/assets/132324914/d98bd8c8-d04a-4895-9d6f-42eeb881aac8)

It looks like the function is running with a default concurrency of 80:

![Screenshot 2023-11-30 at 6 07 59â€¯PM](https://github.com/rocicorp/mono/assets/132324914/26cd7d9b-8789-48e6-aa18-c44388dbefb5)

which is incongruent with to the [documentation](https://cloud.google.com/functions/docs/configuring/concurrency) which says the default is 1, but to be honest 1 is not very efficient.

I''ll set a global default to something lower than 80. ', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('dOjJVvGnPNvB0EYV23XhA', 'ENV deployment timeout is too short', FALSE, 1701286628000.0, 1701278769000.0, 'darkgnotic', 'https://console.cloud.google.com/monitoring/alerting/incidents/0.n55zr14ya686?project=reflect-mirror-prod&pageState=(%22interval%22:(%22i%22:(%22s%22:%222023-11-29T16:21:16.000Z%22,%22e%22:%222023-11-29T17:12:32.311Z%22)))

The deployment timeout fired less than a half second before the deployment was created.

![Screenshot 2023-11-29 at 9 22 34â€¯AM](https://github.com/rocicorp/mono/assets/132324914/5f767ad7-3863-4324-a0e4-0c4496a7ac56)
', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('p0zwE3aWI5O5GPiKh4P_d', 'Error saving dev vars for `reflect env --dev`', FALSE, 1701207411000.0, 1701203642000.0, 'darkgnotic', 'https://console.cloud.google.com/monitoring/alerting/incidents/0.n54qfjgfwu8a?project=reflect-mirror-prod&pageState=(%22interval%22:(%22i%22:(%22s%22:%222023-11-28T19:41:18.000Z%22,%22e%22:%222023-11-28T20:31:53.189Z%22)))

```
Error: ENOENT: no such file or directory, open ''/home/jared/work/2023/garden/.reflect/dev-vars.env''
    at Object.openSync (node:fs:601:3)
    at writeFileSync (node:fs:2249:35)
    at saveDevVars (file:///home/jared/work/2023/garden/node_modules/@rocicorp/reflect/bin/cli.js:139087:3)
    at setDevVars (file:///home/jared/work/2023/garden/node_modules/@rocicorp/reflect/bin/cli.js:139038:3)
    at setVarsHandler (file:///home/jared/work/2023/garden/node_modules/@rocicorp/reflect/bin/cli.js:140345:5)
    at Object.handler (file:///home/jared/work/2023/garden/node_modules/@rocicorp/reflect/bin/cli.js:139251:15)
    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)
```

![Screenshot 2023-11-28 at 12 32 43â€¯PM](https://github.com/rocicorp/mono/assets/132324914/578b61f5-0bff-45d2-afa8-904b76d1e2b2)
', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('knakVggHchm5GL-iH2bDn', '`metrics-backup` needs more memory', FALSE, 1701202072000.0, 1701196942000.0, 'darkgnotic', 'We''re getting enough usage such that a week''s worth of connection data exceeds 256MB.  ðŸŽ‰ 

https://console.cloud.google.com/monitoring/alerting/incidents/0.n54di51hyww0?project=reflect-mirror-prod&pageState=(%22interval%22:(%22d%22:%22P1D%22,%22i%22:(%22s%22:%222023-11-28T10:15:12.000Z%22,%22e%22:%222023-11-28T11:16:11.000Z%22)))

![Screenshot 2023-11-28 at 10 40 59â€¯AM](https://github.com/rocicorp/mono/assets/132324914/67b4cf53-23bf-40ba-ae09-adda932f66a9)
', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('S_2UbVYjMjXaqUmVhkbDn', 'Syntax errors from manual edits of `reflect.config.json`', FALSE, 1701216974000.0, 1700504881000.0, 'darkgnotic', 'Syntax errors from users manually editing `reflect.config.json` are current classified as errors. We should bucket them as warnings.

https://console.cloud.google.com/errors/detail/CJXXuJPQ_IPskwE;time=P30D?project=reflect-mirror-prod

![Screenshot 2023-11-20 at 10 23 51â€¯AM](https://github.com/rocicorp/mono/assets/132324914/bbc05234-f165-4a52-a8cc-4ce918069562)

I think the slightly subtle / tricky thing is downgrading Syntax errors from reading the `reflect.config.json` file without necessarily suppressing  other errors that should be surfaced.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('CaOVuMuUFZqBHnJRJUWNi', 'Firestore library mismatch', FALSE, 1700158696000.0, 1700157449000.0, 'darkgnotic', 'One of the pain points of the nodejs Firestore libraries is that they use `instanceof` for processing FieldValue transforms, and this can break when referencing multiple versions of the library in `node_modules` of different packages in a repo.

https://console.cloud.google.com/monitoring/alerting/incidents/0.n4nzsab1hu6a?project=reflect-mirror-prod&pageState=(%22interval%22:(%22i%22:(%22s%22:%222023-11-16T16:46:12.000Z%22,%22e%22:%222023-11-16T17:54:37.922Z%22)))

![Screenshot 2023-11-16 at 9 55 49â€¯AM](https://github.com/rocicorp/mono/assets/132324914/d6f30187-b24d-43bc-85ab-9ab667a4ebe7)
', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('Dj5IryPiYi5qHccuaKUVR', 'Error Reporting is lumping Cloudflare API errors together', FALSE, 1700005868000.0, 1699987036000.0, 'darkgnotic', 'We didn''t get an alert for #1228 because the Error Reporter is lumping the error in with the errors that we were getting during the Cloudflare outage (which I had set to "Acknowledged").

![Screenshot 2023-11-14 at 10 33 58â€¯AM](https://github.com/rocicorp/mono/assets/132324914/907ae928-5936-4ba4-a0f7-f3a4a8ecfdc3)

I will see if I can figure out how to make the Error Reporter distinguish these errors. It''s probably thinking that they have the same stack trace, i.e.

```
 _FetchResultError: GET /zones/1b044253688b6ddb8e67738539a2b6d0/dns_records: {"success":false,"errors":[{"code":10000,"message":"Internal authentication error: internal server error"}]}
    at cfFetch (file:///workspace/out/index.js:13701:11)
    at process.processTicksAndRejections (node:internal/process/task_queues:95:5) 
```

vs

```
 _FetchResultError: PUT /accounts/085f6d8eb08e5b23debfb08b21bda1eb/workers/dispatch/namespaces/prod/scripts/likeable-lackadaisical-store-lox8pviw: {"result":null,"success":false,"errors":[{"code":10021,"message":"Uncaught ReferenceError: window is not defined\n  at index.js:169:23\n  at index.js:41:70\n  at index.js:42:7 in node_modules/leaflet/dist/leaflet-src.js\n  at index.js:11:50 in __require\n  at index.js:9701:30\n"}],"messages":[]}
    at cfFetch (file:///workspace/out/index.js:13720:11)
    at process.processTicksAndRejections (node:internal/process/task_queues:95:5) 
```', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('LIu0go2daaVpcFXW-rFE7', 'Some Cloudflare publish errors are not caught by esbuild before publishing', FALSE, 1700005867000.0, 1699986772000.0, 'darkgnotic', 'One of our users was able to successfully upload a script that ends up with an error when uploading to Cloudflare:

```
Uncaught ReferenceError: window is not defined
  at index.js:169:23
  at index.js:41:70
  at index.js:42:7 in node_modules/leaflet/dist/leaflet-src.js
  at index.js:11:50 in __require
  at index.js:9701:30
```

![Screenshot 2023-11-14 at 10 31 52â€¯AM](https://github.com/rocicorp/mono/assets/132324914/15f1a842-5390-4e80-a672-4b52c45021eb)

I wonder if `esbuild` is accepting references to `window` whereas Cloudflare is not. Would it be possible for `reflect publish` to reject this before upload?', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('LIcSw2N7FVdlIb4z4eEdu', 'Poor error message when userID passed to constructor doesn''t match userID returned by authHandler', FALSE, 1699741367000.0, 1699740352000.0, 'aboodman', 'See: https://discord.com/channels/830183651022471199/1173010960408137829/1173010965634228315', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('e4fB1x5jR7I4U8Le6SBRx', '`reflect init` doesn''t work on a previous `reflect create`d project', FALSE, 1709537228000.0, 1699409957000.0, 'darkgnotic', 'This may be a weird scenario but we get these errors in production a lot so I don''t think it''s a one-off:

![Screenshot 2023-10-28 at 9 57 56â€¯AM](https://github.com/rocicorp/mono/assets/132324914/9dd56ed4-7c37-4771-a8db-7f3722d25ee8)

The way it happened to me just now:
* run `npx reflect create foo`. This sets everything up to point to `src/reflect/index.ts`
* run `npx reflect publish` to create the server-side app
* delete the app with `npx reflect delete`
* also delete `reflect.config.json`
* now try to revive the app with `npx reflect init`

Unlike `create`, `init` sets up `reflect.config.json` to point to `reflect/index.ts`. I think this may be why we see so many of those `dev` / `ENOENT` errors.

I know `reflect init` is supposed to be in line with the instructions in https://hello.reflect.net/add-to-existing, but it''s a bit unfortunate that it''s not compatible with the project created by `reflect create`.

Not an emergency, but improving this might reduce some bounce factor.

Maybe have `init` check for the file? Or be even fancier and replace the file path if it''s in `src/reflect/index.ts`?

@cesara @aboodman ', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('CPx1yvB48SrvgbYUSpkMq', 'Move secrets out of the App document', FALSE, 1699422390000.0, 1699234885000.0, 'darkgnotic', 'Currently, app secrets (implemented for #679 and #1150) are stored in encrypted form in the App doc. This makes it convenient for triggering a new deployment when secrets change. However, there are a couple of reasons this is non-ideal:

* We currently limit the number of variables per env to 50 * 5kb secrets, which is ~256kb of data. Each Firestore document stores a maximum of 1MB, so this schema limits the number of environments we can support. And it''s better not to load that much data for the App every time we perform an operation on it anyway.
* The App doc is accessible to anyone on the App''s "team", which means we technically give them unlimited power to generate plaintext / ciphertext pairs. Although the generation includes a random initialization vector that they can''t control, it may provide some advantage in brute-forcing the global encryption key. It would be better not to expose the ciphertexts either.

Fix:

* Move app secrets into an app subcollection `envs`, with the default one living in `apps/<appID>/envs/(default)`
* Disallow external read access to the `envs` subcollection.
* In the App doc (or in the future, whatever doc tracks a deployment instance), store the `secretsUpdateTime` in the `DeploymentSpec` and use that to determine when a new deployment in necessary (instead of the current `hashesOfSecrets` field).', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('SpuiULcIq9nfGhRUjbZc_', 'Error Reporter buckets disparate errors together', FALSE, 1698865533000.0, 1698865011000.0, 'darkgnotic', 'Error reports like https://console.cloud.google.com/errors/detail/CMeqyPmjg-zPBQ;time=P30D?project=reflect-mirror-prod&utm_source=error-reporting-notification&utm_medium=slack&utm_content=resolved-error

group many disparate errors together:

![Screenshot 2023-11-01 at 11 55 00â€¯AM](https://github.com/rocicorp/mono/assets/132324914/fd91fbda-6dd3-470d-a39d-324bc8968902)

This is likely because they all have similar stack traces, being thrown by the `error-report` handler.

We can likely improve this by having the thrown errors inherit the original stack trace from the error reported by the cli.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('uNGIrPyPoDby2oNWz2Ayr', 'tail returns a 500 if the app is not deployed', FALSE, 1698859700000.0, 1698798878000.0, 'darkgnotic', 'https://rocicorp.slack.com/archives/C05TT7N4Z09/p1698795167619959

https://console.cloud.google.com/errors/detail/CPzZ--WJrr63Zg;time=P30D?project=reflect-mirror-prod

![Screenshot 2023-10-31 at 4 35 53â€¯PM](https://github.com/rocicorp/mono/assets/132324914/535ed400-ae71-4aa5-a48c-34b999888125)

Short-term fix is to return a 400.

Long-term, perhaps we should not auto-create the app for `tail`, and instead prompt the user to `publish` first?', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('yHifzF5hD9i0QUXh6du60', 'reflect tail to the wrong room results in a TypeError', FALSE, 1699480526000.0, 1698709252000.0, 'darkgnotic', '@arv I think you fixed this in one place but perhaps there is another protocol mismatch (in the AuthDO)?

![Screenshot 2023-10-30 at 4 39 04â€¯PM](https://github.com/rocicorp/mono/assets/132324914/50b797bd-b5bf-4411-8707-022845f02d91)
', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('Vryo7PPVw6fLyhASCJN_D', 'app-publish function needs more Memory', FALSE, 1701202102000.0, 1698518825000.0, 'darkgnotic', 'https://console.cloud.google.com/errors/detail/CKnV1_TJwMrDQw?project=reflect-mirror-prod&supportedpurview=project

![Screenshot 2023-10-28 at 11 46 36â€¯AM](https://github.com/rocicorp/mono/assets/132324914/6d36409d-c367-4637-ab20-83c831962d6b)
', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('B58g9LAcLydOVYSkRW3_t', 'Errors from Firestore-triggered functions are not making it to alerts', FALSE, 1698518550000.0, 1698513286000.0, 'darkgnotic', 'When I was debugging https://github.com/rocicorp/mono/issues/1159 I generated a lot of errors in prod, but they didn''t trigger alerts.

It seems that even if we throw an error from a Firestore-triggered function, the containing "request" ends with a 200:

![Screenshot 2023-10-28 at 10 09 56â€¯AM](https://github.com/rocicorp/mono/assets/132324914/0dd0a596-05b5-4d14-9ea1-f6c57180d574)

And are indeed classified as `2xx` responses in monitoring:

![Screenshot 2023-10-28 at 10 12 23â€¯AM](https://github.com/rocicorp/mono/assets/132324914/8cbed0d1-e735-4f19-8408-c97b9d94c775)

Firestore functions (and [event-driven functions](https://cloud.google.com/functions/docs/writing/write-event-driven-functions) in general) probably have some slightly different semantics in terms of errors (particularly when it comes to retries). I need to investigate this so that these errors don''t go unnoticed.
', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('oSH_7bnghHj3YPNvrVbNb', 'SSL handshake error when attempting to verify liveness of a worker', FALSE, 1698471768000.0, 1698456662000.0, 'darkgnotic', 'I''m seeing this in sandbox. Not sure why.

```
GET https://104.18.14.180:443/ error TypeError: fetch failed
    at Object.fetch (node:internal/deps/undici/undici:11372:11)
    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)
    at async waitForLiveness (file:///workspace/out/index.js:14536:19)
    at async NamespacedScriptHandler.publish (file:///workspace/out/index.js:14427:22)
    at async runDeployment (file:///workspace/out/index.js:14742:22)
    at async file:///workspace/out/index.js:14657:5 {
  cause: [Error: C027AA16023E0000:error:0A000410:SSL routines:ssl3_read_bytes:sslv3 alert handshake failure:../deps/openssl/openssl/ssl/record/rec_layer_s3.c:1586:SSL alert number 40
  ] {
    library: ''SSL routines'',
    reason: ''sslv3 alert handshake failure'',
    code: ''ERR_SSL_SSLV3_ALERT_HANDSHAKE_FAILURE''
  }
}
```

![Screenshot 2023-10-27 at 6 29 35â€¯PM](https://github.com/rocicorp/mono/assets/132324914/b89778ba-9293-40e6-87a4-2cc39511d39c)
', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('9knMv_3VgG6atjXmhsQ_f', 'Demote `publish` compilation errors to WARNING severity', FALSE, 1698429366000.0, 1698425243000.0, 'darkgnotic', 'Compilation errors are generally user error and not something actionable on our part.

https://console.cloud.google.com/monitoring/alerting/incidents/0.n3vqnhz9wgeo?project=reflect-mirror-prod

![Screenshot 2023-10-27 at 9 45 46â€¯AM](https://github.com/rocicorp/mono/assets/132324914/f32edbfe-7cc0-4c85-9cff-cfb2c0c1e5a6)
', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('T_LUVleECWfFHLT9ybQwt', 'Support Environment Variables for Mirror Customers', FALSE, 1699068689000.0, 1698424695000.0, 'darkgnotic', 'Feature is self explanatory.

Design: https://www.notion.so/replicache/Reflect-Server-Vars-21909a4672f340c0a2ef20cd4c503d50', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('L3E2vz_oZYIBSZE5kdOY3', 'Certain `reflect-cli` commands fail when run with Node versions <18', FALSE, 1698280434000.0, 1698273745000.0, 'darkgnotic', 'In particular our calls to `fetch()` for npm dist tags and Google analytics will fail with a `ReferenceError`.

https://rocicorp.slack.com/archives/C05TT7N4Z09/p1698272539648889

![Screenshot 2023-10-25 at 3 27 19â€¯PM](https://github.com/rocicorp/mono/assets/132324914/5879d149-e58c-484e-b51f-37a8d7b2644a)
', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('8tS30s-ZGcWJSgCCcAqw9', 'Avoid reporting FirebaseError errors from the cli', FALSE, 1698190096000.0, 1698111374000.0, 'darkgnotic', '`FirebaseError` errors come from the mirror-server and are already counted (and classified). They shouldn''t be reported by the cli as that is redundant (and we lose the warn vs error classification logic).

https://console.cloud.google.com/monitoring/alerting/incidents/0.n3rhwm8g0m4i?project=reflect-mirror-prod

![Screenshot 2023-10-23 at 6 19 29â€¯PM](https://github.com/rocicorp/mono/assets/132324914/e9d514f5-8c03-4bc9-8318-2da9f0617c88)

@cesara this should be an easy one?', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('HS6lmBiKf0CJwzObZg1EQ', 'Exclude or raise threshold for alerts from roci team member accounts', FALSE, 1698190096000.0, 1698013368000.0, 'darkgnotic', 'Alerts from our own accounts are usually due to us choosing the wrong stack or something else that need not alert the team (as one of us already knows about it).

Example:

https://console.cloud.google.com/monitoring/alerting/incidents/0.n3nyfzyvytcc?project=reflect-mirror-prod

![Screenshot 2023-10-22 at 3 20 32â€¯PM](https://github.com/rocicorp/mono/assets/132324914/4873cc5a-bf09-4e0b-8f98-7b36385c1935)
', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('C_LnNPYX-8v3iNjNo5GQ-', 'Avoid alerting when `npm add / install` fail in `reflect create / init`', FALSE, 1698198485000.0, 1698013085000.0, 'darkgnotic', 'We get a regular cadence of alerts that are non-actionable, such as `npm` action failures. We should treat these differently and avoid alerting on therm, or at least raise the threshold for alerts.

@cesara 

Examples: 

https://console.cloud.google.com/monitoring/alerting/incidents/0.n3nt2pemzawz?project=reflect-mirror-prod

https://console.cloud.google.com/monitoring/alerting/incidents/0.n3nu0ecvz18n?project=reflect-mirror-prod

![Screenshot 2023-10-22 at 3 14 35â€¯PM](https://github.com/rocicorp/mono/assets/132324914/25aa0a7e-c3cd-4321-8ad8-ce07496b969a)
', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('s8-JbMuhMcMaGMpQAts5E', 'Catch (and avoid reporting) ENOENT errors from `reflect dev`', FALSE, 1698430980000.0, 1697817833000.0, 'darkgnotic', 'These are generally user error and not worth grabbing our attention to investigate each time. Thank you @arv!

https://console.cloud.google.com/monitoring/alerting/incidents/0.n3mifj58m8e8?project=reflect-mirror-prod

![Screenshot 2023-10-20 at 12 00 20â€¯PM](https://github.com/rocicorp/mono/assets/132324914/f23539a9-b0fe-43a5-a898-40ea45dd348a)

https://console.cloud.google.com/monitoring/alerting/incidents/0.n3mdczsszhmt?project=reflect-mirror-prod

![Screenshot 2023-10-20 at 11 11 24â€¯AM](https://github.com/rocicorp/mono/assets/132324914/75360008-917c-4cc5-9c7d-7f984806da47)
', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('PlNsdCMV1KeJ6-jm6Os_d', 'Reflect CLI should check node version', TRUE, 1709537245000.0, 1697727821000.0, 'arv', 'The CLI should check the node version and maybe even the npm version since we use npm in some of these cases.

Wrangler has a "wrapper" that does these checks before invoking node again with a new child process.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('oJaCtC8GZ3yGYH4AKobJQ', 'Cloudflare sometimes returns 504: Gateway Timeout on a Custom Hostname GET', FALSE, 1697656427000.0, 1697647879000.0, 'darkgnotic', 'This resulted in an error when @arv was trying to publish. It appears to be transient, as his subsequent publish succeedeed.

Maybe Cloudflare would eventually recover with exponential backoff.

https://console.cloud.google.com/monitoring/alerting/incidents/0.n3jr4772yv4d?project=reflect-mirror-prod

```
Error: GET /zones/1b044253688b6ddb8e67738539a2b6d0/custom_hostnames/997fa415-c18c-4a80-acff-a76d522c8417: 504: Gateway Time-out: SyntaxError: Unexpected token < in JSON at position 0
    at cfFetch (file:///workspace/out/index.js:13417:11)
    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)
    at async createCustomHostname (file:///workspace/out/index.js:13720:66)
    at async Promise.allSettled (index 0)
    at async setCustomHostnames (file:///workspace/out/index.js:13676:19)
    at async publishCustomHostname (file:///workspace/out/index.js:13630:20)
    at async NamespacedScriptHandler._doPublish (file:///workspace/out/index.js:14291:22)
    at async NamespacedScriptHandler.publish (file:///workspace/out/index.js:14222:22)
    at async runDeployment (file:///workspace/out/index.js:14454:22)
    at async file:///workspace/out/index.js:14369:5
```

![Screenshot 2023-10-18 at 12 47 24â€¯PM](https://github.com/rocicorp/mono/assets/132324914/9ff02b4f-3e27-4d9f-8738-1c008494d8ea)
', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('x2DlBWMg86apkaKv4hG_6', '`reflect delete --all` could use a checkbox list', FALSE, 1698190076000.0, 1697645365000.0, 'arv', 'I think a better UI might be to select all the apps to delete first instead of doing them one by one. Something along:

```
[ ] app-1
[x] app-2 
[ ] app-3
(Delete selected)
```', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('T6xu20G4WytR_lhsAqg55', 'Use JSON5 for reflect config files', TRUE, 1709537264000.0, 1697566191000.0, 'arv', 'So that we can have comments in these config files', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('QUYqA_yGLqpHYJZ5fmXR1', 'Use code splitting for reflect-cli', FALSE, 1697190426000.0, 1697108907000.0, 'arv', 'Reflect cli is very large and it is slow to start.

If we use code splitting we can skip loading things that are not needed.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('WbcOiV7djvjQ0ygCFCmDJ', 'Analytics report consistently fails on `reflect create`', FALSE, 1697000917000.0, 1696999738000.0, 'darkgnotic', 'Error reports have shown that the analytics report call often fails on the `reflect create` command:

https://console.cloud.google.com/monitoring/alerting/incidents/0.n385u4w9xu68?project=reflect-mirror-prod
https://console.cloud.google.com/monitoring/alerting/incidents/0.n38tisluhgjk?project=reflect-mirror-prod
https://console.cloud.google.com/monitoring/alerting/incidents/0.n39fudf8ny0w?project=reflect-mirror-prod

and it can actually be consistently reproduced locally:

```
examples $ node ~/roci/mono/mirror/reflect-cli/out/index.mjs --stack=sandbox create analytics-test
Installing @rocicorp/reflect

You''re all set! ðŸŽ‰

Run Reflect dev server and UI:

cd analytics-test && npm run watch

TypeError: fetch failed
    at Object.fetch (node:internal/deps/undici/undici:11457:11)
    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)
    at async sendAnalyticsEvent (file:///Users/ocean/roci/mono/mirror/reflect-cli/out/index.mjs:304:3)
    at async Promise.all (index 0)
    at async Object.handler (file:///Users/ocean/roci/mono/mirror/reflect-cli/out/index.mjs:501:9) {
  cause: ConnectTimeoutError: Connect Timeout Error
      at onConnectTimeout (/Users/ocean/roci/mono/node_modules/miniflare/node_modules/undici/lib/core/connect.js:184:24)
      at /Users/ocean/roci/mono/node_modules/miniflare/node_modules/undici/lib/core/connect.js:131:46
      at Immediate._onImmediate (/Users/ocean/roci/mono/node_modules/miniflare/node_modules/undici/lib/core/connect.js:172:9)
      at process.processImmediate (node:internal/timers:476:21) {
    code: ''UND_ERR_CONNECT_TIMEOUT''
  }
}
 examples $ 
```
One thing that''s puzzling is that the error is coming from `node_modules/miniflare/node_modules/...`. I''m not sure why `fetch` is using that library.

@cesara @arv ', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('W4zPPyyTsmaI1-yxl56WJ', 'Monday reports large number of connection timeout errors', FALSE, 1696949095000.0, 1696875620000.0, 'aboodman', 'See: https://discord.com/channels/830183651022471199/1158706195436159026/1158706201614372954

@grgbkr and I have identified one reason this error gets thrown spuriously, which is that when Reflect is `close()`''d, the timer never gets canceled. There is evidence this is happening often in the logs:

https://discord.com/channels/830183651022471199/1158706195436159026/1160169594158321674', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('NqY5fk7zVCFKzxKBt7N69', 'Monday reports that BroadcastChannel sometimes not found on Safari >= 15.4?', FALSE, 1709537063000.0, 1696280013000.0, 'aboodman', 'I feel like this must be people spoofing their user agent or something? But in any case, we should bring back the polyfil. This would also help people use Reflect on servers.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('D43Q0DwS-qD5VOGcKntt6', 'Ensure reflect-cli / schema compatibility with self-deprecation', FALSE, 1695945546000.0, 1695660588000.0, 'darkgnotic', 'The fact that the `reflect-cli` reads directly from Firestore is a double-edged sword.
* On the one hand, it obviates the need to create and maintain a Cloud Function API for reading data from the store
* On the other hand, it essentially makes the Firestore schema the API for which versioning must be managed instead.

Last week I had forgotten about this and mistakenly removed the required `teamSubdomains` field from the App doc, as the server code was updated to no longer expect it. However, the `reflect-cli` expected it, and the `reflect delete` functionality consequently broke.

There is a yet-to-be-implemented [scheme for handling cli / schema versioning / compatibility](https://www.notion.so/replicache/Mirror-4d1c7f00d95c410299ceecd9311e1560#3ef1f998cefc4c7d97c9c5a8a3e158b8) in which we publish min-supported, max-deprecated, and current versions of the cli, and the cli knows to check these versions before operating. It can prompt the user to upgrade if its own version is deprecated, and refuse to proceed if unsupported. This will allow us to eventually clean up old schemas.

The document proposes maintaining these versions in Firestore, but if the versions do not need to be stack-specific, they could also be dist-tags in npm. `@latest` is already there by default, and we could add `@cli-supported` and `@cli-deprecated` tags.

@aboodman @arv @grgbkr for any additional thoughts / preferences', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('9GXlK-CM2benNeZJ4T0Y-', 'replicache cjs package broken', FALSE, 1695373060000.0, 1695301277000.0, 'arv', 'We compile replicache to both ESM and CJS. However, with v13 some dependencies are kept as external (this was to prevent duplicated code in reflect) so the cjs file has lines like:

```js
require(''@rocicorp/logger'')
```

However, `@rocicorp/logger` (and the other `@rocicorp/*` deps) don''t publish CJS modules. This leads to an cjs module trying to require an esm module, which isn''t supported by nodejs. The error that looks like:

```
temp/funnele/app  main âœ—                                                                                                        7m âš‘
â–¶ SKIP_ENV_VALIDATION=1 npm run build


> app@0.1.0 build
> next build

 âœ“ Creating an optimized production build
 âœ“ Compiled successfully
 âœ“ Linting and checking validity of types
   Collecting page data ...Error [ERR_REQUIRE_ESM]: require() of ES Module /Users/arv/src/temp/funnele/app/node_modules/@rocicorp/logger/out/logger.js from /Users/arv/src/temp/funnele/app/node_modules/replicache/out/replicache.cjs not supported.
Instead change the require of logger.js in /Users/arv/src/temp/funnele/app/node_modules/replicache/out/replicache.cjs to a dynamic import() which is available in all CommonJS modules.
    at mod.require (/Users/arv/src/temp/funnele/app/node_modules/next/dist/server/require-hook.js:64:28)
    at Object.<anonymous> (/Users/arv/src/temp/funnele/app/node_modules/replicache/out/replicache.cjs:1:2550)
    at mod.require (/Users/arv/src/temp/funnele/app/node_modules/next/dist/server/require-hook.js:64:28)
    at 5326 (/Users/arv/src/temp/funnele/app/.next/server/pages/replicache-example.js:1:2335)
    at t (/Users/arv/src/temp/funnele/app/.next/server/webpack-runtime.js:1:128)
    at /Users/arv/src/temp/funnele/app/.next/server/pages/replicache-example.js:1:1209
    at t.a (/Users/arv/src/temp/funnele/app/.next/server/webpack-runtime.js:1:879)
    at 5919 (/Users/arv/src/temp/funnele/app/.next/server/pages/replicache-example.js:1:1132)
    at t (/Users/arv/src/temp/funnele/app/.next/server/webpack-runtime.js:1:128)
    at /Users/arv/src/temp/funnele/app/.next/server/pages/replicache-example.js:1:479
    at t.a (/Users/arv/src/temp/funnele/app/.next/server/webpack-runtime.js:1:879)
    at 45015 (/Users/arv/src/temp/funnele/app/.next/server/pages/replicache-example.js:1:85)
    at t (/Users/arv/src/temp/funnele/app/.next/server/webpack-runtime.js:1:128)
    at r (/Users/arv/src/temp/funnele/app/.next/server/pages/replicache-example.js:1:2580)
    at /Users/arv/src/temp/funnele/app/.next/server/pages/replicache-example.js:1:2615
    at t.X (/Users/arv/src/temp/funnele/app/.next/server/webpack-runtime.js:1:2116)
    at /Users/arv/src/temp/funnele/app/.next/server/pages/replicache-example.js:1:2593
    at Object.<anonymous> (/Users/arv/src/temp/funnele/app/.next/server/pages/replicache-example.js:1:2643)
    at mod.require (/Users/arv/src/temp/funnele/app/node_modules/next/dist/server/require-hook.js:64:28)
    at requirePage (/Users/arv/src/temp/funnele/app/node_modules/next/dist/server/require.js:109:84)
    at /Users/arv/src/temp/funnele/app/node_modules/next/dist/server/load-components.js:59:84
    at async loadComponentsImpl (/Users/arv/src/temp/funnele/app/node_modules/next/dist/server/load-components.js:59:26)
    at async /Users/arv/src/temp/funnele/app/node_modules/next/dist/build/utils.js:1045:32
    at async Span.traceAsyncFn (/Users/arv/src/temp/funnele/app/node_modules/next/dist/trace/trace.js:105:20) {
  code: ''ERR_REQUIRE_ESM''
}

> Build error occurred
Error: Failed to collect page data for /replicache-example
    at /Users/arv/src/temp/funnele/app/node_modules/next/dist/build/utils.js:1195:15
    at process.processTicksAndRejections (node:internal/process/task_queues:95:5) {
  type: ''Error''
}
   Collecting page data .%

```', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('Hv6m4oCF_ILqiPYWw5cLL', 'Reduce risk of AuthDO revalidate causing overloaded errors', TRUE, 1709537289000.0, 1695146703000.0, 'grgbkr', 'Revalidate makes a fetch to each RoomDO its needs to revalidate connections for.  It take a write lock on the authLock for each request.  If the RoomDO is slow to start up or respond it could cause the AuthDO to get overloaded errors, as all connection requests will be blocked by this write lock on the authLock.  Monday WorkCanvas''s AuthDO has had overload errors that may be due to this.

This risk can be reduced by:
1. send a fetch outside the lock to the RoomDO to wake it up, only after its awake take the lock and make the critical fetch that needs to be locked
2. add a timeout to the fetch inside the lock so it can''t hold it for more than say 300ms (if we''ve already woke the RoomDO it should be very fast).

More context https://rocicorp.slack.com/archives/C013XFG80JC/p1695091818933899

cc @d-llama @aboodman ', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('FU2janrOctZZvFLLvMtPs', 'RFE: A way to subscribe to only keys efficiently', FALSE, 1709537043000.0, 1694111023000.0, 'aboodman', 'A common pattern in Replicache/Reflect apps is that there is some container (a canvas, a list of todos, etc) and then there are items.

In React, we want to reduce renders. If the container element subscribes to all the elements and passes them down to child components, then any change in a child will cause the container to re-render, which will also cause all the _other_ children to re-render. This can be mitigated somewhat with `React.memo` on the children elements, but this pattern will always result in at least an extraneous re-render of the container for every child change.

Alternately, we could have the container subscribe to only the keys (`useSubscribe(r, async tx => await tx.scan().keys())`). When a child changes, the query will re-run at the container, but it will compute the same keys so the container won''t re-render. We still do extraneous work, but less.

If there were a way to subscribe to keys and have Replicache only re-run the query when a key is added/removed, then the second pattern could be implemented perfectly efficiently in React. Replicache would only re-run the query when a child element is added or removed, and the container would only re-render when that happened.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('qNtxQnpxgL89FkPLw7b5C', 'Cloudflare domains quota exceeded', FALSE, 1697255776000.0, 1693510992000.0, 'darkgnotic', 'Good news: deployment errors are surfaced to the cli:

<img width="1224" alt="Screenshot 2023-08-31 at 12 39 13 PM" src="https://github.com/rocicorp/mono/assets/132324914/e168a921-ebff-4e07-9987-776a17c69be1">

Bad news: The error from Cloudflare indicates that our domain quota has been exceeded:

```
cfFetch: URL: https://api.cloudflare.com/client/v4/accounts/085f6d8eb08e5b23debfb08b21bda1eb/workers/scripts/inconclusive-entertaining-cornet-llzkgp8i/domains/records response: {
  "result": null,
  "success": false,
  "errors": [
    {
      "code": 100122,
      "message": "workers.api.error.origin_quota_exceeded"
    }
  ],
  "messages": []
}


ParseError: A request to the Cloudflare API (/accounts/085f6d8eb08e5b23debfb08b21bda1eb/workers/scripts/inconclusive-entertaining-cornet-llzkgp8i/domains/records) failed.
    at throwFetchError (file:///workspace/out/index.js:13017:17)
    at cfFetch (file:///workspace/out/index.js:13007:3)
    at process.processTicksAndRejections (node:internal/process/task_queues:95:5)
    at async publishCustomDomains (file:///workspace/out/index.js:13420:3)
    at async Promise.all (index 0)
    at async publish2 (file:///workspace/out/index.js:13537:3)
    at async runDeployment (file:///workspace/out/index.js:13759:5)
    at async file:///workspace/out/index.js:13707:5
```

<img width="1483" alt="Screenshot 2023-08-31 at 12 40 44 PM" src="https://github.com/rocicorp/mono/assets/132324914/728f1e44-d5bc-473a-8c80-afe1d97e7f93">


@arv in case you think we want to handle errors more gracefully than a ParseError (although it''s probably fine).

@aboodman for plan of action w.r.t. Cloudflare. Is there a way to increase our quota?

In the meantime, I''ll start working on cleaning up the staging apps from our prod Cloudflare account. ', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('_cXlBrP9gPy7bPU9e29Qq', 'Build server modules from canonical npm package instead of mono repo source', FALSE, 1695036728000.0, 1692313609000.0, 'darkgnotic', 'Right now the `mirror-cli upload` command resolves the server module from `@rocicorp/reflect` and uploads it as the version in `packages/reflect/package.json`, and unless I''m mistaken, this comes from the current source files in the `mono` repo and not actually from the official `@rocicorp/reflect` package in npm. (This is one of the things @aboodman was wondering about). This means that the server module can have unpublished changes that were committed to the repo without a version bump.

Is there a way to have the `mirror-cli` package reference the canonical `@rocicorp/reflect` npm (while still being in the `mono` repo)? I imagine that having multiple instances of a package with the same version can wreak some havoc, but maybe there is some npm-fu that can do this.

Fwiw, I found a similar question on stack overflow but no answer ...

https://stackoverflow.com/questions/76899569/how-to-use-published-version-of-a-package-in-an-npm-workspace-mono-repo

@arv  ', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('d6yffLhRTtqAPHNsLBj10', 'Private release instructions in README are not correct', FALSE, 1709537314000.0, 1692008625000.0, 'aboodman', 'There is no `rocicorp-replicache` branch.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('tTqGbFV7s5es4M19NaRn3', 'Reconnect on `online` event (Replicache)', TRUE, 1709537120000.0, 1691542142000.0, 'aboodman', NULL, NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('8ZE5fcUusvKXl9oEiJtjG', 'Add a public push method, similar to the existing pull one', FALSE, 1709537095000.0, 1691542080000.0, 'aboodman', 'Thinking `push({now: boolean?}?)`.

If `now` is true, then bypass exponential backoff and everything else and literally push right now. For consistency add same to `pull()`.

See also #807

Recent user request: https://discord.com/channels/830183651022471199/1176237877764571176/1176243172758790224', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('m6qalgqfILNrbKRuVNQYQ', 'Make pull override exponential backoff and immediately pull', FALSE, 1709537100000.0, 1691542041000.0, 'aboodman', 'I am not sure whether this needs to go through the connection loop at all, or whether it can simply bypass it.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('pKyFIZHzJ5xaEmJJpStDa', 'reflect reduce valita validation in prod', TRUE, 1709537340000.0, 1691465582000.0, 'grgbkr', 'Reflect client and server do a lot of valita validation (client validates messages received, server validates messages received, values written my mutators, values read by mutators, meta data read from storage).

Much of this is overly defensive, given how expensive we know this validation to be.  

We should probably disable most of the validation in production builds and deployments.  

cc @aboodman @arv ', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('2y79vz_jBCdGgRqvegYyE', 'Use client-side Firestore SDK in reflect-cli', FALSE, 1691396851000.0, 1691004218000.0, 'darkgnotic', 'We have to use the client-side SDK (i.e. `firebase/firestore`) in the `reflect-cli` in order to act on behalf of the logged in user and honor all of the client-side security rules (i.e. not allow them to read other users'' data). The current use of the `@google-cloud/firestore` library doesn''t do this, and it only happens to work via the default credentials provided by our local gcloud logins (and thus won''t work for our customers).

Unfortunately, this means that we won''t be able to share as much code between the server (Firebase Functions) and client (cli) because the two SDKs are pretty different. And we may need to use the emulator to test stuff on the client side.

Investigating this.

@arv @cesara ', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('9JroQ9_wROsidpiwLa4zv', 'Evenflow: dramatically increasing capacity of Reflect', TRUE, 1690852014000.0, 1690852003000.0, 'aboodman', 'We should implement Evenflow (https://www.notion.so/replicache/Evenflow-7f968a66d88d42e1bafdf1ed195b8d63) so that Reflect can support many more users per room :).', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('O1U5xVLfsbRBoIziTaqtt', 'Setup reflect.net sandbox and cleanup env naming', TRUE, 1709537366000.0, 1690584817000.0, 'grgbkr', 'Current state
===========
**Vercel setup**

- staging.reflect.net is simply the same as reflect.net, both deploy from branch main with Production env vars pointing them at reflect-server.net.  Production deployment deploys reflect-net-east.
- all other branches are deployed with Preview env vars point them at staging.reflect-server.net.  Preview deployment deploys reflect-net-east env.staging (i.e. reflect-net-east-staging).

**CF setup**
- reflect-net-east is served on custom domain reflect-server.net.  It is deployed when Vercel does a Production deploy.  It is used by reflect.net and staging.reflect.net.
- reflect-net-east env.staging (i.e. reflect-net-east-staging) is on custom domain staging.reflect-server.net.  It is deployed when Vercel does a Preview deploy.  It is used by Vercel previews (i.e. the preview links on github code reviews).

Target state
=========
**Vercel setup**
- reflect.net is deployed from branch main with Production env vars pointing at reflect-server.net.  Production deployment deploys reflect-net-east.
- sandbox.reflect.net is deployed from branch sandbox (which is a mirror of main) with its [own env vars ](https://vercel.com/docs/concepts/deployments/git#multiple-preview-phases) that point it at sandbox.reflect-server.net.   When sandbox.reflect.net is deployed it deploys reflect-net-east env.sandbox (i.e. reflect-net-east-sandbox).
- all other branches are deployed with Preview env vars point them at preview.reflect-server.net.  Preview deployment deploys reflect-net-east env.preview (i.e. reflect-net-east-preview).
- For sandbox setup to work, we need to ensure @rocicorp/mono''s `main` branch is mirror''d to a new `sandbox` branch (in the same repo).  This github action looks promising for doing that: https://github.com/google/mirror-branch-action
- staging.reflect.net should go away

**CF setup**
- reflect-net-east is served on custom domain reflect-server.net.  It is deployed when Vercel does a Production deploy.  It is used by reflect.net.
- reflect-net-east env.sandbox (i.e. reflect-net-east-sandbox) is on custom domain sandbox.reflect-server.net.  It is deployed when Vercel does a deploy of the sandbox branch.  It is used by sandbox.reflect.net.
- reflect-net-east env.preview (i.e. reflect-net-east-preview) is on custom domain preview.reflect-server.net.   It is deployed when Vercel does a Preview deploy.  It is used by Vercel previews (i.e. the preview links on github code reviews).
- reflect-net-eash env.staging should go away

cc @d-llama @aboodman 

', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('8MfQ0wLfkAtOot-3yYlcG', 'Odd/inconsistent behavior when duplicate cookie received', FALSE, 1710163848000.0, 1690493550000.0, 'aboodman', 'Reproduction:

1. Follow BYOB instructions: https://doc.replicache.dev/byob/client-view
2. At local mutations step:
  a. Create local mutation
  b. Change data being returned by pull handler, but leave cookie unchanged at `42`
  c. Observe that changes from pull response are shown temporarily, then reverted

![CleanShot 2024-03-03 at 23 41 38](https://github.com/rocicorp/mono/assets/80388/af22cd89-65ce-4687-8719-9c08994780ba)

What''s happening here I believe is that the pull handler code is honoring the new pull response, but the *persist* handler is not (because cookie unchanged).

The two paths should be the same. Both should ignore an unchanged cookie value. We should also print out an info log when this occurs.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('u5grs7H8DScy9xC2mnQNP', 'Consider not acquiring the room lock when handling pings', TRUE, 1709537365000.0, 1690396255000.0, 'darkgnotic', 'This may prevent disconnects when the RoomDO is busy with queued up lock-holding logic, but otherwise up and running.

#497 ', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('17YvA8-30cH0GAskVj8_o', 'doc: byob: break up push (and perhaps pull?) into smaller steps', FALSE, 1709536134000.0, 1690343549000.0, 'aboodman', 'The db setup is currently broken into two steps: the skeleton of `db.ts` and the implementation of `initDB()`.

This is just a nice simple thing that makes it feel easier to move through the guide, because you do it bite by bite. We should do same thing with push. I could imagine a few steps:

1. Read request body and iterate mutations
2. processMutation
3. handle errors?', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('g-IYpg6Mtpp0Z2H49-AhV', 'reflect dev needs watch mode', FALSE, 1697008646000.0, 1690285725000.0, 'arv', 'I''m punting on watch mode at the moment.

esbuild has a watch mode but it is not clear how that works with returning the result as a string (instead of writing an output file).

We could get the dependency graph from esbuild using the `metafile` build option and set up our own file watchers.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('DgbQiKj8hqPTh15QQK8P_', 'Refactor connection revalidation to use an alarm rather than a cron.  ', TRUE, 1709537365000.0, 1690222687000.0, 'grgbkr', 'Refactor connection revalidation to use an [alarm](https://developers.cloudflare.com/workers/runtime-apis/durable-objects/#how-to-use-the-alarm-handler-method) rather than a cron.  

A few benefits:
1. simplifies project setup (don''t need to create cron, don''t need cron handling in worker forward of cron from worker to AuthDO)
2. we can run the alarm only when there are connections to revalidate, currently cron runs even if there are no connections to revalidate', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('wUwlVinCqtaabAYbgkLiZ', 'reflect publish/upload: Use esbuild conditions', FALSE, 1697008929000.0, 1689773791000.0, 'arv', 'When compiling with esbuild we should declare the conditions so that npm packages work better

```
conditions: ["workerd", "worker", "browser"],
```', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('4tzIZz16uaB3EHyaL44w6', 'Publishing reflect is error prone', TRUE, 1709537364000.0, 1689705548000.0, 'grgbkr', 'To ensure correct package contents, before publishing reflect one needs to run `turbo run build --force` from the root directory of the mono repo.

If one forgets to do this, what is packed is whatever is in the `/out` dir.  If the `--force` flag is not used, then the version variable in the result package is likely to be wrong because turbo build will use cached build output of reflect-shared where the [version](https://github.com/rocicorp/mono/blob/main/packages/reflect-shared/src/version.ts#L4) variable is defined (if its available).  

To make this more robust we should:

1.  Move the build that generates â€œversionâ€ into â€œpackages/reflectâ€ (though we may run in to some circular dep issues before packages/reflect, packages/reflect-client and packages/reflect-server).
2. Add a prepack script to packages/reflect (and probably other published packages in mono), that ensures that build is run from root, and tests are run, before the pack happens.
3. Add check-ambient context to reflect-server (currently only on reflect-client)
4. Document release process and check into mono/packages/reflect
5. Explore replacing rollup with api-extractor

cc @arv ', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('yqygBXzMFKPGv1gk9ByR1', 'PullResponse claims to support `null` cookies responses, but doesn''t', FALSE, 1710026568000.0, 1689230868000.0, 'aboodman', 'Reproduction:

1. Follow BYOB instructions: https://doc.replicache.dev/byob/client-view
  * In Client View step, return cookie `null` not `42`.
2. At local mutations step:
  a. Create local mutation
  b. Observe other data besides local data disappear

![CleanShot 2024-03-03 at 23 32 15](https://github.com/rocicorp/mono/assets/80388/8e0a8ea1-605a-4c77-b8f1-30c26a11254b)
', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('Pm9ARiE3x3zpUTVSjzciR', 'In development mode, `ClientStateNotFound` should delete all local state and refresh', TRUE, 1689277411000.0, 1689228715000.0, 'aboodman', 'Currently, when the server returns `ClientStateNotFound`, Replicache "disables" the client group. This prevents pushes and pulls, writes an error to the console, and from the user''s perspective the app is then wedged.

This made sense when designed because the Replicache protocol doesn''t support deleting client records, so this situation is not supposed to happen.

However, it does happen all the time during development, when deleting server databases is common. And unlike in Replicache 12 just refreshing doesn''t solve it, so this is extremely confusing and frustrating.

I think this blocks releasing Replicache 13, because the behavior is so much worse than 12.

I think that in development mode (when NODE_ENV === "development"), when Replicache receives `ClientStateNotFound` from the server, it should by default:

1. Delete all local state
2. Refresh app
3. Print an error to the console saying what happened

I think this is very similar to other error paths we have already so it should be easy to reuse.

In production, the current behavior should remain as-is.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('mGoIMzuUM0tNhoILcqLsx', 'Reflect (and Replicache?) doesn''t run on Safari 15.3 due to reliance on BroadcastChannel', FALSE, 1709537380000.0, 1689207055000.0, 'aboodman', 'See: https://discord.com/channels/830183651022471199/1055564993883549787/1091435550403219576

Is there any other reason we can''t support older Safari? If not we should at least turn this into a no-op (disabling cross-tab sync on older Safari) but perhaps also bring back the local-storage based workaround.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('pv_64OkkrC2V6b_NSiEdn', 'Create and store a custom token to authenticate the reflect-cli ', FALSE, 1689276172000.0, 1689187465000.0, 'darkgnotic', 'In order to (1) authenticate correctly (e.g. handling token expiration and refresh) and (2) leverage the `firebase` client sdk, the reflect-cli needs to initialize the internal firebase user with `signInWith{Xyz}()` method of the firebase auth library.

The way to do this is to convert the github-based authentication retrieved in the login page to a "[custom token](https://firebase.google.com/docs/auth/admin/create-custom-tokens)". This must be done by privileged, server-side code, and the `user-ensure` function is a logical place to do so.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('nE8sHktFu8R_UNrb9LG0y', 'What to do about REFLECT_AUTH_API_KEY in reflect apps', FALSE, 1702714212000.0, 1689061278000.0, 'arv', 'Right now we need to set the CF secret REFLECT_AUTH_API_KEY. It doesn''t matter what this is to kick the tires... But we need this to allow invoking the REST APIs.

We need to figure out how we want to expose this to reflect apps.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('tAUqy8xX_VBtcdy7eWPAt', 'Make socketOrigin nullable for testing', FALSE, 1709536961000.0, 1688755561000.0, 'aboodman', 'https://discord.com/channels/830183651022471199/1020392595450507304/1126942229542482160

This seems like a reasonable easy idea.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('NLhqu8WtqteqJE1VeTeWv', 'Fix withAdminAuthorization', TRUE, 1688632750000.0, 1688632750000.0, 'arv', 'withAdminAuthorization is currently just a hack. Figure out how to deal with this the right way in Firebase.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('6X2YCjgiwHiZWh2-YbeM6', 'license check request failing fo react native', FALSE, 1687814591000.0, 1687545835000.0, 'grgbkr', 'eh_rob reports on discord https://discord.com/channels/830183651022471199/1121148590723711116/1121148590723711116



Hi,
I''m having an issue with the replicache license status ping from a react-native application. Here''s the response I receive:
```
Error sending license active ping: Error: Got 400 fetching https://replicache-license.herokuapp.com/api/1.0/license/active: [
  {
    "code": "invalid_type",
    "expected": "string",
    "received": "undefined",
    "path": [
      "licenseKey"
    ],
    "message": "Required"
  },
  {
    "code": "invalid_type",
    "expected": "string",
    "received": "undefined",
    "path": [
      "profileID"
    ],
    "message": "Required"
  }
]
```

I have verified that the license key payload is being sent in the request. I can see it in my network request logs, and it matches the body sent from a desktop web browser.
Request body format from logs:
```
{"licenseKey":"xxx123mylicensestring"}
```
and my headers look ok:
```
Accept: application/json
Content-Type: application/json
```

Furthermore, I have no issues with the same key from a desktop web browser react app. I''ve compared the request payload and it''s identical. 

Are there maybe some additional headers the license server is looking for that the mobile application isn''t sending?

I recognize that react-native isn''t "officially" supported, but I''ve had success with the TEST_LICENSE_KEY using this helpful package: https://github.com/Braden1996/react-native-replicache

Thanks in advance for any help!', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('LqxDeWIa4yuHRNl-iM4lM', 'wrangler publish from vercel deploy frequently failing', FALSE, 1687793082000.0, 1687497489000.0, 'grgbkr', '<img width="816" alt="image" src="https://github.com/rocicorp/mono/assets/19158916/8b04e173-e1ae-49ca-8a7d-da36c125ab04">


It succeed when I publish from the command line from my machine.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('tZNGGFnoeAlCgEZoimkt2', 'Text exiting ', FALSE, 1709537394000.0, 1687496980000.0, 'aboodman', 'Replicache/Reflect are going to need a recommended solution for text editing.

Perhaps this is just yjs. However it seems worth understanding alternatives.

Does OT make more sense since we are server authoritative? https://twitter.com/justinfagnani/status/1664183367586451458?s=46&t=oIKhjpiwUquk0cgppqu9Ng

codeMirror has a design that seems very compatible with us (immutability, transactions, etc): https://codemirror.net/docs/guide/


https://twitter.com/ekzhang1/status/1672043743115968516?s=46&t=oIKhjpiwUquk0cgppqu9Ng', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('uyKfYI0PJ-odPWeyENJee', 'reflect 0.27.1 logs its version as 0.0.0', FALSE, 1687524570000.0, 1687477622000.0, 'grgbkr', 'The version is logged incorrectly by both the client and server.

<img width="1484" alt="image" src="https://github.com/rocicorp/mono/assets/19158916/8990acc0-3ada-45c2-af92-b8d560ddf4c9">
', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('40G7H1-7njAN3flAvj-Vm', 'replicache 13.0.0-beta.1 ChunkNotFoundError when calling tx.get on an already deleted key', FALSE, 1689241106000.0, 1687203625000.0, 'grgbkr', 'Customer report of reproducible ChunkNotFoundError in replicache 13.0.0-beta.1.
https://discord.com/channels/830183651022471199/1119036709007540257/1119036709007540257

Videos with details of repro:
https://drive.google.com/file/d/1B8i3DeaF6rWOp7aGZIrLdJEfnYhcNyAJ/view?usp=drive_link
https://drive.google.com/file/d/1X8p_LlEu4JvRkqE1618NlrCewhlDhZBO/view?usp=drive_link

Stack trace:

```
Uncaught (in promise) ChunkNotFoundError: Chunk not found 5a3cf77d13ac41a1ab2e764dda4bf06e000000002056
    at mustGetChunk (webpack-internal:///(app-client)/./node_modules/replicache/out/replicache.js:987:9)
    at async BTreeWrite.getNode (webpack-internal:///(app-client)/./node_modules/replicache/out/replicache.js:2267:19)
    at async findLeaf (webpack-internal:///(app-client)/./node_modules/replicache/out/replicache.js:1803:16)
    at async BTreeWrite.get (webpack-internal:///(app-client)/./node_modules/replicache/out/replicache.js:2288:18)
    at async getInternal (webpack-internal:///(app-client)/./node_modules/@rocicorp/rails/out/index.js:92:17)
    at async getImpl (webpack-internal:///(app-client)/./node_modules/@rocicorp/rails/out/index.js:49:12)
    at async getBlockOrCache (webpack-internal:///(app-client)/./src/replicache/schema/repo-space/block/mutators/re-hash.ts:50:23)
    at async eval (webpack-internal:///(app-client)/./src/replicache/schema/repo-space/block/mutators/re-hash.ts:59:13)
    at async Promise.all (index 0)
    at async eval (webpack-internal:///(app-client)/./src/replicache/schema/repo-space/block/mutators/re-hash.ts:58:9)
    at async Promise.all (index 0)
    at async rehashBlockPaths (webpack-internal:///(app-client)/./src/replicache/schema/repo-space/block/mutators/re-hash.ts:57:5)
    at async deleteBlockAndNestedBlocks (webpack-internal:///(app-client)/./src/replicache/schema/repo-space/block/mutators/crud.ts:331:9)
    at async eval (webpack-internal:///(app-client)/./src/replicache/schema/repo-space/block/mutators/crud.ts:340:13)
    at async Promise.all (index 0)
    at async deleteBlockAndNestedBlocks (webpack-internal:///(app-client)/./src/replicache/schema/repo-space/block/mutators/crud.ts:338:5)
    at async deleteBlock (webpack-internal:///(app-client)/./src/replicache/schema/repo-space/block/mutators/crud.ts:259:5)
    at async deleteBlocks (webpack-internal:///(app-client)/./src/replicache/schema/repo-space/block/mutators/crud.ts:300:9)
    at async eval (webpack-internal:///(app-client)/./node_modules/replicache/out/replicache.js:8890:24)
    at async using (webpack-internal:///(app-client)/./node_modules/replicache/out/replicache.js:3189:12)
    at async Object.eval [as deleteBlocks] (webpack-internal:///(app-client)/./node_modules/replicache/out/replicache.js:8853:29)
    at async onDelete (webpack-internal:///(app-client)/./src/app/(dash)/[team-slug]/(repo)/[repo-slug]/content/components/sidebar-tabs/explorer/context-menu-actions.tsx:104:13)
```

cc @arv 

The users description of the mutator that reproduces this first deletes a key and then tries to read that key in the same mutator transaction.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('sOyiw5y37HpoZt4VhLydX', 'Reverse scan', FALSE, 1709536236000.0, 1687162547000.0, 'arv', 'The APIs for BTrees generally includes a reverse scan. It should not be too hard to implement.

Strawman API. ScanOptions gets a `reverse` flag.

Internally it might make sense to separate into a reverseScan. I haven''t thought too hard about it yet.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('xJ10iyYDC5StBYPnkuEyz', 'RoomDO overloaded by single user scribling', FALSE, 1709537409000.0, 1686854010000.0, 'grgbkr', 'Monday reports a reproducible means of causing all clients connected to a room to disconnect.

<img width="991" alt="image" src="https://github.com/rocicorp/mono/assets/19158916/5b578906-5e75-4488-8f66-cf3d5856107c">
https://drive.google.com/file/d/1IhKSMNw0P1kUAy3i8MlgYvn9BG6iOhwy/view

Looking at the logs, I see that the RoomDO is overloaded.  
<img width="1675" alt="image" src="https://github.com/rocicorp/mono/assets/19158916/2073eafd-2840-4fc1-a342-c7c647302854">

It is concerning that a single user drawing rapidly can overload a DO. 

Note I can reproduce with drawing but not with just moving the cursor.  I''ve asked Noam to share the code for the mutators involved in drawing.  

cc @aboodman @d-llama ', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('LVpMPrrTMZ9SQBqf_3Yeu', 'Throw uncaught exceptions in addition to logging at log level error ', TRUE, 1686850051000.0, 1686850024000.0, 'grgbkr', 'In replicache/reflect we tend to catch errors in background processes and if they are not an expected error case, log them at log level error.  

This results in these unexpected error cases not showing up in customer error reporting systems like Sentry.

One example is errors in subscribe bodies.  We should audit and fix everywhere we do this.

Background: 
https://twitter.com/aboodman/status/1668590555880890368

<img width="960" alt="image" src="https://github.com/rocicorp/mono/assets/19158916/6e1f2e7b-f240-4624-baf9-f82f7a2071d5">


cc @aboodman @arv ', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('lx-VPi5fzZ_jTQ5GPxZvb', 'No need to depende on tslib', FALSE, 1686840666000.0, 1686824620000.0, 'arv', 'I hope

https://github.com/rocicorp/mono/blob/41874645c1e3fcc7843ba4dea2e3028331b875e0/mirror/mirror-server/package.json#L30', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('2eEq-R1RACTLoNgkHPJBb', 'After disconnecting due to Reflect.close call, we still get ConnectTimeout', FALSE, 1709537461000.0, 1686605241000.0, 'grgbkr', 'This also results in the error log `disconnect() called while disconnected`.

<img width="1432" alt="image" src="https://github.com/rocicorp/mono/assets/19158916/eb44d671-d3ec-4501-9b88-88364c709b7e">
', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('KlOJuUZboYv6Kf42Eusfv', 'Figure out what to do with reflect-server/cli bin', TRUE, 1709537486000.0, 1686577181000.0, 'arv', '`@rocicorp/reflect-server` has a binary in it. What do we want to do with that?', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('hXkFidPZMhENhrjGCf8mi', 'remove version from reflect-server', FALSE, 1697008956000.0, 1686564031000.0, 'arv', 'https://github.com/rocicorp/mono/blob/df4a4952757f3692d913fc26e941c49db9846746/packages/reflect-server/src/util/version.ts#L1-L3', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('8_3okmjon6nIUdLz2E4TX', 'Map "util" to "node:util" in reflect-server package', FALSE, 1686841267000.0, 1686087231000.0, 'darkgnotic', '              I think we can map "util" to "node:util" and it will work with cf workers... Maybe file an issue to follow up on this in the future.

_Originally posted by @arv in https://github.com/rocicorp/mono/pull/568#discussion_r1219561850_
            ', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('fGYBYI7icLmn87kHdScAk', 'Add timestamps to authentication and invalidation protocols', TRUE, 1709537485000.0, 1685638992000.0, 'darkgnotic', 'Currently, the AuthDO relies on locking and request ordering to avoid one direction of authenticate-invalidate race condition (detailed [here](https://www.notion.so/replicache/Invalidating-Auth-732e9f9abb6a4806b5461c87dfde580f?pvs=4#fc93bf38e9f44bc48aa7d40dd9a678a1)). This can lead to unbounded contention since the `authHandler` can do arbitrary I/O, including calling out to external systems.

The way this race is generally handled in the industry is with timestamps. Each auth token encodes a timestamp, and an invalidation sets a watermark for the earliest valid auth token. (This is how you get logged out of Google properties when you change your Google password). For example, the [decoded contents of a Firebase auth token](https://firebase.google.com/docs/reference/admin/node/firebase-admin.auth.decodedidtoken) includes an auth_time. 

In reflect, authentication is opaque to us and we rely on the customer''s `authHandler` and invalidation request to dictate validity of authentications. As such, we can improve our guarantees without relying on locking and request ordering by adding timestamps to both the response of the `authHandler` and the payload of invalidation requests.

Note that this does involve a protocol change to the customer API, so it should be done in a backwards-compatible manner.

@grgbkr @aboodman  ', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('uR4L2c9D7aoo7a2WeclAH', 'Miscellaneous logging cleanups', TRUE, 1685127716000.0, 1685127702000.0, 'aboodman', 'I tried to do some, but they didn''t work. See:

https://github.com/rocicorp/mono/pull/551#issuecomment-1564803873

Need to redo this correctly. Also while we''re at it, remove all the "id" suffixes from log attributes:

https://rocicorp.slack.com/archives/C013XFG80JC/p1685123287214549', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('4Q5SGl4eAHqkV2xCIsjmX', 'Unify reflect npm packages', FALSE, 1686240532000.0, 1685004872000.0, 'arv', 'Today we have two npm packages:

| Name | NPM Package name | Code Location |
|--------|--------|--------|
| Reflect Client | `@rocicorp/reflect` | packages/reflect |
| Reflect Server | `@rocicorp/reflect-server` | packages/reflect-server | 

The idea is to have a single npm package called `@rocicorp/reflect`. This package will export 2 modules called `@rocicorp/reflect/client` and `@rocicorp/reflect/server`


| Name | NPM Package name and subpath |
|--------|--------|
| Reflect Client | `@rocicorp/reflect/client` |
| Reflect Server | `@rocicorp/reflect/server` |

To make this easier to maintain I suggest we move the current `packages/reflect` source to `packages/reflect-client`.

We treat `packages/reflect-client` and `packages/reflect-server` as internal packages.

Then we create a new `packages/reflect` which imports from the two internal packages.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('7gVsEKOmCjXwNBaAwmA_c', 'Cleanup Reflect connection loop', TRUE, 1684868791000.0, 1684868777000.0, 'aboodman', 'The code is hard to follow currently and it''s very important that this be clean and understandable.

some thoughts:

https://www.notion.so/replicache/Offline-error-in-homepage-c3b040663a6a43febc81f998dc569d23?pvs=4#d5dd0bd1a3434ab4ab20cd92d4bb94f9', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('KkwyzF_g9DQ3AEMx501MW', 'First-class pattern/solution for presence', FALSE, 1709771730000.0, 1684747154000.0, 'aboodman', 'There should be a way to implement presence that works easily and well.

Goals:
- associate state with clients/users that are connected
- automatically delete this state when clients disconnect
- doesn''t get confused by mutation recovery
- integrates naturally with persisted state
- don''t bother persisting this state locally
- don''t bother resending changes related to this state when reconnecting from offline

Background docs:
https://www.notion.so/replicache/Presence-problem-71423904be8b4bc39e030084fdb5f890
', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('AkKLbzDv6fSdL-NJx0eCL', 'Create `options` every time DO is instantiated.', TRUE, 1684744265000.0, 1684664790000.0, 'aboodman', 'Caching across instantiations creates problems. See: https://rocicorp.slack.com/archives/C013XFG80JC/p1684615253955599', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('LAJ4XiFaHxCFur9JKjurC', 'Update embedded wrangler to v3', TRUE, 1684653023000.0, 1684653023000.0, 'aboodman', NULL, NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('aBvSRgSF6g3ppw7fb_1kl', 'dx: monday requests online state change to indicate the error / reason for disconnect', TRUE, 1684481018000.0, 1684481009000.0, 'aboodman', NULL, NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('-pwHLdQfYmU6LhWG0GqBf', 'dx: either start out ''online'' or else add a first-class ''connecting'' state', TRUE, 1685448047000.0, 1684479871000.0, 'aboodman', 'We hit this with reflect.net and Monday just reported it to me.

Currently Reflect''s `online` state starts out `false` which is pedantically true: we don''t yet know whether we''re online so from Reflect''s POV we''re offline.

But from a ux pov this is pretty weird looking. It looks as if something is broken -- you load the app and then see an indicator that it''s offline.

The easy thing would be to just default the other way -- (a) Reflect starts out online. A more technically correct and flexible thing would be to (b) add an ''unknown'' state which is used only at startup, until the first connect attempt completes.

Note that #498 would also solve this in the sense that user could just wait until `onFirstSync` but (a) this makes it harder to implement the online status UI for our users because they have to string together two APIs, and (b) it unnecessarily delays being able to tell the user the online status (because have to wait for actual pokes to be downloaded and played).

I think we should do (b) if it''s not too hard but (a) is also a reasonable solution.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('7oMtDqYIaFq2FauBqU3nR', 'clients that dont send messages get closed as idle when they shouldnt', TRUE, 1684744361000.0, 1684271174000.0, 'grgbkr', 'I misunderstood how ping works.  I thought the client sent a ping every 5 seconds.  Actually the client only sends a ping if its been 5 seconds since it sent or received a message.  

Due to this misunderstanding this change https://github.com/rocicorp/mono/commit/22ac5ffd742525586ce2ab5edefc7af43dda86c5 is causing clients that are only receiving messages but not sending any to be disconnected after 10 seconds.  ', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('Jg_Lc8p-Qo5DJARqYAZa-', 'processPending/processRoom/processFrame naming is poor', TRUE, 1684744380000.0, 1684041928000.0, 'grgbkr', 'I think at one point these made more sense, but the code and names no longer match.

1. `processFrame` does not process one frame
2. What is the distinction be `processPending` (for the roomDO) and `processRoom

Also their is a lot of redundancy in the testing of processPending/processRoom/processFrame.

processPending''s responsibilities are

1. identifying idle connections and closing them
2. selecting the next chunk of mutations to process
3. calling processRoom
4. sending pokes

processRoom''s responsibilities are:

1. creating fast-forward pokes for any clients that are behind
2. calling processFrame

processFrame''s responsibilities are:

1. processing mutations
2. running disconnectHandler
3. creating poke messages for the patches from above mutations and disconnectHandler


We should consider refactoring and renaming things here, possibly something like:

1. processPending is renamed to `processTurn`
2. processTurn calls in turn: `closeIdleClients`, (possibly in the future `processConnects` if we add support for onConnect handlers), `fastForwardStaleClients`, `processMutations`, `processDisconnects` and `sendPokes`.
3. processRoom and processFrame go away.

This refactor is easy except for the tests... its going to be a lot of work to rework the tests.
', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('mcyn0Mc1lRIh88TtaUxgA', 'RFE: We probably need a "connecting..." state too', TRUE, 1685448060000.0, 1683939602000.0, 'aboodman', 'While doing the puzzle demo, I found myself wanting to distinguish to the user between whether we were connecting or disconnected.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('X8Y0cgnbFJhjYHBeHv07A', 'Replicache CTA out of date', FALSE, 1684744418000.0, 1683858614000.0, 'aboodman', 'The Replicache CTA say:

<img width="1082" alt="CleanShot 2023-05-11 at 16 27 51@2x" src="https://github.com/rocicorp/mono/assets/80388/d77c0b8f-ad26-402f-82a1-689afc06527f">

However:

a) This is no longer the easiest way to get started. Our docs now start with an interactive tutorial: https://doc.replicache.dev/, which is the easiest way. 

b) In conflict with the copy, the docs (https://doc.replicache.dev/) no longer have more details on setting up the starter app.

I think the CTA should change to something like:

_You can learn the basics of Replicache in fifteen minutes using our interactive tutorial._

_**Start Learning**_', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('STzcA1Q0z31SUm6sYZMWi', 'RFE: `onFirstSync`', TRUE, 1683837800000.0, 1683837790000.0, 'aboodman', 'Users often want to either delay displaying state until after the first sync, or show a progress bar to let the user know that an update is happening.

Two examples:

* In the ALIVE demo, we disconnect when switching away from the tab and reconnect when switching back. When switching back, we display cached data (either persisted, or in memory, doesn''t matter) and then a second later first sync happens and the puzzle jumps to a different state -- typically *very* different because the bots have been running around moving things. It would be better ux to just delay display until we get that first sync.

* In Notion, and many other collaborative apps, it''s common to display the cached state to allow fast loads, but then hvae a spinner in the corner to let the user know that a background refresh is happening.

For both of these we need some kind of API to let the client know it''s gotten up to date with the server.

Of course the server is constantly changing, so how do we define "first sync". What I''m thinking is that this event would fire after the client receives the fast-forward pokes. That is, after it catches up to the state that the server was at at the moment it connected.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('K305J2SgfJzfnolGfEEtt', 'Monday metrics have a very large number of pings that never connect.', TRUE, 1684744480000.0, 1683764067000.0, 'aboodman', 'We finally got our metrics deployed to Monday and it is currently saying that something like 12% of users do not connect:

https://app.datadoghq.com/dashboard/vm5-ce6-p67/reflect-client-metrics?fullscreen_end_ts=1683764009111&fullscreen_paused=false&fullscreen_section=overview&fullscreen_start_ts=1683760409111&fullscreen_widget=8840315099850438&tpl_var_host%5B0%5D=app.workcanvas.com&tpl_var_service%5B0%5D=canvas-metrics-service&from_ts=1683754105220&to_ts=1683757705220&live=true

![CleanShot 2023-05-10 at 14 14 01@2x](https://github.com/rocicorp/mono/assets/80388/917a9bf9-98aa-4390-9b01-f05583f9ddac)

I dont think this can possibly be true, but we need to understand what''s going on here.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('rDkzXqfJWPsX2mLK1iDCU', 'Add a auth_time server metric.', TRUE, 1685039985000.0, 1683748191000.0, 'aboodman', 'Sadly this will be out very first server-side metric, so it will require some infrastructure to be built/ported from the client.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('Dj6LEOA9zuXzoV6c9klaa', 'Fine-grained write auth', FALSE, 1709537612000.0, 1683342179000.0, 'aboodman', 'Currently, Reflect supports authentication (who is the currently logged in user) and coarse-grained authorization (can they access this room), but not fine grained authorization (can they access this field? can they modify this field?).

When @grgbkr originally designed auth, there was a feature for the auth handler to return additional "context" which could be exposed to mutators and used for write-time authorization decisions.

Our customers frequently ask for the ability to have "admins" for certain rooms, who are the only ones allowed to edit. This feature could be used for that. The auth handler could return an additional `isAdmin` flag which the mutator used 

For example, one thing you could do is return a bit which says whether the user is an admin. This bit could then be used by mutators to decide how to function.

This idea is sketched here:

https://www.notion.so/replicache/Reflect-Authentication-Authorization-and-Security-bf2b1a2e31844efca73c4aa453fdecdb?pvs=4#caf5a6970f6345b99170dbb6f7ec0a9e

As a starting point, even exposing the `userID` to the `WriteTransaction` would be useful, as it would enable features like editing your own content but not other users''.

We would have to decide whether to add yet another field to Replicache''s `WriteTransaction` that really only makes sense on the server, or to extend a new `ReflectWriteTransaction` that adds this field.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('7rq_RVRKkIXMkuWfAUgQo', 'Monday: dramatic increase in "accepting connection to send error"', FALSE, 1683579930000.0, 1683227016000.0, 'aboodman', 'We have a sharp increase in this error starting in reflect-server@0.23.0-beta.0.

See analysis here:

https://replicache.notion.site/replicache/Monday-response-bda8cdf4514b44869487a02424090e18

and original report/context here:

https://discord.com/channels/830183651022471199/1055564082087989299/1103256991075422289', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('AkjVslOD_m3m-18wS4cuE', 'Monday error: "Durable Object exceeded its memory limit"', FALSE, 1683574984000.0, 1683226901000.0, 'aboodman', 'This appeared in reflect-server@0.23.0-beta.0. It is happening in authdo. Unfortunately there''s no stack so it''s hard to know where exactly it''s coming from but the error message gives some hints.

Details here:

https://replicache.notion.site/Monday-response-bda8cdf4514b44869487a02424090e18

Original report and context here:

https://discord.com/channels/830183651022471199/1055564082087989299/1103256991075422289

', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('kPgwDYf2H1N-ancnBJelE', 'Disconnect due to switch tabs is not reported to `onOnlineChange`', FALSE, 1683203232000.0, 1682921147000.0, 'aboodman', 'When you switch to a different tab for 10s, we disconnect properly and reconnect on switch back, but we don''t report this to `onOnlineChange`.

It''s important that this work because it''s a common pattern to destroy client state in `onDisconect` and recreate it in `onOnlineChange`.

All reasons for disconnect should be reported to `onOnlineChange` with the one exception that we should retry reconnect one time before reporting the change to the app.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('eXY6YjHp1QVbQ00wEzGf_', 'Reflect enters a tight reconnection loop when going offline', FALSE, 1683100909000.0, 1682920776000.0, 'aboodman', 'When you are in a reflect app and go offline (ie by killing the server), the client enters a tight loop trying to reconnect without waiting in between attempts.

See https://www.notion.so/replicache/Offline-error-in-homepage-c3b040663a6a43febc81f998dc569d23 for details.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('SRq5IFuVpz0_tczKW2Wup', 'Announcement burndown', FALSE, 1684869621000.0, 1681849319000.0, 'aboodman', 'https://www.notion.so/replicache/Announce-Burndown-98c1e453429145f7af7913cfb3993a61', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('HUUC4Hxvx78QSMFt13C21', 'Remove allowUnconfirmedWrites option', TRUE, 1709537612000.0, 1681411313000.0, 'arv', 'We should not expose this in the API. It is not safe?', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('8SwVfA6MPOWLs44o8HETa', 'Hook up metrics to alive demo and "how it works" demos', FALSE, 1681819178000.0, 1681243253000.0, 'aboodman', NULL, NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('tlbJPIoPQUVH08OyCz1r6', 'Don''t load reflect instances for "how it works" demos until they are scrolled into frame', FALSE, 1681172302000.0, 1680919652000.0, 'aboodman', 'I am pretty sure that they are delaying startup of "alive" demo.

You can confirm this improved startup by looking at the console output:

<img width="545" alt="CleanShot 2023-04-07 at 16 06 54@2x" src="https://user-images.githubusercontent.com/80388/230698612-224a78b4-1468-4b5b-bc7c-4ec21b1eb79b.png">
', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('bcHDVOcyyXBwF2PQg8kmp', 'Do something about "offline" marketing', FALSE, 1681849256000.0, 1680638826000.0, 'aboodman', 'Currently if you go offline and draw a bunch it will be slow to reconect due to #384. We either need to soften the marketing (coming soon) of this feature or else change the impl in the demo sufficiently so that it works.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('Gq69iFiLUO9DqZ0LEMug8', 'authHandler should allow null to mean not authenticated', FALSE, 1681811411000.0, 1680568588000.0, 'aboodman', 'Right now the signature is that it throws which is a little non-idiomatic (throwing should be for exceptional circumstances).', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('1A3uLjcAPJnZChekrNVXC', 'API nit: warn on potential deadlock', TRUE, 1684744576000.0, 1680567962000.0, 'aboodman', 'It''s an easy footgun to open a transaction nested inside another transaction. When this happens neither will proceed and we deadlock. Here''s an example of this happening to a user:

https://discord.com/channels/830183651022471199/1092315354606350386/1092561231753261187

It would be nice to detect this and warn the user.

I do not think we can do the obvious thing, which is to detect user opening a transaction while another is running because it is actually totally legitimate to do that. The problem is not:

- open tx1
- open tx2

It''s that in addition to above, tx1 waiting to close for tx2 to close. In that case, we deadlock. But I do not think that is possible for us to detect because we have no way to know *why* tx1 isn''t closing.

Instead, what I think we could do is: *if* the user opens a transaction *and* there is already running, then start a timer for say 5 seconds. If the lock hasn''t released on 5s, then tell the user there was a deadlock and that they may be executing nested transactions.

---

We may also want to improve the documentation of `mutators` and `query` to make it clear that these are exclusive of each other.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('_rW2bAe2HIoIcPhGRuyOL', 'User report in Replicache 12.0.1: replicache hangs with particular mutator', FALSE, 1680568042000.0, 1680548661000.0, 'aboodman', 'This original report is: https://discord.com/channels/830183651022471199/1091153888704475226/1091153888704475226

We have established a private Discord channel here: https://discord.com/channels/830183651022471199/1092315354606350386

Sleanshot is not a paying customer but has the potential to be. User has put up a repro of the bug at https://app-dev.sleanshot.com, along with instructions:

<img width="1097" alt="CleanShot 2023-04-03 at 09 03 00@2x" src="https://user-images.githubusercontent.com/80388/229602603-373e84e3-e821-446f-bc1d-0296ada57775.png">

You can''t access this URL yet until customer adds you to the account. I have investigated this using the minified build briefly and the hang is waiting on RwLock in LazyStore for some reason.

User is currently trying to facilitate further debugging.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('XlJCP3zurxQelxQz0MLr7', 'Questions about connection loop', FALSE, 1681480897000.0, 1680336011000.0, 'aboodman', '<s>First, it seems like the ping timeout logic is not correct.

https://github.com/rocicorp/mono/blob/main/packages/reflect/src/client/reflect.ts#L883:

```
// When connected we wait for whatever happens first out of:
// - After PING_INTERVAL_MS we send a ping
// - We get disconnected
// - We get a message
// - The tab becomes hidden (with a delay)
```

ISSUE 1: This seems like it will result in the ping continually getting pushed back 2s on each new message, which might result in us not sending pings for arbitrarily long when the socket is busy. I think I have seen us disconnect due to ping timeout under busy conditions frequently.</s>

This one was incorrect -- I forgot that it''s the client that measures ping timeouts, not the server.

===

Stepping back one level, I''m not sure why we wait for a message in the first place? We [await `#nextMessageResolver`](https://github.com/rocicorp/mono/blob/main/packages/reflect/src/client/reflect.ts#L898).

In the case it fulfills, we do nothing and the loop repeats (only pushing back the ping). In the case it rejects, we jump to the catch block: https://github.com/rocicorp/mono/blob/main/packages/reflect/src/client/reflect.ts#L922.

There are two cases where `#nextMessageResolver` rejects:
1. handling an invalid server->client message: https://github.com/rocicorp/mono/blob/main/packages/reflect/src/client/reflect.ts#L449. Note that in this case we do *not* disconnect -- the socket stays connected.
2. handling an error message from server: https://github.com/rocicorp/mono/blob/main/packages/reflect/src/client/reflect.ts#L531. In this case we do disconnect.

So that means in the catch block, in the case where `#nextMessageResolver` rejected:

- The error count gets incremented either way: https://github.com/rocicorp/mono/blob/main/packages/reflect/src/client/reflect.ts#L946
- We set ourselves offline and sleep either way: https://github.com/rocicorp/mono/blob/main/packages/reflect/src/client/reflect.ts#L955, even though we did not necessarily disconnect

ISSUE 2: This seems inconsistent to me. If we are telling the app that the connection is offline, we should actually be offline. Also we should not be receiving messages, but I think in this case we continue to.

===

While a ping is running the connection loop is not waiting on `nextMessageResolver`. But messages do continue to be processed during a ping.

ISSUE 3: So this means that in the special case of an an invalid message being received during a ping, it will *not* enter the catch block and have the error handling behavior.

<s>ISSUE 4: Actually we don''t listen to visibilityWatcher during ping either. So if it happens while a ping is outstanding we will just miss it and stay connected: https://github.com/rocicorp/mono/blob/main/packages/reflect/src/client/reflect.ts#L898</s> -- this one was also incorrect.

===

Summing up it seems like:

- We could get rid of `#nextMessageResolver` completely. I''m not sure what purpose it has, and I think it is causing many other issues in here. <s>This would immediately fix ISSUE 1.</s> It would also fix ISSUE 2 and ISSUE 3 in that an invalid server message received would not ever trigger the catch block, which is consistent with not closing the connection.
<s>- I think that handling visibility change and disconnecting should happen outside the connection loop. That way it will happen even when a ping is outstanding. This would fix ISSUE 4.</s>', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('UpKiMRI46OhfsrBm1hSXv', 'Remove `options.metricsIntervalMs`', FALSE, 1680341645000.0, 1680312752000.0, 'aboodman', 'It was added by mistake.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('iEvVSbWm1EYBpeeEeVX0p', 'Missing commit in refresh', FALSE, 1680598128000.0, 1680255857000.0, 'arv', 'This is not the reason for ChunkNotFound but it is wrong.

https://github.com/rocicorp/mono/blob/3263520a6de4700ece9500ae28e3507c058c4811/packages/replicache/src/persist/refresh.ts#L235', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('D-G0Q7myhKbKFr5E5UPDS', 'Log/metric Reflect client version', FALSE, 1680341662000.0, 1679819306000.0, 'aboodman', 'It would be super useful when debugging to know for sure what version a particular client is on. We should log out at info level the version of Reflect at startup. We should also likely include this information in the tags for the two metrics.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('yz1Z9BASQqi0BW2ZxaEMv', 'Connectivity: canary request', FALSE, 1684606603000.0, 1679818157000.0, 'aboodman', 'The error handling in the browsers on socket requests is not as good as normal requests. This can make it hard to understand why sockets are failing. For example, we recently saw socket connections failing due to ssl errors, but nothing was reported to log. When we do an https request to same host, then it fails with a clear ssl error.

For this reason I think we should do a "canary" https request in parallel with the socket connection and include whether it failed or succeeded in the logs and in the metrics.

For the metrics, it seems like the best thing would be to include it as a "tag" in the last_connect_error metric. That way we can easily compute how many of a particular error type have a canary request that is succeeding/failing.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('64kYNhcr9MRaLzLWAWwRY', 'Mutators thrown out when running with --local --persist', FALSE, 1680164279000.0, 1679610623000.0, 'jesseditson', 'On 771395cf68379850e1b3ec03ce81cf0436799506 (but notably not on the the latest npm versions), when running `npx wrangler --local --persist`, mutators are successfully pushed to the server, but the server never runs them (infinitely prints `running 0 of 1 pending mutations`.

- [x] Repro in reflect-todo
- [x] Verify on latest HEAD of mono', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('swvXBTdDXne3CB_d7L1zi', 'ChunkNotFound', FALSE, 1681258385000.0, 1679607456000.0, 'arv', 'Both Monday.com and Substack have reported this.

Here is a video showing this for substack:

https://share.cleanshot.com/3rQsRdk2

Here is the Discord thread: https://discord.com/channels/830183651022471199/1055564993883549787/1088529642463449148

From the video we see this:

<img width="641" alt="image" src="https://user-images.githubusercontent.com/45845/227367816-bcb771b2-16a2-4bc4-be3a-bdfe8a2c22d4.png">

There is one call to `visitCommit` in refresh: 

https://github.com/rocicorp/mono/blob/c882ec2da6719927e564f7249187e1c7bd716682/packages/replicache/src/persist/refresh.ts#L116-L121', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('WMdjOTB7dm6bpaN4gJVND', 'fastForward sending deletes on sync from null', FALSE, 1679593702000.0, 1679552888000.0, 'aboodman', 'Our fast-forward impl in Reflect sends deletes on sync from null.  These deletes are not needed since a client syncing from null has no state.  The impact is that in long-lived rooms the number of deletes just builds up and up forever :(.

See: https://rocicorp.slack.com/archives/C013XFG80JC/p1679552209080409', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('EAZ1GpjTKqMcMDuvn_Tcb', 'API nit: Remove `allowUnconfirmedWrites` and related code. It''s no longer needed.', FALSE, 1683333565000.0, 1679466100000.0, 'aboodman', '@grgbkr is there any reason not to do this right away?', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('s3Pnkr55mei8sd-aBlx-u', 'valita when deployed on nextjs results in runtime error', FALSE, 1680044558000.0, 1679428252000.0, 'grgbkr', 'This line of code
https://github.com/badrap/valita/blob/a74795390ef45c34178aa82ff2f38211d6c906dd/src/index.ts#L1360
```
class NeverType extends Type<never> {
```
ends up being compiled to:
```
, F = new class extends (null) {
```
Which results in the error:
```
main-e47071de22fcf081.js:1 TypeError: Super constructor null of anonymous class is not a constructor
    at new <anonymous> (124-07691b1e95823a08.js:1:143075)
    at 4958 (124-07691b1e95823a08.js:1:143037)
    at r (webpack-24780b5468e42e63.js:1:148)
    at 3551 ([id]-e3b149a8aa989bc3.js:1:302)
    at r (webpack-24780b5468e42e63.js:1:148)
    at [id]-e3b149a8aa989bc3.js:1:156
    at main-e47071de22fcf081.js:1:30999
J @ main-e47071de22fcf081.js:1
12:43:52.820 
```', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('THdq73qLJ7wht0mX6bx33', 'Setup DNS for reflect.net', FALSE, 1680637390000.0, 1679348452000.0, 'aboodman', 'reflect.net is currently under my namecheap account, as are roci.dev, replicache.dev, and friends.

Let''s setup reflect.net but not touch replicache.dev and roci.dev right now.

We should eventually transfer all of them into cloudflare, but I''m not sure how long that will take.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('WsyA82AUCyjc6Ttf8LqmX', 'Duplicate code in reflect npm module', TRUE, 1709537611000.0, 1679303742000.0, 'arv', 'I need to verify this but...

reflect depends on replicache which has bundled all of its source. replicache bundles `@rocicorp/logger` (and many other things, including `shared`). reflect also includes `@rocicorp/logger` so we end up with two copies of the code in the bundled reflect.

- [ ] Add reflect to the bundle size dashboard
- [x] Verify that we are duplicating the code
- [ ] Do not duplicate the code. I''m not sure what the best way is but reflect should not depend on the bundled version of replicache. ', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('9hFg7azUPgM5Zq-H39ygr', 'Reflect announcement blog post', FALSE, 1681849183000.0, 1679236811000.0, 'aboodman', NULL, NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('LW3sBrJffl51JCGIHx5Lj', 'RFE: `tx.update()`', FALSE, 1709536330000.0, 1679234721000.0, 'aboodman', 'The following pattern is very, very common in Replicache apps:

```
mutators: {
  updateThing: async (tx: WriteTransaction, update: Partial<Thing>) {
    const prev = await tx.get(update.id);
    await tx.put(update.id, {...prev, ...update});
  }
}
```

We chould automate by providing a helper:

```
mutators: {
  updateThing: async (tx: WriteTransaction, update: Partial<Thing>) {
    await tx.update(update.id, update);
  }
}
```

Perhaps there could also be some options to create if item not already present.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('h19U5VVL9VG6VPHx7aqEi', 'invalidateAll etc requires 2 roundtrips', FALSE, 1680568644000.0, 1678698103000.0, 'arv', 'When we use the HTTP endpoint to invalidateAll (also cron I believe) the clients get disconnected.... Aaron outlined the flow in discord:

1. `new Reflect()` on client
2. Client calls `auth()` to get a token
3. Server calls `authHandler` to verify token
4. `authHandler` returns the user ID
5. `<connection success>`
6. Your server calls `invalidate*()`
7. server closes relevant connections
8. Client immediately tries to reconnect, using old tokens
9. Server calls `authHandler`
10. `authHandler` throws
11. Server returns auth error to client
12. Client receives auth error and calls `auth()` to get a fresh token
13. Client reconnects, sending new token
14. Server calls `authHandler` to verify token
15. `authHandler` returns `userID`
16. `<connection success>`

In step 7 when the server closes the relevant connection we can send a close code. If that close code is `AuthInvalidated` we call `auth` locally before trying to reconnect. Then the flow would be:

1. `new Reflect()` on client
2. Client calls `auth()` to get a token
3. Server calls `authHandler` to verify token
4. `authHandler` returns the user ID
5. `<connection success>`
6. Your server calls `invalidate*()`
7. server closes relevant connections with `AuthInvalidated` close code.
8. Client receives auth error and calls `auth()` to get a fresh token
9. Client reconnects, sending new token
10. Server calls `authHandler` to verify token
11. `authHandler` returns `userID`
12. `<connection success>`

Reducing the algorithm by 4 steps and removing the extra connection attempt.

The only question is if it is expected that the auth tokens returned in step 2 are generally revoked when a room auth is invalidated. If they are generally not revoked (cron) then there is no need to call `auth` on the client after the disconnect. 
', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('GgUGVzGishusa7eiVsVsr', 'ReadTransaction::isEmpty not reactive', FALSE, 1678735159000.0, 1678667440000.0, 'aboodman', '```ts
rep.subscribe(async tx => tx.isEmpty())
```

doesn''t re-fire when the emptyness of Replicache changes.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('ojwQdGdrvD6sxgoe0tXWq', 'Do we want to ensure that the userID returned by `authHandler` matches what was passed into `Reflect` ?', FALSE, 1683402496000.0, 1678579070000.0, 'aboodman', 'Right now there''s no such checking:

https://github.com/rocicorp/mono/blob/main/packages/reflect-server/src/server/auth-do.ts#L408', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('Nt4PKu_sMVV62EWLGLosW', 'Recover better from client being ahead of server during connect', FALSE, 1679359407000.0, 1678418742000.0, 'aboodman', 'a8befac65daeb85e14df4b7ce28c67404d01136a cleaned up some error handling on the server, but it removed a helpful error message:

https://github.com/rocicorp/mono/commit/a8befac65daeb85e14df4b7ce28c67404d01136a#diff-acf49911961d801fe90465dd29429bab86cc16c07016381405263ddbc7ae2cffR88

This situation commonly happens during development and this error helped users know what to do. Now it happens and is confusing again:

https://discord.com/channels/830183651022471199/1055564993883549787/1083495573761560637

The server has errors defined for these two situations: https://github.com/rocicorp/mono/blob/main/packages/reflect-protocol/src/error.ts#L21 but I don''t think they are being used. Can we use them and then on the client if received we should:

- print warning
- dump local state
- reconnect', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('lmadQG9qgjCJ-cH0iAUrC', 'Fix Replicache public APIs containing DD31', FALSE, 1678894915000.0, 1678308945000.0, 'arv', 'There are some public APIs that are named DD31. These were never meant to be exposed.

For example:

https://trunk.doc.replicache.dev/api/#pokedd31', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('XeWRpSJn2BkjvCyC6Bd_6', 'Performance of reconnecting with significant offline mutations is terrible', TRUE, 1709537611000.0, 1678295159000.0, 'grgbkr', 'When reconnecting the CPU is pegged by rebasing mutations.  This is because, the pusher logic pushes up mutations individually and then they come down in a series of pokes.  Resulting in something like
1000 pending, poke contains 50, rebasing 950
950 pending, poke contains 50, rebasing 900
900 pending, poke contains 50, rebasing 850
... and so on

This is improved somewhat by the 60fps buffering and playback logic, as the mutations from multiple of the reflect pokes above will often get merged into a single replicache poke.  

This is related to: https://github.com/rocicorp/mono/issues/378', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('gBu2nDQ12xiPyiROFKuTy', 'RFE: overload subscribe callback with a function', FALSE, 1693817777000.0, 1678268312000.0, 'arv', 'Subscribe currently have the type:

```ts
subscribe<R extends ReadonlyJSONValue | undefined>(
  body: (tx: ReadTransaction) => Promise<R>,
  options: SubscribeOptions<R>,
): () => void;
```

with `SubscribeOptions` defined as:

```ts
export interface SubscribeOptions<R extends ReadonlyJSONValue | undefined> {
  /**
   * Called when the return value of the body function changes.
   */
  onData: (result: R) => void;

  /**
   * If present, called when an error occurs.
   */
  onError?: ((error: unknown) => void) | undefined;

  /**
   * If present, called when the subscription is removed/done.
   */
  onDone?: (() => void) | undefined;
}
```

For convenience we should overload the second parameter to allow passing the `onData` function only:

```ts
subscribe<R extends ReadonlyJSONValue | undefined>(
  body: (tx: ReadTransaction) => Promise<R>,
  options: SubscribeOptions<R>,
): () => void;
subscribe<R extends ReadonlyJSONValue | undefined>(
  body: (tx: ReadTransaction) => Promise<R>,
  onData: (result: R) => void
): () => void;
```

The semantics would be that if the argument is a function then that is used as the `onData` function and `onDone` and `onError` would be `undefined`.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('v6dFwAmRkI7Ik95jkz04l', 'RFE: Add a default value to `ReadTransaction::get()`', TRUE, 1680568771000.0, 1678236542000.0, 'aboodman', 'It''s pretty annoying having to type:

```ts
const count = (await tx.get("count")) ?? 0;
```

How about we can say:

```ts
const count = await tx.get("count", 0);
```', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('-Tn95VNrkaxlmduVtuuCD', 'Offline mutations should replay atomically', TRUE, 1678138925000.0, 1678138908000.0, 'aboodman', 'Right now when you go offline, then come back online, collaborators see your work replay on a poke-by-poke basis ðŸ˜¬. This will look pretty funny if there is a lot of offline work.

I think we need something in the protocol to indicate that mutations being sent were created offline. We''d then either (a) buffer the offline mutations server-side until we have them all and replay in one turn, or (b) process them as normal but tell the receivers to pause motion until replay complete.

(a) has the downside that we will eventually exceed server memory limits. (b) looks ugly but should in many cases actually happen pretty fast and may be the easier tradeoff.

Other options are to store the offline mutations somewhere server-side or maybe to use the socket itself as a queue and just process the mutations from the reconnecting client incrementally as part of the turn processing so that they are never in memory?', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('-z7a9Y0Lggy_cMzNNEf7m', 'Reconnect on `online` event (Reflect)', TRUE, 1691542322000.0, 1677949309000.0, 'aboodman', 'I forgot how frustrating waiting 60s to reconnect is.

I think the max time to wait for reconnect when you have network should be 5s. Even that is going to feel slow and frustrating when you know you have connection, so there should also be a fast path that uses the browser online change event.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('HLnQTOvliwqxlxnwDxsQY', 'Expose `experimentalKVStore` in Reflect', FALSE, 1680313153000.0, 1677942217000.0, 'aboodman', 'In Replicache 12.1.0 we added support to plug in a different kvstore:

https://blog.replicache.dev/blog/replicache-12-1-0

Users have been using this in Replicache to add SQLite support for React Native!

https://discord.com/channels/830183651022471199/1072640266684612748/1076897871900717106

Let''s expose `experimentalKVStore` in the Reflect constructor like it is in Replicache so users can use Reflect in RN too!', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('gPVhbmVjtQnpYc9lhqqOp', 'Let''s undo the undefined change', FALSE, 1686081641000.0, 1677894564000.0, 'aboodman', 'See https://discord.com/channels/830183651022471199/1078726432559210536/1081168561071984681 for background.

The idea is to re-allow `undefined` as a value for `JSONObject`. We will not filter it out anywhere explicitly, just let `JSON.stringify()` do it. We have to handle `undefined` in all internal code when dealing with `JSONValue`, as will customers.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('gMuZ1tweNI0m8O0VTFRnz', 'Stop exporting all of `replicache`', FALSE, 1694088344000.0, 1677842289000.0, 'aboodman', 'In reflect''s `mod.ts` we do:

```ts
export * from ''replicache'';
```

This was a lazy convenience. We should check what we need and only export that.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('MPdM2gW91IhLM9Riata5f', 'Audit for places we are mutating values returned from EntryCache.  Consider making values in EntryCache immutable.', TRUE, 1677842339000.0, 1677789451000.0, 'grgbkr', '              Is it safe to mutate the clientRecord. Can we add an issue to review the usage of mutable cache entries?

_Originally posted by @arv in https://github.com/rocicorp/mono/pull/355#discussion_r1122948847_
            ', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('5Lzwt-BLI28FPj8fed01J', 'Deploy Replicache docs from mono', FALSE, 1690363763000.0, 1677767920000.0, 'arv', 'doc.replicache.dev is deployed by vercel from the doc branch of replicache-internal', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('oE5HPVV0KG8ZY0j6h8zM7', 'deleteAllReplicacheData broke in latest minor version', FALSE, 1677765782000.0, 1677764777000.0, 'arv', 'https://doc.replicache.dev/api/#deleteallreplicachedata

This didn''t use to take an argument. We need to make this optional again.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('eXWUPvyzDn7vHecgPEZI0', 'Q: Why no jurisdiction for AuthDO?', FALSE, 1677789788000.0, 1677762264000.0, 'arv', 'When we create the roomDO we pass in the jurisdiction. Is there a reason why we do not do the same for the authDO?', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('7Tefw3JDgEe7D_nmgQ3x4', 'Mirror - The beginning of the Reflect saas', FALSE, 1700011707000.0, 1677701162000.0, 'aboodman', 'In order for people to really try Reflect usefully and have a great experience we need to offer it as a saas.

Fritz designed https://www.notion.so/replicache/Reps-v-Smol-Design-Sketch-818bb4a93bd245e48cc0e3b344a2f473 but there is an even more minimal version in progress here: https://www.notion.so/replicache/Reconsidering-SaaS-3aa7ce8bb2e947aeb17bf4220a4be186.

Even as a microsaas, this is a significant amount of work and has several subcomponents:

- Implementing a new service, `mirror`, which manages the deployments of user apps on our service
  - Setting up customer/app storage somewhere on server (either reusing license server or doing something new in CF)
  - Implementing `publish` endpoint
  - Implementing `dev` endpoint for quick development
  - Implementing `tail` endpoint to get logs
  - Tracking usage while running and implementing `usage` endpoint
- Implementing a set of CLI commands in the `reflect` package to invoke the service:
  - `init` to create a new app on the server
  - `publish` that builds the app and sends to server
  - `tail` that streams the logs to console
  - `dev`
  - `usage`', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('sL61Cb893TvSZQ-FNG-B6', 'Get Monday and Subset on newest version to validate', FALSE, 1683661893000.0, 1677700990000.0, 'aboodman', 'For Monday this is somewhat a duplicate of #351 but more generally before we do an MVP release these two customers should be using the code and happy.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('Zo7n_XGcUrs5eU9Z_awvz', 'Allow access to env for all options', FALSE, 1679490008000.0, 1677700802000.0, 'aboodman', 'Factoring this out of #173.

Right now many of the fields of `createReflectServer` is a callback that takes an `Env` parameter. This is required so that, ie, `getLogLevel` can depend on the env.

However, (a) this is just annoying to type for every field, and (b) we''ve gotten user reports that other methods also want access to the env:

<img width="909" alt="Screen Shot 2023-03-01 at 9 50 04 AM" src="https://user-images.githubusercontent.com/80388/222250307-d72ac052-ada8-4f16-8ac7-3759302c7480.png">

I think CF did a bad thing with their env api design. It should be globally available to all user code somehow. So something like:

```ts
export function createReflectServer({env}: {env: Env}) {
  return {
    mutators,
    authHandler,
    logLevel,
    logSink,
    ...
  }
};
```

This will be a bit tricky to marry to CF''s API because we don''t find out about the env until `fetch()` in the case of the worker. But we can delay calling this function until then.

Another thing to check into is whether in CF, the env can change without restarting the context. I bet that it cannot. But if it can then we need to make the env field in the constructor a getter so it can return latest value (or maybe a function to make it clear it''s dynamic).', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('nuPkE2aUsvOlgl3Plc7F3', 'Fix Monday connectivity', FALSE, 1709537625000.0, 1677699474000.0, 'aboodman', 'There are a number of connectivity issues we are tracking on Monday''s shared bug tracker:

https://github.com/rocicorp/shared-monday/issues/2
https://github.com/rocicorp/shared-monday/issues/3
https://github.com/rocicorp/shared-monday/issues/5
https://github.com/rocicorp/shared-monday/issues/6

As a requirement for beta, we must squash these bugs by any means necessary. Step 1 is to add metrics ( #186 ) but if that confirms the issue we will still need to debug.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('a2XvjZLnYV40X-tvhkML0', '"How it Works" demos', FALSE, 1681819114000.0, 1677699233000.0, 'aboodman', 'On the marketing page there are some slightly interactive demos that explain the product that require eng resources to implement (or maybe @alexhking can do once the product is a tiny bit further along ðŸ‘¹).

Left todo here:

- [x] hook up "reset" buttons
- [x] Delay loading Reflect instances until demo scrolled into frame (#466)
- [ ] make second demo "whizzier" (pending design from @alexhking)
- [x] please make button on second demo into a slider -- all the way left is -xdeg/ms, all the way right is +xdeg/ms, when you let go it snaps back, the two sides are disconnected.
- [ ] I''m seeing fairly flakey behavior from these particularly on iphone, like sometimes I tap and nothing happens :-/, or sometimes the latency seems far longer than expected from slider. @cesara can you take a QA pass over especially on mobile? Not saying it''s your fault could be Reflect ðŸ˜¬.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('o9q8HM5PE0vLGzu6Y05Fc', 'ALIVE demo', FALSE, 1682371389000.0, 1677699109000.0, 'aboodman', 'Current worklog: https://www.notion.so/replicache/v13-worklog-c3b4b0645e3c43f19926ee4253c1182f?pvs=4', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('MFfdqRANMgtlQa43HXWQJ', 'close should wait for persist', TRUE, 1709580454000.0, 1677671715000.0, 'arv', 'If `close` is explicitly called, then it would make sense to make sure that we have persisted all in memory changes to the durable store before resolving the returned promise.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('QHzFs8PB_x1gcmvQecn8U', 'Figure out how to change esbuild target', TRUE, 1684744728000.0, 1677669813000.0, 'arv', 'CloudFlare workers use the v8 version that is in latest stable chrome:

https://developers.cloudflare.com/workers/runtime-apis/web-standards#javascript-standards

But looking at the output from:

```sh
wrangler publish --dry-run outdir=test-out
```

I can see that the esbuild target is something pretty old. We should figure out how to change this to get maximum speedâ„¢ï¸', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('KjrHIoayJ0qvG09u5iDm7', 'Add reflect bundle size to the dashboard', TRUE, 1684744744000.0, 1677616903000.0, 'arv', 'And remove the non esm replicache bundle sizes', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('pq-diA4zFzhkW7VJsn3Qr', 'Perf test is failing on refresh tests', FALSE, 1677617489000.0, 1677601463000.0, 'arv', 'Started failing here

https://github.com/rocicorp/replicache-internal/actions/runs/3652291573/jobs/6170509878

Looks like it is due to different version of Chrome but I need to investigate more', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('0rJxp4rrxgBHvH_hxZjSm', 'What do we need to do to support schema changes as elegantly as Replicache did?', TRUE, 1677873342000.0, 1677554572000.0, 'aboodman', 'Don''t think required for beta', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('niqzkiznUTO33HDXdZ328', 'Reduce write cost temporarily for beta', FALSE, 1677706276000.0, 1677178938000.0, 'aboodman', 'We need to be competitive with liveblocks'' pricing and the writes every four frames really kill us: https://docs.google.com/spreadsheets/u/1/d/1d6xCMg6c9_oKso-124gFkfuKsY1aJXEdRqo_MX8yzk4/edit#gid=2131158829.

So we will need to (for GA) implement the crazy recovery protocol that @grgbkr designed. ITMT, we discussed and we think it''s fine to just be a little unsafe and persist much less frequently. It will be the case that during crashes the client can get ahead of the server. In that case the client should just discard (or perhaps rebase, breaking causal consistency).

We will circle back and make this rock solid before GA.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('rmm3Ju1tM0mCIXzvipoDD', 'Tweak eslint no-floating-promises to disallow void P', TRUE, 1677148364000.0, 1677148364000.0, 'arv', 'https://typescript-eslint.io/rules/no-floating-promises/

              > @grgbkr Maybe we should disallow `void promise` and force people to use `.then`/`.catch` in all cases?

I''d be in support of this, void often leads to uncaught promise errors.

_Originally posted by @grgbkr in https://github.com/rocicorp/mono/issues/22#issuecomment-1440563431_
            ', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('109oqnpl3p17c9QB6qAHA', 'Move all bugs from source repos into this repo', FALSE, 1677096274000.0, 1677036559000.0, 'aboodman', 'Move bugs from `replicache-internal`, `reflect`, and `reflect-server` into this repo.

We can use the `gh` cli for this: https://jloh.co/posts/bulk-migrate-issues-github-cli/.

* For each repo, add a label like `replicache`, `reflect`, or `reflect-server` indicating the area.
* If the bug was P1 at source, add a P1 label at dest
* Create a *Playable Beta* milestone in `mono` like the one in current `reflect-server` and tag all the relevant bugs (this one probably makes sense to do by hand).', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('GnmhxPQj--yAjshNQR5xj', 'Promise rejections in `_processNext` silently swallowed.', FALSE, 1677754392000.0, 1677035813000.0, 'aboodman', 'It appears that any unhandled rejections in `_processNext` are silently swallowed.

@jesseditson and I discovered this in the process of debugging rocicorp/mono#166. `DurableStorage#put` was throwing and this bubbled up to the `setInterval` call here: https://github.com/rocicorp/reflect-server/blob/c342e4eba5edba55b45eac5966052b464de44cfa/src/server/room-do.ts#L385. The error was not printed to the local console from `dev-worker`.

I verified that if I just change that setInterval to `return Promise.reject("bonk")` , that error *also* doesn''t show up in the `dev-worker` console.

This all was happening on `reflect-server@0.21.1` which uses an old wrangler. It is possible that the newest wrangler fixes this bug, that needs to be verified.

It is also possible that these errors would have been reported to logpush, that''s not clear.

Finally, I tried registering the global handler `unhandledrejection` and that did get fired. But the `reason` field was `undefined` :(.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('g-7mAvE-qMLJgg-v0ZzQi', 'Better support for large values', TRUE, 1677790411000.0, 1677024009000.0, 'aboodman', '@jesseditson hit the case in paint fight where keys > 128KB not supported by DO.

Generally we don''t recommend large values right now because they don''t work nicely with our key-wise diff. But in Jesse''s case this made sense to do (this value changed rarely and was the flattened view of the image).

At some point we will want better large value support. I think this will also be useful, e.g., for yjs support.

I''m very tempted to implement this by using a rolling hash, ala prolly tree. I don''t think we need the entire tree just the leaf nodes. We can encode a large value as an array of content-sliced chunks, then write the chunks as keys in replicache. Sync takes care of the rest?', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('_Wef2qqAq4jCHbnSGRoaP', '@rocicorp/reflect build should use platform neutral', FALSE, 1678185815000.0, 1677002707000.0, 'grgbkr', 'This will preserve `process.env.NODE_ENV` in the output so that it can be determined by the customers build step.  This will give them replicache debug asserts and debug behavior like deep freezing values during development.

Currently all builds are done with plaform ''browser'', and with minification on, resulting in process.env.NODE_ENV expressions being automatically defined to "production".

See https://esbuild.github.io/api/#platform', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('w3k2LCZJxewgS1dARkA8Y', 'Turbo repo code Caching on GH Actions for ty', TRUE, 1677165207000.0, 1676892695000.0, 'arv', 'https://turbo.build/repo/docs/ci/github-actions', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('uEXZoER5yNM3vQxWIXg-f', 'Rename replicache-internal to replicache', FALSE, 1677154408000.0, 1676884951000.0, 'arv', 'No need to have the distinction. Everything in mono is internal', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('xSEkY1awzyhuB-c0Jc0UV', 'Move @rocicorp/licensing into this repo', TRUE, 1677093343000.0, 1676884917000.0, 'arv', '`@rocicorp/licensing` is a private npm package and we need to pass in the NPM_TOKEN to the GitHub Actions runners. Moving this into the mono repo would simplify that.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('xM_4rGRsys3jKT0CiGJCP', 'consoleLogSink on server should JSON.stringify log arguments ', FALSE, 1678194107000.0, 1676567808000.0, 'grgbkr', 'I see it in the datadog-log-sink, but I think we need to add something like it to the consoleLogSink for the server as well, because cloudflare''s console log does a bad job of stringifying, giving results like:
```
INF RoomDO doID=219e9351928ed1d8df08e76d617d3c503ec46df3c106a34be75c074b0149fc87 blah Object {
  bar: baz,
  foo: Array(5)
}
```

and 
```
DBG RoomDO doID=219e9351928ed1d8df08e76d617d3c503ec46df3c106a34be75c074b0149fc87 roomID=iOAgoD requestID=mfbwvnbkvxe clientIP=71.228.154.248 wsid=JkNCB40MYzD66H4ABQ5bw client=1598b74f-5af8-4aa6-a061-aad2b8c1e50a wsid=JkNCB40MYzD66H4ABQ5bw msg=f8eesmg3opd processing room clientIDs Array(1) [ Array(2) ]  pendingMutations Array(1) [ Array(2) ]
```

It would be better to do it in the sinks though instead of in all the log lines.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('zV0bdg5YJbbylZqmUhrQt', 'Datadog cleanup', FALSE, 1677698295000.0, 1676285022000.0, 'aboodman', 'The current situation with datadog integration is confusing for users:

* reflect-server exports an implementation of DatadogLogSink, but reflect does not -- application must provide. This inconsistency is odd, but also:
* the two implementations are different. the server one is our own from scratch, the client one wraps datadog''s client library.
* reflect exposes an abstract Metrics interface apps can implement, and we have an impl in datadog-util. But this impl doesn''t stand alone, because it needs to talk to an endpoint that has the api key. This means that it is not really possible for the datadog impl of metrics to really be pluggable (at least not without a supporting server).

We should:

* Pull our custom DataDogLogSink into `datadog-util` so it can be used on both client and server.
* Add an option on both `Reflect` constructor and `createReflectServer()` like: `{datadog: {apiKey: string, enableLogs: true, enableMetrics: true}}`. Use this option to default the log sink.
* Change `logSinks` (plural) back to `logSink` (singular) and change the semantics: if `logSink` is present it overrides everything else and is the sole sink. Otherwise, the log sinks are `consoleLogSink` and `DatadogLogSink` (if `datadog` option provided).
* Enable metrics if `enableMetrics: true`. No need for caller to pass Metrics impl.

This is required to get a metrics release out to Monday.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('BLUsC86cjEIHYkXuiYnTU', 'Validate migration code of rooms and cleanup replidraw-do', FALSE, 1677666988000.0, 1676111018000.0, 'arv', '              Sounds good. @arv can you turn this PR into that task:

- Pick one of the rooms that is exhibiting this error
- Migrate it: https://github.com/rocicorp/reflect-todo/blob/main/doc/server-api.md#migrate-room (secret is in https://docs.google.com/document/d/1aGHaB0L15SY67wkXQMsST80uHh4-IooTUVzKcUlzjdk/edit)
- Confirm error goes away
- If it does, nuke replidraw-do storage. replidraw is using the script name `replidraw` (per: "https://github.com/rocicorp/replidraw-do/blob/main/wrangler.toml#L1"). Go to cloudflare console > workers > find this worker > manage > delete.
- Rerun sample, confirm error stops happening.

Then this PR should not be necessary. Thanks.

_Originally posted by @aboodman in https://github.com/rocicorp/reflect-server/issues/341#issuecomment-1426358530_
            ', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('qQJ73nkTKuavBoXO4aFti', 'Get rid of `getBaseCookie` hack', TRUE, 1677695778000.0, 1676020041000.0, 'arv', 'By exposing this in Replicache

Once we have a mono repo we do not have to use a public API?', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('WrDhErVP1Ef_KNtlUSzjb', 'Implement `pull` over the web socket', FALSE, 1677752055000.0, 1676019971000.0, 'arv', 'We use pull for mutation recovery. At the moment this is done using an HTTP POST. It would be more consistent to just piggy back on the existing web socket connection.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('bFaOQIA4nFbMiKJdA2T8L', 'Worker Runtime API refresh', TRUE, 1684746343000.0, 1675965305000.0, 'aboodman', 'Needs design, WIP. See: https://www.notion.so/replicache/Worker-Runtime-API-Refresh-1bb2b238b6ed4c4eb97d3adb4301b466', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('Gh2cCtydyJTG7-mm6lA2y', 'Add `onRoomStart` server API', FALSE, 1684355253000.0, 1675964410000.0, 'aboodman', 'For writing initial data, or migrating.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('gEXJOAFBf35Kt0q7RN5Lh', 'Add `connectHandler` server API', TRUE, 1684744885000.0, 1675964370000.0, 'aboodman', 'We currently have a `disconnectHandler` [in the server API](https://github.com/rocicorp/reflect-server/blob/c342e4eba5edba55b45eac5966052b464de44cfa/src/server/reflect.ts#L17).

This is most commonly used to clean up per-client state that is created on the client-side.

It would be useful to have an analogous `connectHandler` API. This could be used to populate per-client state that is inconvenient to create client-side (ie because there should be only one of something, or it should happen only once, etc).

This `connectHandler` would be the dual to `disconnectHandler`. Like `disconnectHandler`, it would receive a `WriteTransaction` as an argument, and would run in the `RoomDO`.

We should guarantee to run `connectHandler` before any mutation from that client. Exceptions from the connect handler should prevent the connection from being accepted (and result in an error back to the client). Exceptions should be logged at error level. But they should not take down the server completely.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('Nq0htvSPDitB4OtUjrm_s', 'fastForward only runs on first mutation', FALSE, 1677698738000.0, 1675936360000.0, 'aboodman', 'It should be running immediately on connect. This causes all the samples and user apps to have a pointless `noop` mutation at startup.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('TT5ktZwwQo-f-IqKKYoWB', 'Runtime API cleanup', FALSE, 1677094848000.0, 1675936264000.0, 'aboodman', 'See Notion: https://www.notion.so/replicache/Worker-Runtime-API-Refresh-1bb2b238b6ed4c4eb97d3adb4301b466', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('OjzdiqxGAf2EdgbkenwD7', 'Flush storage only periodically (say, every 5s)', TRUE, 1683332951000.0, 1675935314000.0, 'aboodman', '# Problem

60fps write cost kills us. We have a solution to this that we have spec''d but it''s a lot of work.

# Proposal

Flush only every 5 seconds. There is a chance that if a DO crashes, a client can end up ahead of the server. In that case, we must detect the situation and delete optimistic data.

We can detect this situation by changing the notion of `version` that we store in the DO state. Right now we just store a single `version` field. The problem with that is that when a DO crashes and restarts, it might reuse versions that have already been vended (but not stored).

A simple solution is to store one version per running instance of the DO. We would then communicate to the client on connection which instance it is talking to. The client would save this information in the snapshot along with the version.

On reconnect, the client sends the instance it was last talking to _and_ the version. The server can then compare to the last known version from that instance and determine if the client is ahead. If it is, it closes connection with a special error code that causes the client to nuke its optimistic state.

# Warning

There is a minuscule case that the client goes offline at exactly the moment a DO crash occurs *and* the user works offline for a significant period of time. In this situation all the user''s offline work would be lost.

I am fine with this situation for beta, but for GA we should decide to either abandon strict causal consistency in this situation or else do the fancy solution.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('CfcrJOa7X1DqD87cLDpbh', 'have a test that ensures that if connect throws we hear about it as an error in the client log', FALSE, 1677094849000.0, 1675853163000.0, 'arv', NULL, NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('B82TlBRphU8lxQeUwf5dO', 'Disconnect on hide', FALSE, 1677094849000.0, 1675846732000.0, 'arv', NULL, NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('qUMrP0S5nCH2HxWaEDsqv', 'onOnlineChange should fire only when reconnect fails, not for passing socket drops', FALSE, 1677094850000.0, 1675846722000.0, 'arv', NULL, NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('vhsfVoQ6fiiFCxLtFZcNB', 'Figure out how to deal with schemaVersion mismatch', FALSE, 1677094850000.0, 1675761614000.0, 'arv', 'The server side implementation of the mutators needs to know about the schema version so that they can deal with old schemas.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('EvajFts82v8s3vXjLvwmb', 'Add protocol version on connect', FALSE, 1677094851000.0, 1675761541000.0, 'arv', '### Problem

It is possible that the client and server might be on different versions.

If there are non backwards compatible changes then the client cannot communicate with the server.

### Proposed Solution

- The client sends its protocol version when it tries to connect. 
- If the server is not able to support that version it closes the connection with a `ProtocolMismatch` error kind.
- When the client sees a `ProtocolMismatch` it calls a callback with an error object that can be used to detect this case.
  - Default error handler is to reload?

### Open Question

- Does this overlap with `ReplicacheFormatVersion`?
  - It does not because the Replicache format version is only used to determine how the persistent storage is structured in IDB/memory
- How does this overlap with `PullVersion` and `PushVersion` used in Replicache?
  - A change in either of these would lead to a change in the `ProtocolVersion` change because `PullVersion` and `PushVersion` are used in `Poke` and `Push`
    - Actually `PushVersion` is used when we do a `PushRequest` in Replicache. It is not used in Reflect which seems to be a potential issue.
- How does this overlap with `SchemaVersion`?
  - This should not impact the `ProtocolVersion`...
    - I think we need to consider how `SchemaVersion` can be dealt with in the server side mutations


', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('6U76IyqxlPLwVxDA0zNYa', 'Time out a connect attempt out after say 10s', FALSE, 1677094852000.0, 1675419563000.0, 'arv', NULL, NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('5Ma2f_7pZwbyZBnjl-ynV', 'Backoff so the client is not just reconnecting in a tight loop forever', FALSE, 1677094852000.0, 1675419556000.0, 'arv', NULL, NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('37ERG8iH3VUeHDBRRZ0NZ', 'Add metrics to client to track connectivity', FALSE, 1679818208000.0, 1675121257000.0, 'phritz', 'There is a WIP PR here: https://github.com/rocicorp/mono/pull/334

Fritz''s notes here:

Background:
- seemed like a good idea to use datadog for metrics because we use it for logging. i think that''s still right.
- datadog''s metrics model assumes that metrics are collected/aggregated by something like [statsd](https://github.com/statsd/statsd) (their specific version is dogstatsd), which submits aggregates to datadog itself. statsd can run on node but it seemed like biting off more than we wanted to chew to just get metrics up and running to try to get statsd running on CF in a DO so i did not go that route, or at least deferred it until the client and dashboard pieces were up and running. i also didn''t want to call into existence a separate process running it somewhere else like heroku. so for expedience decided to just submit metrics directly to datadog.
- submitting via the http api comes with constraints:
  - see https://docs.datadoghq.com/developers/dogstatsd/data_aggregation/#why-aggregate-metrics and https://docs.datadoghq.com/metrics/types/?tab=count#submission-types-and-datadog-in-app-types but basically we are constrained to using just count, rate, or distribution points when submitting via http api. and distribution points are the thing that is easiest to aggregate on the CF end, so that''s what we use under the hood for our metrics: distribution points. 
  - the http api does not support cors because as mentioned datadog assumes something else is doing the aggregation and datadog does not want to receive a zillion dataponits individually from browsers or anywhere for that matter. so since we cannot submit points directly from the client to datadog we proxy them through a [worker endpoint](https://github.com/rocicorp/reflect-server/blob/04cc5ebcd2e239303a00fdc2a614201775e9bf50/src/server/worker.ts#L251). it literally just takes the body and sends it to datadog. 
  - in particular the proxy endpoint does not aggregate metrics. every submission from a client is sent directly to datadog. **this doesn''t scale** so can''t be used as is for customers with lots of clients. but is good enough for monday. this issue https://github.com/rocicorp/reflect-server/issues/293 captures the fact that it doesn''t scale. what needs to happen is that the endpoint, possibly a DO though not necessarily, should buffer changes from clients and report them periodically to datadog instead of one at a time. doing this would also enable us to have a richer set of metrics on the client eg a Set of clientids in a particular state that gets buffered and translated into a count distribution point at datadog submission time. in doing this we''d need to be cognizant of the client reporting interval and the dashboard rollup window which must match in order for the dashboards to be accurate/sensible (more on this below).
  - our metrics proxy endpoint is not rate limited or authenticated
  - the capability is there but we do not currently pass tags like the app that is submitting the metrics or the environment (prod vs dev) so metrics from all sources are currently aggregated together in the dashboard
  - replidraw-do has metrics hooked up
- there is a script for spamming datadog with metrics so you can see the dashboards work: https://github.com/rocicorp/datadog-util/blob/main/tool/report-metrics.ts. otherwise there is too little traffic to see much of anything meaningful. 
- https://www.notion.so/replicache/metrics-0b848461ffce4312bdb645d65adb7da2 sketches the metrics that we started implementing. we have the time to connect (including never) and the reason the last connect failed metrics in place. 

OK so briefly how do they work? The thing we want is a count of clients that have a particular state in a moment of time, or over a period of time. For example, we want to say across all clients how long have they taken to connect, or how many are (un)connected at this moment, or how many clients are logged out because of auth. Notice that these most important questions ask for a count of *clients* that have a property at a given instant or over a period of time. They do not ask for a count of *events* like ''a client got logged out for auth'' or ''a connect attempt took 2s''. If we counted events like this then the counts would be skewed by clients that are reconnect looping or similar. We want to know how many *clients* in the *population* have a property. 

The way we do this is by sampling in the client and aggregating in the dashboard. We have a reporting period in the client that is currently 2m. Once every 2m each client submits exactly one datapoint per metric. So if we look at a 2m span we will see very close to if not exactly one data point per client. We can count the number of clients with a given property over that span and it also gives us the total number of clients reporting which is just how many data points we see for a given metric in that span. In the dashboard we do a count rollup over 2m to get these numbers. If the client is reporting at a different interval, or the dashboard is not rolling up over a span that matches the reporting interval, the numbers are not going to make sense. 

Also I guess note that metrics are good for counting things in a snapshot of time, or instantaneously. They are not good for stateful computations like what''s the history of this client. We''d need to keep the data in a form separately queryable to answer those questions. 

In order to create a dashboard:
1. you have to enable ''advanced percentages'' for the distribution metric in question in data dog. It''s under Metrics > Summary, search for the metric and enable advanced percentiles which enable threshold queries.
2. stack graphs are typically what we want. configure a dashboard with timeseries type and then add the metrics in question eg `count(v: v<=600000):time_to_connect_ms{*}.as_count().rollup(120)` is the count of clients with time to connect <= 600s over a 120s window. the little bar slider icon on the right of the metric field in the edit view will enable you to edit the formula directly wihtout having to use the pulldowns and whatnot. 
3. create the stack. list the metrics in the order that you want them (a first, b second, etc). sometimes you want to do math on metrics like to subtract the count of clients connecting in 20s from 10s so you can stack just those in the range 10s-20s on top of the count of those connecting in <=10s. to do this you can toggle display of the metric in question of by clicking its letter eg ''a''. we need the metric but we don''t want it displayed. instead, add a formula to the bottom with the right order. eg you can say something like:
  ```
  (not selected for display)  a: count(v: v<=20000):time_to_connect_ms{*}.as_count().rollup(120)
  (not selected for display)  b: count(v: v<=10000):time_to_connect_ms{*}.as_count().rollup(120)
  ...
  (selected for display):       formula: a-b
  (selected for display):       formula: b
  ```
  This will show the count of clients connecting in 10s with count of connecting between 10s and 20s on top of it. Note that datadog will sometimes "clean up" the formula and re-name the metric labels which is annoying but it usually does it in a way that doesn''t mess the graph up. It must have some kind of canonical form for storage that they get put into.
  
  4. Add tags to the metrics contructor eg `[''env:prod'', ''service:reflectserver'']` and they you can use them in the space aggregator in the metric formula eg `count(v: v<=20000):time_to_connect_ms{env:prod,service:reflectserver}.as_count().rollup(120)`

You can see how metrics are integrated here: https://github.com/rocicorp/replidraw-do/blob/d3d3f1711b2475e614f13640bfd2a498599bd74c/src/pages/d/%5Bid%5D.tsx#L30-L36 and then you simply pass the metrics to the reflect constructor: https://github.com/rocicorp/replidraw-do/blob/d3d3f1711b2475e614f13640bfd2a498599bd74c/src/pages/d/%5Bid%5D.tsx#L49. When you want metrics in reflect server itself we''d just do the same thing.

Dashboard for our two metrics are found here: https://app.datadoghq.com/dashboard/vm5-ce6-p67/reflect-client-metrics. On the left are the "real" dashboards that roll up over 2m (120s). On the right are the same but with 10s rollupts for use with the report-metrics metric-spamming script above.

For a customer to use these dashboards they need to:
- instantiate and pass metrics like replidraw-do does, ideally using tags for their service and environment
- once they start receiving metrics they need to configure advanced percentiles for our metrics: time_to_connect_ms, last_connect_error_{auth_invalidated, client_not_found, invalid_connection_request, invalid_message, ping_timeout, room_closed, room_not_found, unauthorized, unexpected_base_cookie, unexpected_last_mutation_id}. 
- import our dashboard. we can export it to json from the dashboard view and they can import the json.

@arv note that if you add a timeout as a connect error reason to cover connect() actually timing out, you''ll need:
1. add it to the report-metrics script and run it so we start getting the metric
2. configure advanced percentiles for the metric in datadog
3. add the metric to the last connect error dashboards, but the real one and the test/report-metrics one.

for some reason my browser screenshotting is not working so here''s a full screen shot of the two graphs (report-metrics spammed versions).
<img width="2560" alt="screengraphs" src="https://user-images.githubusercontent.com/157153/215613858-f5ae4d23-f93b-4918-ae3c-7092d6effecd.png">
', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('8730DfuqU_eRsNeuVF36-', 'This comment seems outdated.', FALSE, 1677094853000.0, 1674601715000.0, 'arv', '              This comment seems outdated.

_Originally posted by @phritz in https://github.com/rocicorp/reflect-server/pull/288#discussion_r1086022587_
            ', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('ksNwVR3DysT5WFxCK3WIa', 'RFE: Make clientID sync', FALSE, 1709536406000.0, 1674548085000.0, 'arv', '`clientID` is currently created inside `initClient` which is async but there is nothing async about creating the actual client ID. We could create it synchronously and pass it in.

We do however use `await rep.clientID` in the tests as a signal that the persistent storage is ready. We could expose a `ready` promise or we could just tell people to do `await rep.query(() => true)` which internally waits for the ready promise but it is a bit hackyy. (Not say what you mean)', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('hjO4PWlvoCCXj_AlS4CNk', 'Handle reauth on the client', FALSE, 1677094853000.0, 1674468411000.0, 'arv', '## Replicache

In Replicache we already set the precedence for how to do this. Here is how it works in **Replicache**:

- The API has a way to provide a function called `getAuth` with the type `() => MaybePromise<string | null | undefined>`.
- Whenever the server returns a `401` we call the `getAuth` function. The function can do HTTP requests, put up dialogs, etc.
- If the function returns a string. `replicachecInstance.auth` is assigned this new auth value and...
- Replicache retries the previously failed push/pull with the new auth.
- This retrying happens `MAX_REAUTH_TRIES` times.

## Reflect Client

In reflect we will do pretty much the same.
- reflect-server will send a close with an error kind
- When we get a `ErrorKind.AuthInvalidated` or `ErrorKind.Unauthorized` we enter into a state where _needsReauth_ is `true`
  - When _neesReauth_ is `true` we will "call" the `options.authToken` getter again. This can be a string or a function that returns a string. It can be async as well.

The API is:

```ts
authToken: MabePromise<string> | (() => MaybePromise<string>);
```', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('uKLntOMhEHkSnEv0g33OI', 'proxy metrics needs improvement', TRUE, 1684745997000.0, 1674192863000.0, 'phritz', 'Added in https://github.com/rocicorp/reflect-server/pull/292. 

It doesn''t scale because it sends a request to datadog for every request it receives (which is 1/reporting period=2min/client, so for 100k clients and a 2 minute reporting period that''s almost 1000 requests to datadog per second. It should buffer and report periodically in batches. But this should work for the time being (eg 1000 clients / 2 minutes = 8 requests/sec). 

There are other ways it needs improvements, see code comments (eg, auth).', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('V6GY_WByY1kDHoeefuCtK', 'Allow mocking IDB', FALSE, 1677091391000.0, 1674056885000.0, 'arv', 'We added experimental `kvStore` to allow mocking out IDB. But since we adde that we started using IDB in a few places.

Consider replacing `new IDBStore` with `new KVStore` everywhere where KVStore is something that can get passed into `ReplicacheOptions`.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('eYxcTP-MuuJ36qlHHEXbC', 'Get connectivity failure rate from replidraw-do', FALSE, 1677696998000.0, 1673894767000.0, 'aboodman', 'Monday has a connectivity failure rate of 2.5% according to our own logs (https://github.com/rocicorp/shared-monday/issues/2#issuecomment-1374336766) and from Noam''s.

We can''t currently compare to replidraw-do because the client side logging the analysis depends on isn''t present. We need to add the client-side logging to replidraw-do and then compare.

If the connection failure on replidraw-do is also high then we can debug more rapidly by iterating on our own app.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('xut4J7fPqhioTXsCGnVc8', 'Improve Datadog Logger', FALSE, 1684892165000.0, 1673617527000.0, 'arv', 'Datadog has a concept of [attributes](https://docs.datadoghq.com/logs/log_configuration/attributes_naming_convention/#default-standard-attribute-list), These are exposed in the UI in a special way.

The DD browser API allows you to pass these attributes using the `messageContext` parameter.

```ts
log(message, messageContext, status)
```

For the DD HTTP REST API the `messageContext` is the json object sent as the payload.

If `messageContext` is set to `{network: {client: {ip: ''1.2.3.4''}}}` then the Client IP shows up in their UI as shown in this screenshot.

<img width="763" alt="dd-ip" src="https://user-images.githubusercontent.com/45845/212326598-707db242-b2fe-47f0-948d-60b5d259e960.png">

`LogContext` has a different concept of _context_. Its context is added to the `message`.

Instead of the current way, I''m proposing we change `LogContext` `addContext` to add to a context object and pass this to DataDog as a `messageContext`. For example:

```ts
const lc = new LogContext();
lc.info(''a'');
// sends
// {message: ''a'', status: ''info''}
const lc2 = lc.addContext(''b'', ''c'');
lc2.info(''d''); 
// {message: ''d'', status: ''info'', b: ''c''}
```

DD also allows the attribute to contain `.` and then it builds the object structure as needed. For example, the following two context objects are equivalent:

```js
const a = {''network.client.ip'': ''1.2.3.4''};
const b = {network: {client: {ip'': ''1.2.3.4''}}};
```', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('QJZsGPRqM-9XZYPkJMrkJ', 'Assert JSONObject in WriteTransaction put', TRUE, 1677705585000.0, 1673469405000.0, 'arv', 'Replicache asserts a valid JSON value is passed into `WriteTransaction` `put` in debug mode.

https://github.com/rocicorp/replicache-internal/blob/ec0066f6ce2907307ab9196bbcd3838c3bb7fc1f/src/transactions.ts#L247

This calls [deepFreeze](https://github.com/rocicorp/replicache-internal/blob/ec0066f6ce2907307ab9196bbcd3838c3bb7fc1f/src/json.ts#L269) which calls [deepFreezeInternal](https://github.com/rocicorp/replicache-internal/blob/ec0066f6ce2907307ab9196bbcd3838c3bb7fc1f/src/json.ts#L316) which throws for non JSON values.

However, the server side implementations do not do any kind of runtime assertions.

We need to add a debug/release mode and enable assertions in debug mode.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('uKGqplhXFnm_N7UWmi_mL', 'Consider doing the WS auth as a message instead', TRUE, 1677697408000.0, 1673447552000.0, 'arv', 'We currently (ab)use the `Sec-WebSocket-Protocol` to do authentication. We should look into doing auth as a websocket message instead.

I have a spidey sense that this header might cause issues with proxies.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('Rohqczqcb8_uiuhA568rn', 'Too much work being done in durable-storage.ts', FALSE, 1678113601000.0, 1673430794000.0, 'arv', 'In `src/storage/durable-storage.ts`

```ts
  put<T extends JSONValue>(key: string, value: T): Promise<void> {
    return putEntry(this._durable, key, value, {
      ...baseOptions,
      allowUnconfirmed: this._allowUnconfirmed,
    });
  }
```

This has a few small issues but this is such a low level operation that these should be fixed:

1. We create a new object every time this is called.
2. We use object spread which is far from free. https://tc39.es/ecma262/#sec-copydataproperties

', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('pWlTHTkynqjd5gmljVESD', 'getNode is slow?', FALSE, 1684746615000.0, 1673258564000.0, 'arv', 'https://rocicorp.slack.com/archives/C013XFG80JC/p1673055269485049

getNode takes a total of 131ms in @jesseditson demo. It is not clear based on the Slack thread if this due to being called too often or if the cloning of the data is hidden in there.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('GpU_EDkk_C4jBuucWoRYZ', 'clean up the zillion of ERROR logs on the client (and server)', FALSE, 1677698463000.0, 1673229920000.0, 'phritz', 'There are lots of errors reported from the client (less so, server) that are not actually things that should get an engineer to look at them. We should clean these errors up (fix the actual problem or convert to info or whatever) so that we have a clear signal when something goes truly wrong on the client, and not just the network is being flaky. You can see a ton of these in monday''s logs eg
```
name=reflect-37603806-IyjKuXhw1Y3yDFMB4_e6a8AiBALlOLZn, bgIntervalProcess=ClientGC, 
intervalID=64, Error running., TransactionInactiveError: Failed to execute ''get'' on 
''IDBObjectStore'': The transaction is inactive or finished.
```', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('TApJHYop1jupTX3YsgT10', 'Ensure "Connected" log line prints out a req key that connects it to "Connecting..."', FALSE, 1677094854000.0, 1673054992000.0, 'aboodman', NULL, NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('2o486QHPAKSkPKEp09j2c', 'understand how we tear down/recover when a new instance of a DO becomes canonical', TRUE, 1684746121000.0, 1672973395000.0, 'phritz', 'A new instance of a DO can become canonical while the old instance is still extant. The old instance doesn''t know that it is no longer canonical until it touches storage or makes an http request: https://discord.com/channels/595317990191398933/773219443911819284/1042891744225796176. I think this means that clients can/will remain connected to an old instance until something happens that requires touching storage or making an http request. So users could be connected to a room that is no longer functional until someone tries to perform a mutation at which point the request to storage should throw with something like "Cannot perform I/O on behalf of a different Durable Object". What should happen in this case is that everyone gets disconnected. Presumably they will reconnect to the new canonical room. We should ensure that this actually happens! It seems possible the exception might be caught by us, and unless CF deallocates the DO, the connect users could stay connected. ', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('xQmT4UvY5YlFGXX5XfKiW', 'ChunkNotFoundError in DD31 build ', FALSE, 1681242589000.0, 1672950710000.0, 'grgbkr', 'Reported by Subset when using a build of reflect that has a version of Replicache with dd31.

![image](https://user-images.githubusercontent.com/19158916/210873594-69eb7156-b9da-4c2e-afcc-facb2f51a047.png)

Jesse Ditson and Aaron say they have also encountered this.  

', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('wRqEqQYUCjqI022sBVO8i', 'consider whether auth is leading to increased connection problems', FALSE, 1677094855000.0, 1672939473000.0, 'phritz', 'aaron suggested:

> could Sec-WebSocket-Protocol be screwing this? neither figma/notion/linear use this header and i recall it was somewhat nonstandard usage.

this should probably wait until we can measure whether a change has any effect (ie, after rocicorp/reflect-server#254 )
', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('hHwAQIHg-U3VXQAU7DJIF', 'we should ensure all the reflect server entrypoints are versioned', FALSE, 1677094855000.0, 1672894830000.0, 'phritz', 'Most entrypoints are versioned but createRoom and connect are not: https://github.com/rocicorp/reflect-server/blob/b01288be9d0ab8a5a7b1f6d25a13568133545000/src/server/dispatch.ts#L48 

This means that old clients can successfully connect to a new, incompatible version of the server (and also new clients can connect to old, incompatible versions of the server). Subset might have tried to do this by downgrading reflect but keeping reflect-server at 0.19/20 with who knows what resulting undesirable behavior. ', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('dVgRDv1EGyv1GI5KJSXhe', 'revisit connection logic ', FALSE, 1679346828000.0, 1672882869000.0, 'phritz', 'This issue replaces rocicorp/mono#226 which was more of a placeholder.

Background: we didn''t put the care and thought into reflect connection/reconnection logic that we did into replicache client connection logic. We have seen a large number of connection-related issues with monday and would like to put the connection logic on firm footing going forward. Some of the relevant monday issues:
- https://github.com/rocicorp/mono/issues/276
- https://github.com/rocicorp/reflect-server/issues/230
- https://github.com/rocicorp/reflect-server/issues/233
- https://github.com/rocicorp/reflect-server/issues/234
- https://github.com/rocicorp/reflect-server/issues/236
- https://github.com/rocicorp/mono/issues/233

I think to summarize what''s involved in closing this issue, which should probably include a design sketch, is overhauling how connection logic works, inclusive of:

- [x] **Reauth**:
  - [x] Ensure that there is a signal to the app to re-auth the user on auth failure. This should likely already be solved by https://github.com/rocicorp/mono/issues/230 but if not should be part of this.
  - [x] rocicorp/mono#188
- [x] rocicorp/mono#185
- [x] rocicorp/mono#184
- [x] rocicorp/mono#181
- [x] rocicorp/mono#180
  - related to https://github.com/rocicorp/reflect-server/issues/230 for DO duration reasons we should probably disconnect on "hide" and reconnect on "show" (but stop short of disconnecting users from idle rooms, which requires more thought/discussion)
- [ ] consider having a canary http request that is issued when connect fails to determine if the user is offline and include that bit in the logs/metrics
- [x] rocicorp/mono#179
- [ ] take a look at the out of order poke problem which we do not have an explanation for. is there anything we can add or track that could help shed light on it? https://github.com/rocicorp/mono/issues/233
- ensure the logs include the following information:
  - [ ] when a connection is closed log the time the connection was open and how many messages were received
  - [x] connection closed log line should include the reason the connection closed (auth failure, blur, connect timeout, ping/pong failure, etc)
  - [ ] when it successfully connects, include the length of time it took to connect in the log line
- [ ] I think we want metrics that tell us connection-related information. This requires metrics to be added (https://github.com/rocicorp/reflect-server/issues/254) but also some design to figure out how to express what we are interested in. A proposal will follow. Kind of intuitively we want:
  - from the client:
    - information about whether a client is connecting or having trouble connecting. If the client is online and it can''t connect we want to know how often it is happening and why (connect timeout, auth failure, connection quickly reset for some reason). 
    - [ ] how long it takes to connect
    - [ ] how frequently it is getting disconnect for an unexpected reason (server abruptly disconnects, ping/pong failure, etc), exclusive of blur and clean shutdown, and also connections that are closed without receiving a message
    - [ ] connection uptime 
- [ ] in the above, we need to factor in that bc of dd31 going offline for long periods of time is expected 
- Something something ondisconnect? 

I think this issue does NOT necessarily include adding an http fallback. I think this is getting our current connection logic into a place we want. We can evaluate from here whether it is worth prioritizing a fallback.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('KzIYXTz64Zx0_fyMDywos', 'add client and server metrics', FALSE, 1677094856000.0, 1672881415000.0, 'phritz', 'just a placeholder for now, will fill in with details later. in particular we need connection-related metrics which i will flesh out a proposal for. 

ensure it includes:
- DO restarts', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('nlwrOyWJN0vyNMRf86PQ8', 'consider if there is a better way to wait for the client id to be added to the log context', FALSE, 1677094857000.0, 1672875851000.0, 'phritz', 'see https://github.com/rocicorp/reflect/pull/52#pullrequestreview-1236557590', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('-7BhUMh2YaWRHglsa2qVX', 'ensure the logger is stringifying objects', FALSE, 1677094857000.0, 1672874779000.0, 'phritz', 'see https://github.com/rocicorp/reflect/pull/52#discussion_r1061943225 and i think separate discussion on slack with arv. possibly related is https://github.com/rocicorp/reflect-server/issues/91. 

this issue should probably also include logging the request at the point of the original comment', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('yi2sfNm8hnpVw5QzGLhhR', 'ensure request id is properly passed through from hop to hop', FALSE, 1677094858000.0, 1672872956000.0, 'phritz', 'We use a request id to tie log lines together across hops (eg client to worker to DO). We need to take a pass and ensure that the request id is being faithfully copied hop to hop, and initiated in the right places. This issue spans client and server. In particular I think it should include:

- [x] a poke should generate a request id and include it, and then whatever happens in the client as a result of it should carry that request id
- [x] worker, authdo and roomdo should all look for the http header carrying the request id and populate a log context with it that is used by the code that handles the request. the logcontext can be populated and passed to the handler in router middleware, see https://github.com/rocicorp/reflect-server/pull/238 and https://github.com/rocicorp/reflect-server/issues/250 
- [x] push and pull and anything else on the client that initiates a call to the server should include request id, see https://github.com/rocicorp/reflect/pull/51
', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('VWXO0N4fdf-KhdXcsXD4T', 'finish sorting out routing', FALSE, 1677094859000.0, 1672871889000.0, 'phritz', '@aboodman has started replacing our use of itty router with a custom thing. He or someone else should take that over the finish line, meaning at least:
- get https://github.com/rocicorp/reflect-server/pull/238 in _and manually test it with our existing sample apps_
- replace dispatch with the new routing and delete dispatch.ts 
- establish the pattern for CORS
', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('f32CMpps9G8iPf1V5jL9e', 'Implement whatever cost-reduction mechanisms we think are necessary', FALSE, 1677094859000.0, 1672783416000.0, 'phritz', 'I have lost track but our cost spreadsheets typically make assumptions that the client is disconnecting aggressively and similar. There is probably work to do to implement what we need.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('jnvkd4BC3WfzBxCc9yVD2', 'Add `mutationID` to `WriteTransaction`', FALSE, 1678763107000.0, 1672741807000.0, 'aboodman', 'Useful as a random seed and for other things.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('ABtgmV-PsHQxnlSahn33p', 'v0 docs', FALSE, 1709537669000.0, 1672741489000.0, 'aboodman', 'This needs some thought. The docs for Reflect can naturally be much much lighter than Replicache, but we still need a few really nice docs to get users going.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('Us6h6NpZMZ5xUJtKVOIaY', 'Revisit `allowConcurrency`', FALSE, 1677094860000.0, 1672726343000.0, 'aboodman', 'We have `allowConcurrency` set to true in our use of Durable Objects. This was done originally because the game loop serializes at a higher level, but:

(a) we are rebuilding the room DO in a different way and those old assumptions probably don''t apply
(b) the same `allowConcurrency: true` setting is being used in AuthDO where it *definitely* doesn''t apply, necessitating things like `_authLock` and `_roomRecordLock` which is adding significant complexity.

I think we should at the very least set `allowConcurrency` to `false` in the auth DO, and get rid of the manual locking there. But perhaps we should also remove from the room DO.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('gv4JD5Tag5mehVMUhGQ09', 'Clients stay connected too long', FALSE, 1684746193000.0, 1671673179000.0, 'aboodman', 'We see examples of clients in the logs that do nothing but ping for hours (like 12 hours!) straight. This costs money by keeping durable objects alive for no reason.

Probably a common cause of this is tabs being in the background and forgotten about. We could disconnect the socket on the tab hide event and reconnect on show.

It''s also worth implementing an "idle" feature: if a client doesn''t send any mutations for awhile, or receive any pokes, it disconnects itself. A potential problem with this is that if the tab is in the foreground, and this idle disconnect happens, then a collaborator could join and start doing things and user would not notice.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('PHPgfv6OYAOSsNEjhl64x', 'Monday: Users seem to take a long time to connect', FALSE, 1683763877000.0, 1671672014000.0, 'aboodman', 'From @noamackerman (https://discord.com/channels/@me/1040553788911661156/1055081386714857543):

@aa it seems we have degradation in number of failed connections (this is from the last 3 days):

<img width="1449" alt="1" src="https://user-images.githubusercontent.com/80388/209033985-183b7ece-6cad-44fd-8343-4f8349199be7.png">

In terms of absolute numbers, - 54 unique (and logged-in)  users that failed to connect is a lot for 3 days 
I''m also getting tickets from users who say Canvas does not load for them - and looking at the logs it seems like they can''t connect
looking at the increase in the number of these tickets and from generally looking at our metrics it seems something is off - is it possible that one of the last versions we deployed had a change that could cause it?
Anyway, I understand you want to wait to get more info before adding http fallback, but for me this is something we need to fix asap. Since we are time sensitive, I feel comfortable saying we need to add the fallback (or other solution you may find more suitable)
This is the room id of the last user that said he can''t connect - 0qXSAxeauYhR6owats8Dx0fekcv3VLmZ. 
I think that looking at data dog we miss some key data points. In DD you are looking at who connected eventually . In the screenshot below we are looking at a 10 seconds time window. It shows us the number of clients who moved from offline to online within 10 seconds or less. We can see that ~10% didn''t connect within 10 seconds or less 

<img width="1474" alt="2" src="https://user-images.githubusercontent.com/80388/209034023-24d2ea39-1c98-4452-8fa9-bfac5edb0d7c.png">

Here we are looking at 60, 120 and 360 seconds time windows 

<img width="1470" alt="3" src="https://user-images.githubusercontent.com/80388/209034056-6708c45c-4a1d-45b6-8057-d43db583d11a.png">
<img width="1453" alt="4" src="https://user-images.githubusercontent.com/80388/209034053-06f26fcd-9dfc-4f61-bfae-fb47c9006013.png">
<img width="1486" alt="5" src="https://user-images.githubusercontent.com/80388/209034049-d38ad9bc-8999-42ff-8fc9-fb5631df359d.png">

So the last one comes close to what we see at data dog, but this is a 360 seconds time window. So this is a critical metric to look at. We should be at a higher number within a few seconds', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('kyIsazaqxZ0MaMc2BPUNb', 'Cloudflare has logpush for workers now!', FALSE, 1684746248000.0, 1671671206000.0, 'aboodman', 'Woo, this means we get all unhandled exceptions in the log and also probably runtime issues like OOM! We should set this up for replidraw and document how to do it for customers:

https://developers.cloudflare.com/workers/platform/logpush/', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('ZVMaUpEXC2DMc58HGmCmN', 'Monday: User report: cannot connect: room 0qXSAxeauYhR6owats8Dx0fekcv3VLmZ', FALSE, 1683763973000.0, 1671666464000.0, 'aboodman', 'From datadog, this room has been accessed by two different IPs. The first IP is from Comcast Denver and it accessed this room and one other:

<img width="1568" alt="Screen Shot 2022-12-21 at 12 48 16 PM" src="https://user-images.githubusercontent.com/80388/209017532-743f4896-845e-4d08-9ea3-01c06cb368e3.png">

Neither room ever shows up in server logs. And we log the URL of the connection request extremely early in startup, so it is near impossible that if our code starts the room ID would not show up servers-side.

I am still suspicious that this isn''t just proxies and other networking hijinks though. I do see something else happening on the server around that same time, but I can''t be sure it''s from same client:

<img width="1169" alt="Screen Shot 2022-12-21 at 1 40 37 PM" src="https://user-images.githubusercontent.com/80388/209023813-baba15b4-be14-4a1d-a422-5c2e5d8d1f48.png">

It''s the "account" room starting up. Unfortunately, I don''t have the client-side logs for the account room so I can''t know if this is from the same device/user.

If it is from same user it would be interesting because it would mean some messages from that user *are* getting to server after all and something else is going on.

# Summary

So summarizing, for this particular user, all we can say with confidence is that they tried to connect but never did successfully.

There''s a possibility that they did actually talk to the server and then something else went wrong.

# Actions for this user:

* @noamackerman: Do you have the account ID of the user who reported the bug? Is it `14530460`? If it is the user that would be intersting.

# Actions to help debug things like this in the future:

Monday:

1. Can you please setup client-side datadog logging for the account room too?
2. It appears that you are setting the same disconnect handler for the account room as you are the canvas room, which is causing noise:

<img width="636" alt="Screen Shot 2022-12-21 at 1 29 06 PM" src="https://user-images.githubusercontent.com/80388/209022707-53dddd8e-54b2-4ffb-892a-828a03c0cbd9.png">

Note that the roomID is `account/`. If you do mean to run a disconnect handler here, you should guard against this state not existing yet when disconnect happens because it is possible for disconnect to happen before any message is processed and the state hasn''t been created.

Rocicorp:

* We need to add the client IP to the server-side log context so we can for sure know if a specific device *ever* talks to our server: https://github.com/rocicorp/reflect-server/issues/229
* We should document how to do logpush so Cloudflare can do and we can be certain we''re getting all errors: https://github.com/rocicorp/mono/issues/212', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('SdR6HejgAoHeOXZDxNk04', 'We should send the client IP in the server logs too.', FALSE, 1677094860000.0, 1671662636000.0, 'aboodman', 'This would enable us to join with client logs across rooms.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('NfGzs2Ek979noQug0u6is', 'Finally finish undo', TRUE, 1677703277000.0, 1671590382000.0, 'aboodman', 'We never shipped [undo](https://github.com/rocicorp/undo) because I (aaron) screwed up the design. It doesn''t cover one of the cases from the famous Figma article:

https://www.notion.so/Redoing-undo-c0183b91d12c4272a3482e3233cc2890', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('D0bqjZ7ARO4z7oKD-XOav', 'Audit all entrypoints and ensure they have beautiful clear error messages', TRUE, 1680636120000.0, 1671590071000.0, 'aboodman', 'Entry points include:
- methods or properties users call
- places we parse data from network or storage

See same bug in Replicache: https://github.com/rocicorp/mono/issues/49', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('nAUaIawWukZ0ILh81OI5o', 'Enable using watch()/subscribe() to maintain computed data server-side', TRUE, 1684746367000.0, 1671589975000.0, 'aboodman', 'People often want to maintain some in-memory state in the DO -- lookup tables or  computed values -- that they will need inside mutators. Without this they have to do expensive scans to recompute this data constantly, or else carefully maintain it as a side-effect of performing mutations.

What many people ask about and feels natural to them is an interface that feels more similar to the `Reflect` interface from the client, where they can use `watch()` and `suscribe()` via the API they already know.

This needs some thought because I don''t think we want people actually programmatically invoking mutators via the server, but perhaps we could subset the interface somehow.

Other common requests in this area:

- Is there some way an external process can invoke some change on a room?
- onDisconnect (if we do allow users to directly call mutators, then the onDisconnect API could be the same on client and server)
- Is there some way to do some work async (ie call some other system) then mutate after?', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('QJX3WHGHFxj4Jd5lMCV_K', 'Client-side room creation', FALSE, 1678186223000.0, 1671589049000.0, 'aboodman', 'We should bring back the simple declarative client-side room creation.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('DoPst6sihTnuPFQJyoOQs', 'Finish experimentalWatch()', FALSE, 1677704576000.0, 1671588980000.0, 'aboodman', 'See https://github.com/rocicorp/mono/issues/76', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('4YoRIX20muvgX9UTonFNY', 'Add whether we are on server to `WriteTransaction`', FALSE, 1678763108000.0, 1671588935000.0, 'aboodman', NULL, NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('t8JkuDLqepHnD-M9px6pc', 'Hide indexes feature', FALSE, 1677781527000.0, 1671588728000.0, 'aboodman', 'I don''t think we want this in Reflect initially. We are limiting data to 25MB, so indexes not really needed and it''s not really finished.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('hslhpa8d1IpVjc22fWGBs', 'Evaluate if we should return a ClientStateNotFoundResponse when a pull attempt is made on a room that is not open', FALSE, 1678220708000.0, 1671231873000.0, 'grgbkr', '        What does the client do when it discovers that a room it is trying to push to is not open? Re-try forever?

_Originally posted by @phritz in https://github.com/rocicorp/reflect-server/pull/211#discussion_r1050951501_
      ', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('ceHAdNti1cEW8D47EDxDm', 'IDB errors in Safari', TRUE, 1677093100000.0, 1671222363000.0, 'aboodman', 'From Monday logs:

<img width="1409" alt="Screen Shot 2022-12-16 at 10 25 41 AM" src="https://user-images.githubusercontent.com/80388/208183302-84a9a701-cc78-4eec-becb-a361ce843bc0.png">

clientID: ae56e50d

It doesn''t appear to stop the product from running, but is unexpected.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('jKyY9OC7B6RkQ2sJG9Dah', 'Update reflect-server to use @rocicorp/eslint-config', FALSE, 1677094861000.0, 1671210573000.0, 'grgbkr', '        no else after return... `@rocicorp/eslint-config` would have caught this

_Originally posted by @arv in https://github.com/rocicorp/reflect-server/pull/215#discussion_r1049352674_
      ', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('CHzWF5_HOZ_9Dr1ejtVuJ', 'Monday: Some users seem to disconnect/reconnect frequently', FALSE, 1683763969000.0, 1671091991000.0, 'aboodman', 'Sometimes in the logs we see people reconnecting a few times per minute. We should add a metric/logging to track the distribution of connection up-time.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('qjL0v-_SW6cpPNEduJnCi', 'revisit reconnect logic', FALSE, 1677094862000.0, 1670960790000.0, 'phritz', 'we probably need some backoff, currently it just loops like crazy', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('5Efy5Z6Q4sHNkjon62G0r', 'PullResponseOKSDD should use JSON for cookie', FALSE, 1677091391000.0, 1670931122000.0, 'arv', 'And not the `Cookie` type.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('bX_QF_IyWVB96VWPtvuRC', 'Implement assertCookie', FALSE, 1677091392000.0, 1670931059000.0, 'arv', 'The assertion method we are currently using is assertJSONValue which is not quite correct any more.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('CDe4S8jYbbE_YjPDjCJJg', 'Index redefinition test cleanup/commented code', FALSE, 1677670789000.0, 1670920420000.0, 'arv', '        Should this part be commented out?  Isn''t this where the index is redefined and what this test is testing?

_Originally posted by @grgbkr in https://github.com/rocicorp/replicache-internal/pull/458#discussion_r1046153911_
      ', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('P7VQ12P0oM_wetK1eb8yB', 'Change how we expose internal APIs to Reflect client?', TRUE, 1677090675000.0, 1670920249000.0, 'arv', '        I felt like I didn''t want to include these names in the compiled bundle.

Does reflect use these?

_Originally posted by @arv in https://github.com/rocicorp/replicache-internal/pull/466#discussion_r1043833649_
      ', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('e3LTAd2Ycn8zk4kKceyEf', 'Document how cookies are compared', TRUE, 1677090676000.0, 1670920108000.0, 'arv', '        Should we document how they are compared here as well?

_Originally posted by @grgbkr in https://github.com/rocicorp/replicache-internal/pull/484#discussion_r1046117384_
      ', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('EfDktb1UjMH7V8A-uVj9n', 'First-class support for partially syncing', TRUE, 1677093101000.0, 1670797262000.0, 'aboodman', 'This will be needed for Vercel to adopt Reflect. They are already using partial sync on their busier projects.

Imagine a deployment of comments on vercel that has hundreds or thousands of comments organized into threads. Threads can be "resolved" in which case they disappear from the default view.

Really what Vercel wants is a way to sync just the unresolved threads at first. Then if the user chooses to see resolved threads, then they want to get the rest of the threads, but perhaps paginated by last-modified or created.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('RynRjG25I_BAM3DJOqDmI', 'Ship experimentalKVStore', TRUE, 1677090676000.0, 1670640210000.0, 'aboodman', 'And remove the `withRead`, `withWrite` methods. I don''t think we need them -- they can be implemented by Replicache in terms of just `read` and `write`.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('3jz_BefDC360O5zZVF1YS', 'Introduce a Cookie type', FALSE, 1677091392000.0, 1670586565000.0, 'arv', '```
string | number | {order: string | number, ....}
```', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('VfuNdbCGYyHDLzfNsIqxU', '@rocicorp/reflect-server{-debug} package (same with client)', TRUE, 1677093102000.0, 1670566681000.0, 'aboodman', 'This would be very useful for Monday and other paying customers. I don''t want to make it publicly visible though for IP/legal reasons.

The package will have literally the exact same configuration as current one, just that the build will be unminified. Is the right thing to do a workspace with a shared common package with all the code in it, and then two top-level entrypoints?', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('g-ved13slCh2SylOTH-Eh', 'we need instructions for how to build your own thing in reflect-todo', FALSE, 1677094862000.0, 1670557584000.0, 'phritz', 'replidraw has https://github.com/rocicorp/replidraw-do#building-your-own-thing but reflect-todo has nothing...', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('89oMKPoYWax07t24iwPP9', 'we need a re-auth signal to the client', FALSE, 1677094863000.0, 1670556184000.0, 'phritz', 'we have this TODO: https://github.com/rocicorp/reflect/blob/main/src/client/reflect.ts#L313 but since the status code is opaque to the client we need to rely on an error message piped down from the server. if we get structured errors out of https://github.com/rocicorp/reflect-server/issues/206 then to close this issue it''s merely a matter of:

- ensuring we have a reauth hook that is triggered when we receive the relevant error from the server
- updating documentation to cover this', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('deF6IymyIRJjUt_G0bKPa', 'upintegrate error message piping into main, and make the errors structured', FALSE, 1677094805000.0, 1670556017000.0, 'phritz', 'As part of https://github.com/rocicorp/reflect-server/issues/232 we created a branch of 0.18.0 for monday that pipes connect error messages down over the websocket so the client can see them, see this pr: https://github.com/rocicorp/reflect-server/pull/205

We should make main have this behavior as well. Note that we''ll have to dovetail with similar one-off code for room not found errors that is already on main: https://github.com/rocicorp/reflect-server/blob/76d1e37dedc202fec525dccb64b52e22f0bfce42/src/server/auth-do.ts#L253

Right now the error messages are unstructured, meaning that the messages are a brittle signal for the client to take action on. For example, if the client might need to re-auth the user. we should make them structured and treat them like an api.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('sCZFFkpWjtQRxsQiy9jWN', 'have better protection against devs accidentally re-using a room', FALSE, 1709537709000.0, 1670519427000.0, 'phritz', 'we''re adding a simple check to 0.19.0 during connection, but we should do better. eg from https://rocicorp.slack.com/archives/C013XFG80JC/p1670461281534869?thread_ts=1670433034.094169&cid=C013XFG80JC

> having the room choose and persist a random value when it is created and return it to a client the first time the client connects. the client passes this in future connections and if it ever does not match what the server has then the server errors the client out.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('ziAKPSegyw9lpjiFq3z3k', 'Monday: Out-of-order poke', FALSE, 1709537714000.0, 1670496437000.0, 'aboodman', 'Noam had a room in a browser that would not start -- it never rendered any data in the app.

The room ID was: Y3n0Tx4DVQHtahaXrgdE8jKH9vn

When we looked at the console together, we saw:

- multiple socket connections for same room being opened, but none seem to be getting closed
- error messages like "WebSocket is already in CLOSING or CLOSED state" when moving mouse

When I find this room on the server I see that it experiencing "out of order poke" frequently.

This error doesn''t seem possible given the way connection works. The client sends its current version as an argument to oonnect.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('cq_yNS_2PFGbgkMVfUIGj', 'Monday: Large number of users seem to never connect', FALSE, 1683763941000.0, 1670456298000.0, 'aboodman', 'Noam reports that it appears a large number of users (up to 7%) never connect successfully.

Noam, can you please provide the following information to get us started:

- How is this metric calculated exactly? I know that you are using the [`onOnlineChange`](https://github.com/rocicorp/reflect/blob/main/src/client/reflect.ts#L55) event, but how are change in that value translated into this statistic?
- How often do you receive user reports of this problem? I know you''ve received one, but is that it?
- Have you been able to capture a debug log from either the client or server when this problem occurs?
- Have you ever been able to reproduce it?
- Do you happen to have user agent strings for the affected users?', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('_wqsHhqrLyNzqzeNwBRxj', 'eliminate log spam: Your worker called response.clone(), but did not read the body of both clones', TRUE, 1677093103000.0, 1670429266000.0, 'phritz', 'This log line appears any time we clone a request but don''t use the original. Since [dispatch always clones the request](https://github.com/rocicorp/reflect-server/blob/52c462517c9b5a44d46cd01d3760b59c3d714b46/src/server/dispatch.ts#L189) no matter what the handler is doing with the body, we will get this message on several paths including createRoom in the roomDO (but also auth-related calls). 

To eliminate this log spam we should read the body exactly once and pass its contents through to handlers in case they need it. Handlers should get the body from the parameter instead of through the request. ', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('A2g5IUtalmSIsZUT4tq5L', 'eliminate log spam: An event handler returned a promise that will be ignored', TRUE, 1677093104000.0, 1670429100000.0, 'phritz', 'One of our DOs causes CF to complain:
```
An event handler returned a promise that will be ignored. Event handlers 
should not have a return value and should not be async functions.
```
We should eliminate this log spam and potentially fix the problem. I verified this exists in 0.18.0, so it predates the GDPR changes. 

To repro `npx wrangler dev` and connect to it with a dev server. The worker output will include this line.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('wGxWtCO7rh5qTTOZIjV-Y', 'Update `experimentalPendingMutations` to include client id for pending mutations (or filter to the current client id?).', FALSE, 1677091393000.0, 1670356600000.0, 'arv', NULL, NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('IIJSH7IsERZHmvlPaVATf', 'Update `"allow redefinition of indexes"` test when persist exists. Without persist we cannot check that this works.', FALSE, 1677091393000.0, 1670355931000.0, 'arv', NULL, NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('OxUUoS8GGuW8tN31kfs36', 'Update concepts/perf with latest perf numbers', FALSE, 1677091394000.0, 1670270830000.0, 'aboodman', 'I know that v12 has improved significantly on bulk write performance, so I believe that this is now out of date at least on that dimension. We should update it, and then refer to it from the release notes.

It''s possible that we have improved significantly enough on other metrics that they should be updated as well.
', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('tJP-LuUyEizU4KGVEHfu3', 'Update persist benchmark to reflect perf diff in persisting snapshots vs mutations', TRUE, 1677090677000.0, 1669996398000.0, 'grgbkr', '        > @arv while their wasn''t much perf difference on my machine, there is some on the benchmark runner:
> 
> https://rocicorp.github.io/replicache-internal/perf-v2/
> 
> particularly for createIndex and persist.
> 
> I''m baffled as to why createIndex would be slower, I didn''t think flipping the DD31 flag really changed anything there.
> 
> I''m looking a bit at why persist is slower, but it is a different algorithm.

For persist, since we set up all the data to be persisted in the benchmark with mutations, in the new algorithm this is done by rebasing the mutations onto the perdag (as opposed to gathering chunks and persisting chunks in the old algorithm).  this is necessary for correctness.  Its more realistic that the bulk of the data to persist would come from a snapshot commit, with smaller amounts of data coming from local mutations.  We should update the benchmark to reflect the different perf of persisting snapshots vs mutations.

_Originally posted by @grgbkr in https://github.com/rocicorp/replicache-internal/issues/428#issuecomment-1335453904_
      ', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('WivK6UNoZ8wjsnawtbEf6', 'Remove `reasonServer` from public API', FALSE, 1677091394000.0, 1669995285000.0, 'arv', 'I think we can change the public API.

_Originally posted by @arv in https://github.com/rocicorp/replicache-internal/pull/424#discussion_r1038263987_
      ', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('3mkBB9ILsSZ5QhzswwmDZ', 'RFE: Make persistence optional (memory mode)', FALSE, 1709536435000.0, 1669921382000.0, 'aboodman', 'I think we may want to make persistence optional for a number of reasons:

1. Some customers don''t need it and it makes things more complicated for them. Examples:
 * Vercel doesn''t currently use persistence, but the presence of persistent data means they need code to delete idbs on logout for privacy reasons (#432).
 * Placemark eventually wants to use persistence to support larger drawings but for right now, they are blocked on that for performance reasons. So persistence just creates problems for them (ie rocicorp/mono#46)
3. IDB doesn''t work in Firefox incognito (#433), so we need a memory path eventually at least for that reason.

With Reflect the core of the featureset might very well move to multiplayer with persistence a valuable and optional addition rather than something that should be default-on. From the perspective of somebody who bought Replicache for multiplayer being able to just `persistence: true` seems absurdly magical.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('PT3jXRcu19PYHh-q7hV3_', 'Replicache doesn''t work in Firefox/Private Browsing Mode', FALSE, 1677091395000.0, 1669921182000.0, 'aboodman', 'This is because IDB doesn''t work there: https://bugzilla.mozilla.org/show_bug.cgi?id=781982.

In order to function in Firefox/PBM, we would need some memory-only fallback for Replicache.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('3tkiPrWToNgD4RhlwrM2D', 'RFE: enumerateCaches()', FALSE, 1709537728000.0, 1669918206000.0, 'aboodman', 'We have the awkwardly named `deleteAllReplicacheData()` :). But there are a few examples where we want to find all the Replicache instances for some other reason:

1. Vercel runs Replicache in the context of some host app. Their IDB data is intermixed with any Replicache data from host. They want an easy way to find and delete just their IDB instances (e.g., on logout). Right now they can do that by parsing our registry idb but it relies on private Replicache knowledge and is awkward.

2. An inspector tool for viewing Replicache data would ideally want some way to find all the Replicache data (and its format version, etc) so that it could provide a viewer for them.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('UORH6N6zcrn-pQ8nzqpaE', 'RFE: Support rolling back format upgrades', FALSE, 1677091395000.0, 1669859640000.0, 'aboodman', 'Thinking about this more, I do think it''s a bug that the product basically fails if you rollback. There''s no way to know ahead of time that you can''t do this.

So we should do *something*. Right now, Placemark says that when you do this, there is a "hash mismatch" error. Perhaps this is rocicorp/mono#47?

Big picture it seems like we need to define what should happen when you rollback. I don''t think it''s practical to say that we support mutation recovery. I think our options are:

1. don''t try to recover newer formats (ignore them)
2. delete newer formats (assumption: we are rolling back because something bad happened and there''s a chance we can''t read that data anyway)
3. delete all persistent replicache state', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('_s7mPxujoqBtybUn78cKJ', 'maybe send auth errors down the websocket?', FALSE, 1677094805000.0, 1669160057000.0, 'phritz', 'We send ''room doesn''t exist'' errors down the websocket when a user tries to connect to a non-existent room: https://github.com/rocicorp/reflect-server/blob/0c80ed4da3a1acbf4b9fa7cc0e1a4157d5ffaba5/src/server/auth-do.ts#L277. If we don''t do this then the client has no way to know what happened. We should _consider_ sending auth errors down this way too. Not sure clear cut tho because if someone is unauthorized I dunno if we should be enabling them to allocate websockets in our server.

', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('16bjNPfoHawGEaDZRHPwO', 'Make "hashes" less strict', FALSE, 1677091396000.0, 1669152917000.0, 'arv', 'We saw issues when people moved from 11.3b and 11.2 because the shape of the hash changed and the regexp we use in 11.2 does not match the hashes in 11.3.

We now have the same issue between 12 and 11.

One way forward would be to do a 11.2.1 where we relax the regexp.

Also, for the future we should just make the hash be of type string of a minimum length of X?', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('8P8fGLJ2uldgKFT5ERK1R', 'feature request: Clone room API', TRUE, 1677093058000.0, 1669113771000.0, 'aboodman', 'Monday requests a server-side API to clone a room. By clone what they mean is the data gets cloned, but not the clients or version. I think this would also be useful for offline scenarios (clone a room to work offline w/o conflicts).

https://discord.com/channels/830183651022471199/830183651022471202/1044194617094066207', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('tgIGh7Lmke7WUoduo-KP8', 'manually assert  ReplicaheOptions types', FALSE, 1709536564000.0, 1669067459000.0, 'cesara', 'problem: 

Currently we only have typescript validate the option parameter types but this can allow Rplicache to persist potentially invalid options. (e.g. name)

solution: 

Assert the ReplicacheOptions types prior to initializing Replicache.

', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('6Xj3jAEB5s2jWQnJWq6rK', 'audit error messages to ensure they are useful for identifying the cause/location of the problem', FALSE, 1709536584000.0, 1669056139000.0, 'phritz', 'Context: https://rocicorp.slack.com/archives/C013XFG80JC/p1669055894830389?thread_ts=1669053514.900719&cid=C013XFG80JC

> we have to prioritize doing an audit of all the entrypoints to replicache and making sure they have good error messages. If we''re going to ship compiled code, we have to make sure we always explain what went wrong because the stack is useless.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('63NUk-wOlEz2mX3Ba1XjU', 'Overlapped pokes flaky', FALSE, 1709536597000.0, 1669025950000.0, 'arv', 'I see this often on Firefox (on the bots only) but it also happens in Chromium.

This is for DD31

https://github.com/rocicorp/replicache-internal/actions/runs/3496201514/jobs/5885915939#step:8:83

```
 âŒ overlapped pokes not supported (failed on Firefox and Chromium)
      Error: Overlapping syncs
        at maybeEndPull/< (src/sync/pull.ts:581:12)
        at async*withWrite (src/dag/lazy-store.ts:167:19)
        at async*maybeEndPull (src/sync/pull.ts:548:21)
        at _maybeEndPull (src/replicache.ts:939:50)
        at async*poke (src/replicache.ts:1233:15)
        at async* (src/replicache-poke.test.ts:123:17)
```', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('ZffAHdq6EUF36pwBdx7VK', 'Document spaces better', FALSE, 1677091397000.0, 1668897563000.0, 'aboodman', 'Now that we have decided on spaces being a more fundamental part of Replicache they should be documented that way. There needs to be a page for spaces, perhaps under "Understand Replicache".

I think the key things to communicate are:

- Spaces are not currently "part of" the Replicache protocol (the stuff represented in push/pull). They are "outside" Replicache in that sense.
- But in practice ~every application of Replicache uses this concept.
- Key constraints on spaces

I also think it''s worth reviewing rest of doc and making sure that spaces are discussed at appropriate times. ', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('CpwNCPCX7E2ZqQ3FI5ztD', 'placeholder: maintability', FALSE, 1691541923000.0, 1668740935000.0, 'phritz', 'Recent experience with reflect-server has made me want to keep a list of things we should probably consider/implement before reps beta, so I''m adding this to the beta milestone.

Needed for reflect-server/reps maintainability:
- alerting on logged errors 
- plotting and alerting on a few critical metrics: restarts, latency, HTTP error rate, ws error rate
- capability to centrally sift logs by customer, roomID, not sure what else (not sure what we currently have, worth a review)
- ~uniform routing~
- a strategy for managing auth and room storage schema, i don''t want anything complicated but we have to know how to make changes

@grgbkr feel free to add, I''m treating this as a living list.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('-sGvypX4I-6NgCjMA7w0r', 'bad reused name', FALSE, 1677091397000.0, 1668464279000.0, 'arv', '        bad reused name

_Originally posted by @grgbkr in https://github.com/rocicorp/replicache-internal/pull/385#discussion_r1021684616_
      ', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('eI0jMVHQqf6y-vEGTUXSp', 'Would be good to disable these for the ''experiment KV Store'' test as well.', FALSE, 1677091398000.0, 1668464253000.0, 'arv', '        Would be good to disable these for the ''experiment KV Store'' test as well.

_Originally posted by @grgbkr in https://github.com/rocicorp/replicache-internal/pull/385#discussion_r1021685960_
      ', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('msfBFWwz1BHa9a4P0wpxj', 'JSONValue Optional Properties?', FALSE, 1677091398000.0, 1668422175000.0, 'arv', 'rocicorp/replicache-internal#364 

## Background:

- JSON does not allow undefined.
- IDB allows undefined
- We have historically allowed our JSON objects to have properties with an undefined value by using TS optional properties.
- We have filtered out undefined properties in some code paths
  - before writing to a store
  - but not when manually calling mutators

## Goals

- No cloning or freezing in release mode

### Nice to haves

- Get static (compile time) errors when passing in an undefined property.
- Runtime errors in debug mode when passing in an undefined property.
- No difference in behavior between debug and release mode.


## Alternative A

- Allow optional property values in JSON objects but do no filtering
- When pull happens these properties will be missing so reading them will get undefined again
- Footgun with object spread operator because it might replace existing properties with undefined values:

```ts
type Data = {
  id: string,
  name: string,
  description?: string,
};

type UpdateData = {
  id: string,
  name?: string,
  description?: string,
};


async function update(tx, updateData: UpdateData) {
  const oldData: Data | undefined = await tx.get(updateData.id);
  assert(oldData);
  const newData: Data = { ...oldData, ...updateData };
  await tx.put(newData);
}
```

If `updateData` has a property with value of undefined the above will override the existing property.



## Alternative B

- Do not allow optional properties in JSON objects
- One problem is that TypeScript is not very strict about this and fails to flag  incorrect usage. We can enable `exactOptionalPropertyTypes` and we can ask our customers to do the same. But, it is a pretty invasive change. Without `exactOptionalPropertyTypes` you can pass an object with and explicit `undefined` value into replicache and [TS will not complain](https://www.typescriptlang.org/play?useUnknownInCatchVariables=true&exactOptionalPropertyTypes=false&ts=4.8.4#code/KYDwDg9gTgLgBDAnmYcBSBlA8gOQGoCGANgK6oC8AUHHAD5wB2JRR1dcAzjFAJYMDmbegCMIEIsAIMhjEgFthwKDICCUKAUQAeTLkKlgAPhm6cWYQCtgAYxgBuSpVCRYCZKlPmrtuOTgAFAlgeYi0AbwBtAGtgRAAuTm4+fgBdBNN9MgBfQwcncGh4JBR0bHxiMgAmXxkmFhkuXgEZUXFJaRp6JgUlVXVNHTLM4ErjTtLcLxsYSrznQrcSz0tp6r9ImPjEptT0oYqRrLywtmLUADExXzgwuDYaYSCExuSAGnu4R4AvAH5npIE7xoWUcNAAZiQGLYeBAGHAwFAINZgBwOABGAAUYLECUuEAAlHtJisfCcaDQoMAYCQoHDsRAHMDQXAIVCYDC4QikSiOJUsTi4HjCRMzCSZjcPpTqbSWWJGXAQWwucjUZiwo8oAkAORorWvT4EL4JSEAE2AYL4wBNWXx8uVPL56qecC1lT1BqNcFN5st1ttlEVlDJiwuV3Wd3JBs12zeH2+-x27G9FoYVqBCuZrOhsPhiJV6P5EFxYmFy288GDFKpNLpcrYivBkOznLzDsLxYJRNF5eqlbgUprsoZ9eZ9tVGKd0Z17vjXoYZpTVptdtbqMdGu1bv1s+TvuXAcoQA).
- Detect this in debug mode and throw an error.
  - It is very likely that customers will only test replicache in release mode.
', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('-lyB2jG8ruMO0XLDAcM1-', 'Release blocked: HEAD doesn''t run on any sample apps', FALSE, 1677091399000.0, 1668314041000.0, 'aboodman', 'When you pack and install HEAD on any of our sample apps, it fails with below error:

<img width="1552" alt="Screen Shot 2022-11-12 at 6 28 01 PM" src="https://user-images.githubusercontent.com/80388/201505758-29c124ba-c887-4ebd-9406-3dc08a7557ef.png">

With `--debug` build, the unminified error is:

<img width="1552" alt="Screen Shot 2022-11-12 at 6 28 44 PM" src="https://user-images.githubusercontent.com/80388/201505770-c5b2bc69-fd5f-4a1f-bea0-97dfb36250c3.png">

For the record, crypto is defined:

<img width="1286" alt="Screen Shot 2022-11-12 at 6 32 30 PM" src="https://user-images.githubusercontent.com/80388/201505863-117e3d0c-3123-4747-b7fa-d3f449a7e085.png">

I bisected this to 217116d77f291ce166207919758a9e400f29a3d4. It''s not clear exactly how this commit introduced this.

One weird thing I see in the code that is probably unrelated:

<img width="1037" alt="Screen Shot 2022-11-12 at 6 33 12 PM" src="https://user-images.githubusercontent.com/80388/201505883-c398ab1d-b1d8-4c44-86bd-282d17da9cf8.png">

If `crypto` is undefined, we use `uuidNoNative` which ... proceeds to call `crypto`.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('BNyyrMVDWQYoraAdNu2bn', 'src/replicache.test.ts is flaky on Firefox', FALSE, 1677091400000.0, 1668083173000.0, 'arv', 'src/replicache.test.ts:

 âŒ pull (failed on Firefox)
      AssertionError: expected 3 to equal 2
      + expected - actual
      
      -3
      +2
      
      at r (node_modules/@esm-bundle/chai/node_modules/.pnpm/chai@4.3.4/node_modules/chai/chai.js:9[57](https://github.com/rocicorp/replicache-internal/actions/runs/3425553200/jobs/5706460873#step:8:58)4:12)
      at n.exports<[3]</t.exports/i.prototype.assert (node_modules/@esm-bundle/chai/node_modules/.pnpm/chai@4.3.4/node_modules/chai/chai.js:250:12)
      at p (node_modules/@esm-bundle/chai/node_modules/.pnpm/chai@4.3.4/node_modules/chai/chai.js:1409:11)
      at c (node_modules/@esm-bundle/chai/node_modules/.pnpm/chai@4.3.4/node_modules/chai/chai.js:7884:24)
      at src/replicache.test.ts:745:25', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('nIg73t7LpImlEz-rwv_eW', 'src/replicache-subscribe.test.ts flaky on Firefox', FALSE, 1677091401000.0, 1668082894000.0, 'arv', 'src/replicache-subscribe.test.ts:

 âŒ subscription coalescing (failed on Firefox)
      AssertionError: expected 1 to equal 0
      + expected - actual
      
      -1
      +0
      
      at r (node_modules/@esm-bundle/chai/node_modules/.pnpm/chai@4.3.4/node_modules/chai/chai.js:95[74](https://github.com/rocicorp/replicache-internal/actions/runs/3436369895/jobs/5729816322#step:8:75):12)
      at n.exports<[3]</t.exports/i.prototype.assert (node_modules/@esm-bundle/chai/node_modules/.pnpm/chai@4.3.4/node_modules/chai/chai.js:250:12)
      at p (node_modules/@esm-bundle/chai/node_modules/.pnpm/chai@4.3.4/node_modules/chai/chai.js:1409:11)
      at c (node_modules/@esm-bundle/chai/node_modules/.pnpm/chai@4.3.4/node_modules/chai/chai.js:[78](https://github.com/rocicorp/replicache-internal/actions/runs/3436369895/jobs/5729816322#step:8:79)84:24)
      at src/replicache-subscribe.test.ts:857:35', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('jef7IlxqYTf2FK_GqujAf', 'docs: Add something about offline usage when using out of bounds blobs', TRUE, 1677090538000.0, 1667987251000.0, 'arv', 'If we are offline when we try to upload or download a blob we get errors. For the upload case there needs to be code to try again.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('r-yUB5GoR9btnEGqzNr30', 'Fine-grained write auth', FALSE, 1683342026000.0, 1667864174000.0, 'phritz', 'https://www.notion.so/replicache/Reflect-Authentication-Authorization-and-Security-bf2b1a2e31844efca73c4aa453fdecdb#7993b8db69ea4cefa544cb22519f4af1', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('SHr0OH-dH5OzMQznNglDj', 'Add a new `user` option to constructor, make `name` optional', TRUE, 1677090539000.0, 1667677795000.0, 'aboodman', 'We have to explain everywhere that you should put the userID in `name` and that that is why it''s required.

It would be more user friendly to have a `user` option that is required for this purpose and then make `name` optional (I expect it would be rarely used, only in cases where you have more than one Replicache space per user).', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('paZ3kPMWit7ec3d077oQk', 'we should make it easier to debug failing tests', FALSE, 1677094806000.0, 1667619962000.0, 'phritz', 'The LogSink accumulates logging messages in reflect-server. Pretty much every debug, info, or error message I''ve needed to debug failing tests when factoring createRoom out of connect were in the log. However tests often use something like the [TestLogSink](https://github.com/rocicorp/reflect-server/blob/1954e4e842b9b398003390b2c05493c03d9a3c15/src/util/test-utils.ts#L102) which accumulates the messages, but does not output them. This is great when running passing tests, but when a test fails we want to be able to see the messages; a strong hint at the problem is almost always there. We should have an _easy_ way to dump those log messages when a test fails. Right now the only strategy is to either add manual `console.log` calls to the code, which doesn''t benefit from the log messages, or to find the assertion that failed and add something like `testLogSink.messages.map((m) => console.error(m))`, which often requires refactoring the surrounding code to ensure the log sink is actually available. 

We should have a mechanism that makes it easy to output log messages from a test that fails. I''m happy if they spew for all failing tests. But even better if we can do it selectively. Adding a line of code would be fine. 

Separately, it would be nice to be able to run a single test eg from the command line. 

', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('_vkx4XL8YLrJdj48myGlu', 'I am not sure why, but this type shows up in the generated .d.ts file, but not `PushRequestDD31`.', FALSE, 1677091402000.0, 1667554360000.0, 'arv', '        I am not sure why, but this type shows up in the generated .d.ts file, but not `PushRequestDD31`.

As far as I can tell, the type isn''t referenced by anything in the .d.ts file except the export list, so I guess it''s not doing a huge amount of damage. But it does seem like it would be documented which is unexpected and show up in intellisense and so on. Can we make it go away somehow?

_Originally posted by @aboodman in https://github.com/rocicorp/replicache-internal/pull/351#discussion_r1013796480_
      ', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('QJS5_XjX-Q2gjWnz8zRW7', 'Validate writes are correct JSON not just reads', FALSE, 1677094812000.0, 1667502691000.0, 'aboodman', 'From @noamackerman:

Received below stack trace, it crashed dev server. Error *not* reproducible.

```
RoomDO doID=910a47d52e70b6710b1d16119d54dda02031f31017821e37f1d06b1b0e4d0cda roomID=e191cy_DXMfTQQUU1hgddkPUm23wycj6 req=fpcgquiztm6 skipping mutation because error {"clientID":"8064160e-b095-451e-9443-7adfba4c7c45","id":5814,"name":"changeElements","args":{"info":[["textBlock-6z75h8jbFlF72kGEo1UhY",{"hidden":true}]]},"timestamp":1667501929902.5} Ge: At path: value -- Expected the value to satisfy a union of `union | array | record`, but received: [object Object]
    at nt (/Users/noamac/workcanvas/workcanvas/dist/index.mjs:162:13)
    at Pt (/Users/noamac/workcanvas/workcanvas/dist/index.mjs:139:11)
    at at (/Users/noamac/workcanvas/workcanvas/dist/index.mjs:632:29)
    at ri (/Users/noamac/workcanvas/workcanvas/dist/index.mjs:1670:12)
    at q.get (/Users/noamac/workcanvas/workcanvas/dist/index.mjs:654:13)
    at q.get (/Users/noamac/workcanvas/workcanvas/dist/index.mjs:654:13)
    at q.get (/Users/noamac/workcanvas/workcanvas/dist/index.mjs:654:13)
    at St.get (/Users/noamac/workcanvas/workcanvas/dist/index.mjs:1383:13)
    at getElement (/Users/noamac/workcanvas/workcanvas/dist/index.mjs:6809:14)
    at changeElements (/Users/noamac/workcanvas/workcanvas/dist/index.mjs:6933:21) {
  value: {
    type: ''textBlock'',
    id: ''6z75h8jbFlF72kGEo1UhY'',
    x: -6350.206713017092,
    y: -2008.6537282241807,
    fill: ''#000000'',
    fontSize: 36,
    width: 300,
    height: 43,
    cursorPosition: 1,
    attachedConnectors: {},
    textPosition: { x: 0, y: 0 },
    align: ''center'',
    zIndexLastChangeTime: 1667501917077,
    fontProps: 0,
    frameId: ''frame-2c48kqxnQQhJzxmKLamVf'',
    lastModifiedTimestamp: 1667501921904,
    text: undefined,
    textColor: ''#15BBB4ff''
  },
  key: ''value'',
  type: ''union'',
  refinement: undefined,
  path: [ ''value'' ],
  branch: [
    { deleted: false, version: 1993, value: [Object] },
    {
      type: ''textBlock'',
      id: ''6z75h8jbFlF72kGEo1UhY'',
      x: -6350.206713017092,
      y: -2008.6537282241807,
      fill: ''#000000'',
      fontSize: 36,
      width: 300,
      height: 43,
      cursorPosition: 1,
      attachedConnectors: {},
      textPosition: [Object],
      align: ''center'',
      zIndexLastChangeTime: 1667501917077,
      fontProps: 0,
      frameId: ''frame-2c48kqxnQQhJzxmKLamVf'',
      lastModifiedTimestamp: 1667501921904,
      text: undefined,
      textColor: ''#15BBB4ff''
    }
  ],
  failures: [Function (anonymous)]
```

Some questions:

1. What does this stack correspond to? Can we use the trick that @arv just did in Replicache to demangle the stack and see where this is coming from?
2. Why isn''t it persistent? From looking at stack I''m guessing this is happening when *reading* data from Replicache. Only way I would expect it to be not persistent is if:
  a. The value getting read was out of a pending transaction
  b. The value getting read isn''t read in every run of the app
3. Why isn''t the error better: it looks like what''s happening here are there are some undefined values in the JSON. The error should say what field exactly was undefined if that''s the issue.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('vStALsmM6QE6DhyDiTFte', 'change subscription onError param type to unknown', FALSE, 1677091402000.0, 1667487765000.0, 'arv', 'To be consistent with TS:

Change:

```ts
export interface SubscribeOptions<R extends ReadonlyJSONValue | undefined, E> {
  /**
   * If present, called when an error occurs.
   */
  onError?: (error: E) => void;
}
```

to

```ts
export interface SubscribeOptions<R extends ReadonlyJSONValue | undefined> {
  /**
   * If present, called when an error occurs.
   */
  onError?: (error: unknown) => void;
}
```', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('45o4ZNvHvQOD-7lnw_2Ud', 'add eslint rule for unnecessary async', FALSE, 1677091403000.0, 1666975101000.0, 'grgbkr', '        remove async... maybe add eslint rule?

_Originally posted by @arv in https://github.com/rocicorp/replicache-internal/pull/326#discussion_r1001599759_
      ', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('4CL6b_WYsSrA-CsdtAjFQ', 'Provide private source builds of Reflect for easier debugging', FALSE, 1709536620000.0, 1666296229000.0, 'arv', 'The following comment shows how to do this:

https://github.com/rocicorp/replicache/issues/985#issuecomment-1110708975', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('VUhmmw-1HmU-09h035uA9', 'Add DevTools custom formatter', TRUE, 1677090672000.0, 1666168138000.0, 'arv', 'https://www.mattzeunert.com/2016/02/19/custom-chrome-devtools-object-formatters.html

We could add a custom formatter for Store. Optimally we would want this for chunks but we need the store to follow those hashes so we can see the commits.

This would be enabled in tests only.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('RT9OCN_PhnDZgZJKK-VSo', 'Get rid of IndexChangeMeta all together.', TRUE, 1677090540000.0, 1666118695000.0, 'arv', NULL, NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('Ag5p9iWOp9-DEap2jg6jy', 'Can we support large numbers of mutations per-frame?', TRUE, 1677090541000.0, 1665587844000.0, 'aboodman', 'A fairly frequent request for things like graphics editors is to be able to send many mutations per frame e.g., for multiselect and drag. However this currently causes severe performance problems locally:

https://discord.com/channels/830183651022471199/830183651022471202/1029741664799043595

I''m not sure what exactly is behind the perf problems or whether we can/want to support lots per frame, but it''s certainly something that comes up and people try to do.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('1Fv8DCZaPgZHVrHVTuK2o', 'Allow mutators to have multiple arguments', TRUE, 1686065391000.0, 1665349885000.0, 'arv', 'Right now mutators have one optional argument. We want to change it to a varargs.

We started of with a single argument and changing it to allow multiple arguments is a format version change.

With DD31 we are doing a format change so lets try to get it in there.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('XUkqCj2VMbXS8qw2jyjw-', 'Get rid of indexes param to newWriteSnapshot', FALSE, 1677091403000.0, 1665043671000.0, 'arv', 'With DD31 indexes can only be created in the genesis.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('J7PndUQpTfoaX6vujTDPM', 'bug: empty prefix for scan() scans all entries, not just user prefixed entries', FALSE, 1677094806000.0, 1665015474000.0, 'ingar', 'Logic is wrong here, this should always use the user value prefix when scanning the DO''s store.  As is, it returns all  entries, including internal ones.

https://github.com/rocicorp/reflect-server/blob/ca9f6316e0548d22a9b59b49ffe494d645ebdcb2/src/storage/replicache-transaction.ts#L99

Something like:
```ts
userValueKey(prefix || "")
```', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('WOUeYWWHfkscpp3MMjg9q', 'Implement B+Tree multi put and use that for patch', TRUE, 1677090543000.0, 1664966218000.0, 'arv', 'I seem to recall that doing a multi put is more efficient than inserting one entry at a time.

If the patch does not contain a `clear` operation we can sort the patch and do a multi put.

Also, if the patch contains a `clear` we can prune the patch operations before the clear.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('5j4vJjNL8XvXu8XOAgdZ-', 'POST_DD31_CLEANUP: No more need to support concurrent requests in `ConnectionLoop`', TRUE, 1677090543000.0, 1664900962000.0, 'aboodman', 'Ref: https://github.com/rocicorp/replicache-internal/blob/345df2b3594352dcd6cab64b58956711473892ee/src/connection-loop-delegates.ts#L11', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('F6tcpFPBlYqFuAAGAF_3C', 'POST_DD31_CLEANUP: Question about line in connection-loop.test.ts', FALSE, 1677091404000.0, 1664899985000.0, 'aboodman', 'Ref: https://github.com/rocicorp/replicache-internal/blob/17fbc0618b02610994875f45e63c7fb5d3d543ec/src/connection-loop.test.ts#L59

I''m confused as to why if `invokeResult !== true` then the logged line is prefixed with `f`. I assume `f` means `false`, but it seems like `invokeResult` could also be true in this case?', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('zvV2RenfHuqN3CiHcz8_u', 'Unexpected performance regression in Reflect 0.9 on Monday Canvas', FALSE, 1677091404000.0, 1664839588000.0, 'aboodman', 'Monday has been unable to update reflect (client) because when they move the mouse rapidly, rendering janks, sometimes for several seconds.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('pWMR6ND8jk3rJSnUisV3O', 'Indexes affect performance worse than desirable', TRUE, 1677090544000.0, 1664553725000.0, 'aboodman', 'Populate and persist scale almost exactly linearly with indexes. With 1 index it takes twice as long. With 2 it takes thrice.

I don''t think this is acceptable. I expect answer is (#212) but making this bug more problem-oriented in case other solutions are possible.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('m1apcipgZBQ6MvY0x-kRW', 'Add benchmarks for up to 100MB', TRUE, 1677090545000.0, 1664553538000.0, 'aboodman', '- persist @ 100MB
- populate @ 100MB
- writeSubRead 64MB -> 100MB

anything with indexes?', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('qNDHq-v9DJi9Q8Phkqi7p', 'pull mutate options flaky on Firefox', FALSE, 1677091405000.0, 1664438479000.0, 'arv', 'The bots often fail the "pull mutate options" test on Firefox

https://github.com/rocicorp/replicache-internal/actions/runs/3138671081/jobs/5098260368#step:8:62

```
src/replicache.test.ts:

 âŒ pull mutate options (failed on Firefox)
      AssertionError: expected [ Array(21) ] to deeply equal [ Array(21) ]
      + expected - actual
      
       [
         1000
      -  1040
      +  1030
         1060
         1090
         1120
         1150
      
      at r (node_modules/@esm-bundle/chai/node_modules/.pnpm/chai@4.3.4/node_modules/chai/chai.js:9574:12)
      at n.exports<[3]</t.exports/i.prototype.assert (node_modules/@esm-bundle/chai/node_modules/.pnpm/chai@4.3.4/node_modules/chai/chai.js:250:12)
      at l (node_modules/@esm-bundle/chai/node_modules/.pnpm/chai@4.3.4/node_modules/chai/chai.js:1467:9)
      at c (node_modules/@esm-bundle/chai/node_modules/.pnpm/chai@4.3.4/node_modules/chai/chai.js:7884:24)
      at p (node_modules/@esm-bundle/chai/node_modules/.pnpm/chai@4.3.4/node_modules/chai/chai.js:1406:11)
      at c (node_modules/@esm-bundle/chai/node_modules/.pnpm/chai@4.3.4/node_modules/chai/chai.js:7884:24)
      at src/replicache.test.ts:21[64](https://github.com/rocicorp/replicache-internal/actions/runs/3146198834/jobs/5114375312#step:8:65):22
```', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('CthUwWD206hFESRP8kDf5', 'One-mutation-per-push caused performance regression in canvas (monday) :(', FALSE, 1677094812000.0, 1664358118000.0, 'aboodman', 'More information here: https://www.notion.so/replicache/2022-09-27-95be9489ca484e3d8ec9a7c127e10171

I rolled out the change: d20fdc3b36dae36f8e0af84e80e0d81d7038f316

I don''t know why this happens, the profile is super weird. We don''t see rebases for long spans of time. Then a huge number at once. It seems like we aren''t getting pokes for long periods.

I can''t reproduce this on replidraw. I can easily reproduce on https://app-dev.workcanvas.com/.

Profile attached. More information when less tired :).

[Profile-live.json.zip](https://github.com/rocicorp/reflect-server/files/9663620/Profile-live.json.zip)
', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('MEtP8zRWMlRBupye-z2cU', 'We can get rid of the whole complicated updateClients (which was needed due to async hashing and indexeddb).  Callers can use setClients (and they need to put their own chunks).', FALSE, 1677091437000.0, 1664205709000.0, 'arv', '        We can get rid of the whole complicated updateClients (which was needed due to async hashing and indexeddb).  Callers can use setClients (and they need to put their own chunks).  

Fine to do as a follow up as this is already quite a big pr.

_Originally posted by @grgbkr in https://github.com/rocicorp/replicache-internal/pull/275#discussion_r978787914_
      ', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('562u6FSuRLUhO1WfLCOKU', '(dis)confirm that the auth do doesn''t end up proxying all ws messages ', TRUE, 1677093106000.0, 1663900342000.0, 'phritz', 'The worker does, but the auth do in our mental model shouldn''t. However the upgrade is issued from the auth do not the worker so i suppose it is possible. I can only think of bad things that happen if they do this but who knows. 

See this thread: https://rocicorp.slack.com/archives/C013XFG80JC/p1663122781349039', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('0OoBGWKQ-7QLSmC3AlQpk', 're-estimate cost of running reflect', FALSE, 1677094807000.0, 1663899945000.0, 'phritz', 'A lot has changed, let''s see where we stand. 
- [old, rough notes on the cost estimation](https://www.notion.so/replicache/Estimating-reps-capacity-3061e9e059734f9cb4ce27cfd502219c#776078509f7d437c90afb5b115186fb9)
- [old cost estimation spreadsheet](https://docs.google.com/spreadsheets/d/18Y7C0Wi5MaMfYtpEYXYEh6C3Mu2xqoFc0bvaaW0srGg/edit#gid=1522528439)

It should probably factor in our perf envelope (#63). ', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('RIOpt4qqn1wHFAbbVGLfZ', 'De-experimentalify watch', TRUE, 1677091221000.0, 1663807148000.0, 'aboodman', 'Supersedes rocicorp/mono#150.

I think we might someday want to do the other fancy stuff like filters, sorts, and joins so maybe we should redesign the API taking that into account? Or maybe we should just yolo and ship what we have.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('UIjwj9vMRKJWAM6JxYKO5', 'Expose experimentalWatch() in reflect', FALSE, 1677094813000.0, 1663806840000.0, 'aboodman', 'I think this is purely a matter of putting a `experimentalWatch()` on `Reflect`.

De-experimentalifying is a different matter: https://github.com/rocicorp/mono/issues/76', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('w-m-1MtivCOqjAD1DEVPY', 'make FPS safe and correct', FALSE, 1681147166000.0, 1663741115000.0, 'phritz', '## Background
- we are not doing [streaming reflect](https://www.notion.so/replicache/WIP-Streaming-Replicache-4acd7513121949f5898f7eeeeeaef96f) in the near term
- however reflect as currently built assumes the output gate is disabled, as it would be for streaming reflect
- unfortunately, without a complete streaming implementation, replicache is not correct unless the output gate is enabled
- monday has enabled the output gate, and [output gate enabled will become the default](https://github.com/rocicorp/reflect-server/issues/154)
- we therefore have a reflect that operates under the assumption that the output gate is disabled, which not longer holds. we need to make the adjustments that enable MP to work well when the output gate is enabled
- the primary effect of enabling the output gate is that a write to transactional storage takes ~30ms and prevents outgoing messages from being sent until it is complete
- @ingar has demonstrated that ws message sending throughput from DO to client is not materially affected by this additional latency. latency obviously increases, but throughput doesn''t change much. 
- i have demonstrated to myself that the maximum send rate of out outbound 4kb ws messages from a DO to ~50 clients is in the neighborhood of 8000/second. and that''s if the DO does nothing but send messages. It would be prudent to limit ourselves to say ~2000 outgoing msgs/sec.

## Current model
The idea in the current reflect server code is to send a poke to each client on every frame. Sixty times per second (so every ~16ms) the server creates a frame update. It applies all the mutations it has to in-memory data structures, computes the pokes, writes the data structures to transactional storage (with output gate disabled), and sends pokes to all connected clients. Mutations arriving during and between runs are queued for processing in the next run, and all available mutations are consumed during a run. The client replays pokes in order as soon as it receives them.

Aside from the not-correctness of revealing mutations before they have been confirmed, here are some other things that can happen that might not be desirable (or maybe we don''t care, dunno):
- reflect server doesn''t use information about the temporal spacing of mutations in the originating client. Two mutations that happened frames apart but are delivered to the reflect server at the same time get lumped into the same frame. (Similarly, two mutations that happened in the same frame could appear in different frames if delivery of the second is delayed). Not saying this is a problem we need to fix, just saying this is going to happen.
- similar thing on the receiving client end: it just plays pokes as they are received. Pokes that were sent representing separate frames that arrive close together get replayed in the same frame, and if the second of two consecutive frames'' pokes is delayed then there is a gap in client replay. 

When we enable the output gate we guarantee that we''ll end up making this kind of thing worse: we''ll group changes that span several frames into the same frame maybe on the server and receiving client, reducing apparent FPS on the receiving client. 

The fundamental problem is trying to do 30ms of work (flushing writes) in 16ms. If there''s a mechanism that causes the loop to iterate only every 30ms, eg backpressure or waiting for the last write to flush, then two frames of incoming mutations pile up while the write completes and end up grouped together in the next frame. If there''s no such mechanism then the loop just gets farther and farther behind (it continues to pile up 30ms writes followed by msg sends every 16ms, meaning clients get further and further behind). And either way, pokes only flow out every 30-45ms, depending on how it works, meaning tons of skipped frames. 

## What to do
It seems clear that we need to resurrect something like the old [game loop idea](https://www.notion.so/replicache/Replicache-Game-Loop-37d8d320ea6540ef8c1907dee26e232b) but simpler, where every N > 30 ms we process several frames of mutations into a sequence of updates to be played back sequentially. Aaron has said that if the loop turns every 500ms that would be a bummer bc of the latency hit but still probably ok. Luckily it seems like we could do much better. If the loop turns every ~4 frames (=4*16ms=64ms) it seems like the ~30ms write should have plenty of time to complete. 

Some additional thoughts:
- i don''t see any reason to queue up mutations and process them in a batch, it seems like we could process mutations as they arrive
- for efficiency''s sake (writes to storage including cache are expensive) we probably want to process mutations in memory and flush to storage at the end of every turn. This however potentially limits size of the data we can work with bc DOs are limited to 128MB including the cache (which I guess we can disable). 
- we do not have an explicit signal that writes have flushed. without one, I''m not sure how we could tell whether the server is getting behind. YOLO?
- i don''t think we should try to re-use the current loop implementation except for the in-memory datastore part. there''s a lot of complexity that seems unnecessary at this point
- there needs to be some mechanism for the receiving client to jump forward and apply mutations in batch if it has a bunch of frames but not enough time to replay them. 
- we have in the past been enamored of the idea that we could use mutation timestamps from the sending client to have perfect replay. the mechanism by which the sending client, server, and receiving client clocks are aligned is not clear and seems complicated, my guess is we could start with maybe server receive timestamp (or better, frame number) and see if it works
- let''s start low tech 
- we should have a way to tinker with the settings and see what it looks like under various assumptions. @arv i think had an amazing simulator for replidraw IIRC that automated a ton of clients all moving stuff around at the same time, something like that would be killer.
- note rocicorp/reflect-server#154 should probably come first? ', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('_7MfUNR1OlSWhzplIW2EY', 'Perf `populate 1024x1000 (clean, indexes: 1)` regressed', FALSE, 1677091438000.0, 1663600288000.0, 'arv', 'https://rocicorp.github.io/replicache-internal/perf-v2/

The `populate 1024x1000 (clean, indexes: 1)` regressed in 603bda224dfbab6731e8e8d41563cdd6b77d3426

Last good commit: 5b02db6a31bf59dc8b793966f345b092ad516a91', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('Q3YfcKImxaRAo9CUO8br9', 'Do not depend on getSizeOfValue in B+Tree', FALSE, 1677091439000.0, 1663594246000.0, 'arv', 'We use an approximate size of a value to figure out how to partitition the B+Tree. The thinking was that we wanted chunks that are close to a certain size to get efficient read/write to disk. Our abstraction is very far away from the actual disk IO and having to compute the approximate size is a bottle neck in https://github.com/rocicorp/mono/issues/79.

We need to figure out a better way to decode where to partition the B+Tree nodes.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('TCK1ahMSWldhL0R9h9kj7', 'Change LazyStore to not depend on getSizeOfValue', FALSE, 1677091447000.0, 1663594083000.0, 'arv', '`getSizeOfValue` shows up as a bottle neck in https://github.com/rocicorp/mono/issues/79

For LazyStore we use it as a heuristic for the amount of data to store in memory.

For LazyStore we can probably get away with say N chunks instead.

We would need to do some instrumentation to figure out what N should be.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('EbeuhaYtM2Z6OTR2qLYTP', 'Customer report: unexpectedly poor import performance', FALSE, 1677091439000.0, 1663349873000.0, 'aboodman', 'Discord thread: https://discord.com/channels/830183651022471199/830183651022471202/1020358780610953356

Summary:

* Tom (placemark) reports that import of a 20MB blocks main thread for 2-3s
* We expect 20MB to import in roughly 600ms from our perf tests
* The trace shows even commit taking 600ms alone, and put() is dominated by measuring the size of nodes (though note that everything is slower with trace running, typically ~2x).

Tom created a reduced test case here: https://codesandbox.io/s/gracious-sunset-ricd80?file=/src/App.js.

[trace.json.zip](https://github.com/rocicorp/replicache/files/9586083/trace.json.zip)
', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('cUeAIhPNRUHux1RxsHt5f', 'data for EU users stays in EU (GDPR)', FALSE, 1677094807000.0, 1663293806000.0, 'phritz', 'background:
- [monday request](https://www.notion.so/replicache/Monday-Priorities-shared-ce186403a079408abcdbb6aa123c48f8#9461759c182c46e6a6af5a0d3af15fb6)
- [durable objects support for keeping data in the EU](https://blog.cloudflare.com/supporting-jurisdictional-restrictions-for-durable-objects/)

I think we need at least the following questions answered to figure out what to do here:
- determine what governs whether a DO is pinned to the EU: whether the first client to connect is associated with a customer user in the EU? how do they tell us that? 
- if a DO is not pinned to the EU is it ok for an EU user to join its room and presumably create data that is not stored in the EU?
- decide whether metadata (eg authdo) has to be stored in EU? seems like EU just starts sucking in the world if so.
- confirm we don''t have to support migrating a room, eg if the user who owns the room moves to the EU or becomes a citizen or whatever
- confirm that monday''s lawyers view on all this lines up with ours


', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('KBWekcPn50kLp9V8V3MEo', 'delete a user''s data upon request', FALSE, 1677094808000.0, 1663292986000.0, 'phritz', 'background: 
- [feature request from monday](https://www.notion.so/replicache/Monday-Priorities-shared-ce186403a079408abcdbb6aa123c48f8#4591cf6710c9427fbfa04b825dc99738) (be sure to read the comments)
- [this conversation](https://rocicorp.slack.com/archives/G013XFG80JC/p1663122658077229) is related

Distilling it down:
- for each end user account the customer has, the customer keeps the list of rooms that user owns
- when an end user deletes their account with the customer they can request that their data be deleted too. ''their data'' means:
   - all rooms owned by that user and whatever is in them
   - all metadata about the user''s rooms
   - (unclear -- confirming this) room data on any client that is or attempts to connect to the room (?)
   - any of this user''s data in _other_ rooms has PII removed. **This would not be an easy thing for us to do** and it seems complicated to provide the interfaces that would enable the customer to tell us how to do it. Luckily monday already stores anonymized data in CF so this is a nop for us. Seems like customers that are real businesses are all going to need a solution here, so maybe we should be telling them to store anonymized data in CF? @aboodman ?
- it is NOT required (and in fact undesirable) to delete any data the user contributed to rooms they do not own

Seems like to close this issue we should do something like the following. Note that I am new to reflect so this might not be complete or even accurate.
- reflect gains a [new http interface](https://github.com/rocicorp/reflect-server/blob/255606bb461c418fe7a8de036a4df1dcb94a114b/src/server/dispatch.ts#L26) called `deleteRoom` that takes the roomID. It logs everyone out of the room (authInvalidateForRoom) and then calls into an interface on the roomdo that deletes all its data (`storage.deleteAll` -- read the docs for deleteAll, it might need to be called repeatedly until it succeeds). Once the roomdo has deleted all the data then the authdo should probably remove all the connection records for the room from its storage so `authRevalidateConnections` doesn''t go re-create the object. 
   - @grgbkr is there any other metadata aside from the list of connections that needs to be cleaned up? any cache? other list of rooms? etc
- decide whether a room can be re-created. my sense is ''no'' but it''s not a strong feeling -- to me a room name is a unique identifier and we are asking for problems re-using them.
   - if ''no'' then we need to decide the mechanism by which a deleted room is prevented from being recreated, as well as say what should happen when a client tries to connect to a deleted room. this last part is related to the question of whether we need to delete room data from all clients.
   - if ''yes'' then we need to say how we distinguish between a client who has data from the old instance of the room and one who has data from the new room. maybe the cookie just works for this, dunno. also the customer has to be completely sure they have scrubbed all metadata for the room (eg, perms) bc they definitely do not want old metadata to apply to a new room.
- let monday know when the api is available so they can start using it. they need to be sure that they clean up all metadata for the room on their end.

', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('g1J-K1Wj1lyepl3m7KIE0', 'Test on older/crappy/Android phones', TRUE, 1677093059000.0, 1663288597000.0, 'aboodman', 'See: https://discord.com/channels/830183651022471199/830183651022471202/1020113164437831750

We should test this at some point.

@phritz @arv you two use Android right? Can you test https://replidraw-do.vercel.app/ quick and see what you see?', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('gdMJp5JtT1YYdkWKJ-d9D', 'send mutations from the client as they happen', FALSE, 1677094808000.0, 1663285297000.0, 'phritz', 'There doesn''t seem to be an apparent advantage to batching them when sending over the web socket and by _not_ batching them we decrease the chances that a batch will run afoul of CF''s 1MB message limit. We are still open to an individual mutation being more than 1MB, that is out of scope for this issue. 

This is a high priority feature request from monday.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('K7XngRCzZgJJVEZiQ6JX0', 'Implement `ClientNotFound` behavior in Reflect', TRUE, 1677093060000.0, 1663278756000.0, 'aboodman', 'Replicache has a `ClientNotFound` message in the protocol that causes the client to auto-reload. This is super useful during development when it is common to delete server storage. Without this, it is common for developers to delete server-side state, then their open tabs start throwing errors in a confusing way.

To implement in Reflect we need to:

1. implement the server part of the protocol
2. make sure reflect has the version of reflect that has this code and wire it up', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('p5Fif_PwXzinwCvOJucOV', 'Add peerDependency on tested version of wrangler to reflect and/or reflect-server', FALSE, 1677094813000.0, 1663278429000.0, 'aboodman', 'This was @grgbkr''s idea. We can prevent bad dx where user build a new app based on reflect and gets a version of wrangler we have not tested by putting a `peerDependency` in reflect-server and/or reflect that expresses the version we will work with.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('9QHgQYv9VgCJGaikLDNlo', 'authHandler parameter to createReflectServer should be optional', FALSE, 1680045031000.0, 1663278154000.0, 'aboodman', 'It''s common when tire-kicking to not need auth and this just creates friction.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('EOZMl2CohCGenk9DOmvi2', 'Default output gate ON', FALSE, 1677094814000.0, 1663246567000.0, 'aboodman', 'Currently the output gate is default OFF and you have to set `allowUnconfirmedWrites` to `true` in the options to enable it. Let''s default the other way.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('vNwsE_q89dNPC9vl3HeZP', 'Switch to zod', FALSE, 1677703455000.0, 1663246500000.0, 'aboodman', 'reflect-server uses superstruct, but everywhere else in repliland, we use zod. We should standardize on zod I think.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('XVKywqA9fFHLhfQumEKvg', 'Audit fastForward', FALSE, 1679304621000.0, 1663246342000.0, 'aboodman', 'I believe that fastForward has a bug where if the client passes a cookie or lmid from the future, the server does not correctly reject the connection. Instead, the server just kind of  lets the client connect and waits for the server state to catch up with the cookie the client provided.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('yE6fo6hZkgpqm1O-W_TYq', 'Clean up packing and deployment of replidraw-do', FALSE, 1677704917000.0, 1663246209000.0, 'aboodman', '* pure React, not Next.js
* Move to CF pages, not Vercel
* Continuous deployment', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('hJLcOo1hA-3z3yKOg0fQR', '`room` -> `space` in API', FALSE, 1677094814000.0, 1663246108000.0, 'aboodman', 'We''ve settled on the terminology `space` elsewhere.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('hGFLxlfWmXMgHCyrkUWbq', 'Merge `reflect` repo back into `replicache`', FALSE, 1677704703000.0, 1663246077000.0, 'aboodman', 'API strawman:

The main difference between reflect and replicache at this point is just the socket interface vs http. So have two different option bags on `ReplicacheOptions`, one called `http` and the other `ws`.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('0-Jc6iqbtwPqVIFXmJ3OW', 'Not compatible with wrangler 2.1.4', FALSE, 1677094815000.0, 1663242199000.0, 'aboodman', 'When we upgrade replidraw-do to wrangler 2.1.4, we see this error on client connection:

```
GET /connect?clientID=9565b490-0bfd-400c-a460-da0668ebdc01&roomID=I2c1wt&baseCookie=&ts=50297.700000000186&lmid=0 101 Switching Protocols (6.22ms)
[mf:err] Unhandled Promise Rejection: TypeError: WebSocket already closed
    at WebSocket.[kClose] (/Users/aa/work/replidraw-do/node_modules/@miniflare/web-sockets/src/websocket.ts:191:30)
    at WebSocket.close (/Users/aa/work/replidraw-do/node_modules/@miniflare/web-sockets/src/websocket.ts:180:10)
    at Cr (/Users/aa/work/replidraw-do/node_modules/@rocicorp/reflect-server/out/reflect-server.js:1:23411)
    at /Users/aa/work/replidraw-do/node_modules/@rocicorp/reflect-server/out/reflect-server.js:1:27343
    at Je (/Users/aa/work/replidraw-do/node_modules/@rocicorp/reflect-server/out/reflect-server.js:1:10197)
RoomDO doID=a0124dccb0c0054e06a5de46ffd10204bb6cdf5ebfd0e09bf1d06b1b0e4d0cda roomID=I2c1wt req=si39y4fhjik client=e51d7b9e-2259-45f6-8133-64cebe600cf0 parsed request {
  clientID: ''e51d7b9e-2259-45f6-8133-64cebe600cf0'',
  userData: ''redacted'',
  baseCookie: null,
  timestamp: 55365,
  lmid: 0
}
```

This appears to actually be an issue with (wait for it) miniflare!

I''m starting to think Miniflare is not a great idea and we should mock out the dependencies for testing instead. Unfortunately, non-miniflare mode is also not working: https://github.com/rocicorp/mono/issues/292 :(', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('r9Z7YnNl4Yastlbp90SQG', 'Switch to `wrangler dev` (and away from `wrangler dev --local) for developing apps on reflect', FALSE, 1677094815000.0, 1663242170000.0, 'aboodman', 'We have had a lot of problems with `wrangler dev --local` ("miniflare mode"). We should switch  to non-miniflare mode because it runs on the real production system and will have less compatibility problems.

However currently non-miniflare mode is not working for most of our team and we need to figure out why.

Notably @ingar claims it works for him. But not greg and I.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('C2HQKc4ef_wrkOW4gnoqm', 'Initial "fast-forward" poke not sent until first mutation', FALSE, 1679687456000.0, 1663241947000.0, 'aboodman', 'I never noticed this because the sample app has tons of mutations due to mouse moves. But in a slower moving app, like a todo list, the initial data doesn''t show up until the first mutation after the client connects.

This is happening because the logic around when to send the fastForward poke is not correct. It is currently done in `processRoom` but `processRoom` only happens as long as there are pending mutations.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('Wv6ENlaDy7KBftIR2v7yg', '`tx` should be renamed `dbWrite`', FALSE, 1677091448000.0, 1663180225000.0, 'arv', '`tx` should be renamed `dbWrite`

_Originally posted by @grgbkr in https://github.com/rocicorp/replicache-internal/pull/252#discussion_r970931405_', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('6gSYfSTZo0vCn_PD4Q8si', 'should this be ClientID instead of string', FALSE, 1677091449000.0, 1663180197000.0, 'arv', 'should this be ClientID instead of string

_Originally posted by @grgbkr in https://github.com/rocicorp/replicache-internal/pull/229#discussion_r971014765_', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('ifZtIP406pHKav23mtYmL', 'Problems using exported `PushRequest`', FALSE, 1677091440000.0, 1663125890000.0, 'aboodman', '1. `Mutation` isn''t exported so you can''t use it for example for a parameter to some function that processes a push.
2. The `args` in `Mutation` is `InternalValue` not `JSONValue`.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('3aPdgUKvOirJqqDGWZCpk', 'Strip (or guard against) process.env.NODE_ENV==="production" in `out/replicache.mjs` output on npm', TRUE, 1677090546000.0, 1663087499000.0, 'jamischarles', '`import { Replicache, TEST_LICENSE_KEY } from ''https://unpkg.com/replicache@11.2.1/out/replicache.mjs'';`
On my demo I tried importing RC like this, but couldn''t initially because of this code:

`var et=process.env.NODE_ENV==="production"`
https://unpkg.com/browse/replicache@11.2.0/out/replicache.mjs

Once I polyfilled it, it worked great. 

I''d suggest guarding against that somehow so it can just be imported as an es6 module.

Keep up the great work!




', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('SOlle2-ebTy4xp3_1OMwn', 'Implement `scan()` in mutators', FALSE, 1677094816000.0, 1662803581000.0, 'aboodman', 'I started on this, but never finished. It''s in my local repo.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('iiz4kdlkl-0uxT-tWoOPF', 'RFE: Readonly view/mirror of an existing Replicache instance', FALSE, 1677091450000.0, 1662364919000.0, 'arv', 'One scenario that keeps coming up is to do full text indexing of data in a worker.

Some people move the Replicache instance to the worker but that has some overhead marshalling everything to the web worker.

Another option is to have 2 instances of Replicache. One on the main thread and one on the web worker. That has the problem that both of the instances have to `pull` to be synchronized. With DD31 the need to pull to sync goes away.

But even with DD31, one needs to provide the same mutators and indexes to get the instance to share the sync branch. It also needs to periodically refresh. 

Another option might be to provide a readonly view/mirror on top of an existing Replicache IDB. This would periodically run refresh or get triggered to refresh from other clients on the same sync branch.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('CuPT9CKpVUFRsq07M51wn', 'Fix incorrect flushing of write''s made by mutation that throws an error', FALSE, 1677866008000.0, 1661879835000.0, 'grgbkr', 'We should not flush changes made by a mutator that threw an error.  This could result in a mutator being executed non-transactionally (some of its write apply but not all).  

We still want to putClientRecord with new lastMutationID and putVersion, which will avoid continually trying to reprocess the erroring mutation.

Current code:

```
    const tx = new ReplicacheTransaction(cache, clientID, version);
    try {
      const mutator = mutators.get(mutation.name);
      if (!mutator) {
        lc.info?.("skipping unknown mutator", JSON.stringify(mutation));
      } else {
        await mutator(tx, mutation.args);
      }
    } catch (e) {
      lc.info?.("skipping mutation because error", JSON.stringify(mutation), e);
    }

    record.lastMutationID = expectedMutationID;
    await putClientRecord(clientID, record, cache);
    await putVersion(version, cache);
    await cache.flush();
```', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('Wgp3eU5OrhcKtJ0LrY4fm', 'Update the version field in PushRequest', FALSE, 1677091450000.0, 1661863457000.0, 'arv', NULL, NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('sOcAm32sRs1idyG7L4qHn', 'Add `branchID` and `clientID` to `Mutation`', FALSE, 1677091451000.0, 1661854203000.0, 'arv', NULL, NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('pvgwRLWBQcmtbXQydIcUl', 'Normalize index definitions', FALSE, 1677091451000.0, 1661763299000.0, 'arv', '`IndexDefinition` has optional properties but to make the code paths simpler we should normalize these on entry.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('vaga-cTK9yTB98XShYA5x', 'License server should be a subdomain of replicache.dev', TRUE, 1677090547000.0, 1661535577000.0, 'aboodman', 'Putting it on herokuapp looks kind of bad:

https://rocicorp.slack.com/archives/C02CT58AZ5H/p1661533754005729?thread_ts=1661532069.699159&cid=C02CT58AZ5H', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('TpxU6KF2rQJcgQpTd2SDT', 'Need a way to not send license pings direct from client for important customers', FALSE, 1677091441000.0, 1661534432000.0, 'aboodman', 'See: https://rocicorp.slack.com/archives/C02CT58AZ5H/p1661532069699159

@arv is it possible to have the compiled code check for environment variables? Perhaps we could have a DISABLE_LICENSE undocumented env var.

Or maybe there could similar be an undocumented client API?', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('F4sB9lLVF5Wpk6Pd1yR09', 'An error in persist prevents future persists from being scheduled', FALSE, 1677091441000.0, 1661304014000.0, 'grgbkr', 'In replicache.ts we have:

```
  private _schedulePersist(): void {
    if (this._persistIsScheduled) {
      return;
    }
    this._persistIsScheduled = true;
    void (async () => {
      await requestIdle(PERSIST_TIMEOUT);
      await this._persist();
      this._persistIsScheduled = false;
    })();
  }
```

If `this._persist()` throws an error `this._persistIsScheduled` is not reset to false, and no future persists will be scheduled.  `this._persistIsScheduled ` should be reset to false in a finally.

Something like:
```
  private _schedulePersist(): void {
    if (this._persistIsScheduled) {
      return;
    }
    this._persistIsScheduled = true;
    void (async () => {
      await requestIdle(PERSIST_TIMEOUT);
      await this._persist();
    })().finally(() => (this._persistIsScheduled = false));
  }

```

We should add a test for this.

', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('GojkNTEGE0yuF5O5rx7WU', 'De-Nextjs replicache-todo', FALSE, 1677091442000.0, 1659479589000.0, 'aboodman', 'Public bug: https://github.com/rocicorp/replicache/issues/1021

Next.js is not doing that much for us and adds mental overhead to the quickstart. Let''s get rid of it.

Let''s factor a `replicache-node` out of `replicache-nextjs`. I think we should go for pure node. No express, no nothing. I think this package would be server-only, no client (no client bits needed, because without Next.js the complexity of instantiating Replicache client goes away).

This will become the main quickstart. 

`replicache-nextjs` will remain as a thin wrapper around it `replicache-node` -- the main purpose is to support the nextjs integration and partnership.

We will also build https://github.com/rocicorp/replicache/issues/1016, https://github.com/rocicorp/replicache/issues/1015, https://github.com/rocicorp/replicache/issues/1014, and https://github.com/rocicorp/replicache/issues/1013 all depending on `replicache-node`. Woo!', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('KpVnCc00295evmfce4M-i', 'Indexes should probably be non-covering', TRUE, 1677090548000.0, 1659458101000.0, 'aboodman', 'I''m generally concerned that we implemented our indexes "covering" (that is each index contains a copy of all the primary data). This was probably a bad idea -- it just generates a ton more data to write. If we were making use of multiple threads for writing then ... maybe, but we''re not.

We should perf test making them non-covering, but I''m betting it will dramatically outperform on write, and only cost a little on read.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('g5yqTgtmjJNBfB8UuT3wZ', 'First-class incremental and window sync', FALSE, 1677091443000.0, 1659241408000.0, 'aboodman', 'It is possible to use Replicache cookies and other features to incrementally sync a large client view in chunks and/or to sync only a subset of available data. The Repliear demo shows how to do this and we have some docs on that.

We should consider adding features to Replicache to make this first-class.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('w11aPgBD5OzWd_wfpEYax', '`makeScanResult()` helper returns impl relying on `InternalValue`', FALSE, 1677091443000.0, 1657658482000.0, 'ingar', 'The `makeScanResult()` helper for implementing custom backends returns a [`ScanResultImpl`](https://github.com/rocicorp/replicache-internal/blob/273bee0bdc9d0fd3744b872211a0b36372174cc6/src/scan-iterator.ts#L352), which now uses `InternalValue`.  This causes an error: `Error: Internal value expected` when the `.values()` method is called (eg. in the replicache-todo backend).

', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('hah-xpgyGUYzoFWzjU9f1', 'RFE: A `deleteEverything()` API that deletes all Replicache IDBs', FALSE, 1677091444000.0, 1657166229000.0, 'aboodman', 'We get requests like this quite frequently: https://discord.com/channels/830183651022471199/830183651022471202/994362299764707357

Replicache maintains a `replicache_dbs_v0` database that lists all of our databases. There is presumably internal API somewhere that knows how to read this database-of-databases.

<img width="471" alt="Screen Shot 2022-07-06 at 5 54 36 PM" src="https://user-images.githubusercontent.com/80388/177687216-d28c4b3e-698c-41b7-97f2-124342f1a8ac.png">

We should add a global function called `async deleteAllReplicacheData()` that drops each one of the recorded databases, then drops the database-of-databases too.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('Zyp6cKng_eCdyX2pZi4IM', 'Rails "done done"', FALSE, 1677091445000.0, 1657087314000.0, 'aboodman', 'To steal a phrase from @phritz:

- [ ] Update Replidraw to use
- [ ] Update ReplidrawDO to use
- [ ] Update Repliear to use
- [ ] Rename to something else... replicache-crud?', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('a2QYIcY8XOP56qUNkoLcr', 'Maybe call this test `crossClientGetMutationID` or something to make more clear why it only runs under DD31. A comment would also help.', FALSE, 1677091452000.0, 1656407388000.0, 'arv', 'Maybe call this test `crossClientGetMutationID` or something to make more clear why it only runs under DD31. A comment would also help.

_Originally posted by @aboodman in https://github.com/rocicorp/replicache-internal/pull/177#discussion_r906419411_', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('NhOEDbtZQav19y4Cestl2', 'Why not adjust these tests to have a dag.Read. Even if most of the commit types / tests don''t need it, some do. Would be easier to read the test if you didn''t have to understand why it''s OK to pass null here.', FALSE, 1677091453000.0, 1656407353000.0, 'arv', 'Why not adjust these tests to have a dag.Read. Even if most of the commit types / tests don''t need it, some do. Would be easier to read the test if you didn''t have to understand why it''s OK to pass null here.

_Originally posted by @aboodman in https://github.com/rocicorp/replicache-internal/pull/177#discussion_r906418791_', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('7t6SB5rCJST-6CgnCuwnX', 'DD 3.1', FALSE, 1709536712000.0, 1655804122000.0, 'arv', 'Tracking bug for https://www.notion.so/replicache/DD-3-1-e42489fc2e6b4340a01c7fa0de353a30

### Infrastructure

The code will live on the `main` branch.

We will introduce a global flag called DD31 which will be `false` by default.

There are some public APIs that will need changes. During the transition period we will duplicate them and name them with DD31 in their names. We will however not export these from `src/mod.ts` so they will not show up in the npm package.

For testing we can run the tests twice, once with DD31 true and once with false.

- [x] Have esbuild strip this code
- [x] Setup the test runner to run twice.
- [ ] ~Run perf tests on both?~ 

### perdag

TBD

Verify that we have these three heads and no other heads

- [x] `mainBranches`
- [x] `clients`

### memdag

memdag has one new head:
- [x] `refresh`

### Clients

- [x] Add `tempRefreshHash?: Hash;`, including correctly managing refs on the ClientMap chunk for these hashes
- [x] Add `mainBranchID: BranchID;`
- [x] simplify updateClients once we go back to synchronous hashing (or uuids), today it uses a test and set approach to avoid async hashing in idb transaction

### Branches
- [x] Create Branch helpers analogous to Client helpers.   getBranch, getBranches, setBranch, setBranches.  
- [x] Ensure that the chunk that contains the BranchMap has refs to each Branchs''  headHash.

### Commits

- [x] Add `clientID` to `LocalMeta`
- [x] Change `SnapshotMeta` to use a map of last mutation IDS: `+ readonly lastMutationIDs: Map<ClientID, number>;`
- [ ] rocicorp/mono#64
- [x] Implement `getLastMutationIDForClient(clientID)` that walks the commit chain as needed. (64bfc27dbabedff31a6b8e977360850b5d720088) end up just calling this getMutationID(clientID)
- [x] Implement getNextMutationID on top of getLastMutationIDForClient(clientID) (64bfc27dbabedff31a6b8e977360850b5d720088)
- [x] rocicorp/mono#39


### Startup

- [x] Bootstrap from `main` branch with identical mutator names and indexes if one exist.
- [x] Otherwise, if there are existing `main` branches,  chose `main` branch with newest cookie and fork from it''s base snapshot (fixing up indexes as necessary and clearing last mutation ids).     
- [x] If no main branches exist, simply create a new `main` branch with an empty base snapshot.

### Persist

- [x] Implement `gatherLocalCommitsGreaterThan` (4c5dec18a2c5c36b01561f00dfafa01a5a19dcb1)
- [x] Implement `getLastMutationIDForClient(clientID)` that walks the commit chain as needed. (64bfc27dbabedff31a6b8e977360850b5d720088)
- [x] Implement/refactor `rebase`. Our code is not structured like that at the moment. We have `Replicache.p._maybeEndPull` and `sync maybeEndPull` which deals with this logic. (65557c14fc4ada6b2c3f2209b050998f601c36be)
- [x] Implement persist algorithm https://www.notion.so/replicache/DD-3-1-e42489fc2e6b4340a01c7fa0de353a30#92b4b58d07d14e9296ba8b975e312130 (ecbe9f1b8ff0f13c78a28578acb4e81ee8652ff6)

### Hashing and temp hashes
- [x] replace perdag async hashing with sync hashing (either wasm or uuid)
- [x] possibly replace memdag temp hashing with uuids, in which case need to add a new way to identify memdag only chunks for copying to perdag as part of persist

### Refresh

- [x] Implement [algorithm](https://www.notion.so/replicache/DD-3-1-e42489fc2e6b4340a01c7fa0de353a30#ecdb40894ff442c2ba7623600b63e3f1)
- [x] Figure out interval to run on, probably want to run after running persist to increase chance of base snapshot in perdag main branch >= then memdag main branch (59ab943174bdd9a94e58faba8a3c85a4e1cf4be7)

### Push

Updates the protocol slightly

- [x] rocicorp/mono#144
- [x] rocicorp/mono#145

### Pull

Pull now has to handle rebasing interleave mutations from multiple clients.
- [x] `gatherPendingMutations`
- [x] `getBaseSnapshot`
- [x] Update `pullVersion`
- [x] Include `branchID` in request
- [x] Add `isNewBranch` and set to true if previous snapshot has last mutation ids
- [x] Introduce new response field: lastMutationIDChanges = all last mutation ids from clients in branchID that changed between PullRequest.cookie and PullResponseOK.cookie
- [x] Construct new base snapshot lastMutationIDs from previous base snapshots lastMutationIDs and responses lastMutationIDChanges.
- [x] Remove `clientID` from `PullRequestDD31`
- [x] Check that pull response''s cookie >= than cookie sent in pull request and ensure error is logged otherwise (68b5673114c247d5463d45886e2c6b16f5a4e746)
- [x] Check that pull response''s cookie > than current snapshot cookie and discard pull otherwise (this can happenw hen refresh happens during pull) (68b5673114c247d5463d45886e2c6b16f5a4e746)


### Heartbeats

- [x] No need to update on persist

### Collecting Client-Side Client State

- [x] Implement collection of branches from BranchMap (063834e12fd84b6cfb714994399728324667862d
- [x] Ensure idb is not gc''d until all client groups are gc''d 
 
### Collecting Server-Side Sync State And Handling Missing Server-Side Sync State
- [x] On ClientStateNotFound server response disable client group and log error

### Indexes

- [x] Add `indexes?: Record<string, IndexDef>` to ReplicacheOptions.
- [x] Remove `Replicache.createIndex` and `Replicache.dropIndex`.
- [x] Correctly setup indexes as part of startup
- [ ] Eliminate IndexChangeMeta commits

### Mutation recovery

- [x] No longer recover from other clients on the same main branch in the same database
- [x] Update to recover branch at a time instead of client at a time.  We recover all of the branch''s clients'' interleaved mutations in a single push.  Client.mutationId and Client.lastServerAckdMutationID become Branch.mutationIDs and Branch.lastServerAckdMutationIDs.
- [x] Keep code for recovering client by client for now in order to recover from pre DD3.1 Replicache format (can remove in future).
- [x] Test that we can recover from pre DD3.1 Replicache format dbs 
- [x] Recover from other branches in same idb database, and from branches in other idb databases with the same Replicache name (and still supported Replicache format verison).

### User-Level Versioning and onNewVersion
See: https://www.notion.so/DD-3-1-e42489fc2e6b4340a01c7fa0de353a30?d=c966ca11d3574237a04e9832a0898039#1b0ff4c4034846febda91af009dfd84a
and
https://www.notion.so/replicache/Version-incompatibilities-528f68f02f9c43f2904cdfef1e35e9c3
- [x] Expose event for informing user when a new branch for the same replicache name has been created so they can aggressively reload (by default we should just reload the page).
- [ ] Document on user versioning (previous attempt https://github.com/rocicorp/replicache-internal/pull/12)

### experimentalPendingMutations
- [x] rocicorp/mono#38

### LazyStore

- [x] See if we need to rework GC.  There is some risk that refresh will thrash the LazyStore chunk cache with how GC currently work (but possibly not, as running diff as part of refresh may establish enough refs in the cache as to avoid GCing things that are actively in use)  (4378df2f13176e9065e10021fbd110c97411e25d, dff9c8d8e0c222b3051e7080333f87f3b376211b)

### Update sample apps
- [ ] todo
- [ ] replidraw
  - [ ] Make sure we support v0 and v1 of pull/push and manually test mutation recovery
- [ ] repliear
- [ ] others?

### Offline limits testing
 - [ ] what is first sync perf after being offline for multiple weeks
   - [ ] Time slice rebase
 - [ ] is there anything we can do to avoid rebasing all the offline mutations during first sync while back online
 - [x] one known issue is refresh and persist need basesnapshot cookie, and currently have to walk history to get it


## Reflect 
See https://www.notion.so/replicache/DD-3-1-e42489fc2e6b4340a01c7fa0de353a30#9629bd3b9f604aa18dd0072442eaded2
', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('p2DS_Ww4_P7n5OlZlTPY3', 'When server-side data is deleted things go wrong fast', TRUE, 1689319383000.0, 1655690311000.0, 'aboodman', '# Problem

We already have a special [ClientStateNotFound](https://github.com/rocicorp/replicache-internal/blob/main/src/puller.ts#L31) error when the server-side database exists but the requesting client''s state has been deleted. This causes Replicache to refresh, assigning a new client.

But what happens if the entire *database* is deleted? This comes up a lot during development, when e.g., a dev might delete entire server database storage.

We don''t have any concept of a "database" in the protocol directly, but in practice applications typically do. For example, all our sample apps use the concept of a *space* which is in the ID. 

What happens when a space gets deleted on the server side? This is not well defined by our protocol, and it practice,  e.g., in replicache-todo, it''s complex:

* For any pulls that happen before the space is re-created, it''s basically a no-op (the lmid defaults to zero and server returns empty patch)
* The next push from any client recreates the space with cookie:0
* All existing clients still have their old data, but don''t get updates initially because they are at cookie X but the server reset to cookie zero. They also can''t send new data because their lmid reset to zero, but they are at lmid > 0.
* New clients from an existing cache fork from existing clients. This means that they inherit that old client''s cookie! So they can''t get updates either! (for awhile, see below)
* But, the new client is at lmid:0, so they start sending pushes which are accepted. These pushes are accepted by the server, but not acknowledged (local cache is at cookie: X>0, server is at 0, so the pull is empty). Thus the local mutations are rolled back. For awhile.
* After enough pushes, the server''s cookie reaches X again and ''catches up'' with the client. After this, pushes from this cache begin being accepted again and the situation resolves.

Note that `ClientStateNotFound` doesn''t solve this issue. If the server returns that, it will cause existing clients to refresh, which just creates a new client forked from the old one.

# Proposal

Introduce a new `ServerNotFound` error which causes the client to delete the entire cache and reload. 

This can be done as a backward-compatible extension of the protocol, albeit a little ugly:

1. Define a `onServerNotFound` callback similar to the existing `onClientStateNotFound` callback. Default this to:
  * delete local cache (entire idb)
  * reload 
2. Extend the current pull response to also allow a new `ServerNotFoundResponse` with `{error: "ServerNotFound"}`. Wire this to the new `onServerNotFound` callback.
3. Currently the response to the push endpoint is ignored. Add an optional `application/json` response. If the response is JSON, parse it. Allow both `ClientStateNotFound` and `ServerNotFound` errors for the push handler and wire them to the appropriate callbacks.
', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('c0121ZvhhZvz7do4IEUWl', 'put during scan?', FALSE, 1677091453000.0, 1655388699000.0, 'arv', 'If we have a for await loop for a `scan` and we then try to mutate the map in the loop body.

```ts
mutators: {
  async test(tx) {
    for await (const key of tx.scan().keys()) {
      await tx.put(''e'', 4);
    }
  },
},
```

Today this dead-locks. We have an `RWLock` for the `BTreeWrite` and we create a read lock for `scan` and a write lock for `put`.

What should the expected behavior be?

The optimal behavior is that the scan is live and new entries _after_ the current entry show up and deleted entries _after_ the current entry are skipped.




', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('gsbadTprMp4-PWu6YEJzx', 'Change return type of mutators to be a Promise', TRUE, 1677091106000.0, 1655110856000.0, 'arv', 'It is a foot gun to return a non promise in a Mutator. We should change the return type from `MaybePromise` to `Promise`.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('LR3JYhtUJQa1m9H8d_axg', 'Recovering from a completely missing database', FALSE, 1677091446000.0, 1654815577000.0, 'aboodman', 'Right now if you delete the backend database that Replicache is talking to completely, the client does something ... weird.

For example on the replicache-todo app, if the server is deleted while the client is running:

- push will correctly see the client as new and skip the mutations because they don''t start at correct mutation ID
- pull will either return an empty patch (correctly) or if the server returns ClientNotFound then the client will reload

But the interesting part happens on reload. There''s now a new client ID.

- push will start processing mutations. The new client is correctly starting at clientID zero. A new space will be allocated.
- pull will not succeed initially (return no patch) because request cookie is higher than server cookie. But once enough mutations get sent it will start working. This means that the client will be permanently out of sync with the server as it has state from before the server rebooted that never gets taken away.

I think we need to have an explicit error code that means "the database you are trying to talk to doesn''t exist anymore, give up all hope, delete everything".

But probably also we should wait until the new offline-first stuff is designed as it might affect the way client IDs are allocated (or might not).', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('_7mrJDQchBc4gXQsuwJji', 'zero-dependency starter app', FALSE, 1677091447000.0, 1654778840000.0, 'aboodman', 'I removed the docker/supabase goop from the starter app, because it was buggy and confusing. But now it''s even harder to install because (a) there is a bigger chance of screwing up the postgres install and (b) you need a pusher account.

We need to do something about this. We need it to be one step to get started.

I looked into knex and was really hopeful. I got it working for postgres. But knex is not really a SQL abstraction, so many things break between it and SQLite. Example: SQLite doesn''t support boolean or date columns. It just seems really brittle.

Maybe the better thing is what @cesara suggested -- use pgmem. I discarded this in favor of knex because I was thinking it would be hard to test offline with an in-memory database (because can''t kill it and restart it). But I forgot -- you could kill it by pausing the process with ctrl+z!.

So I''m thinking the right path is maybe pgmem + ... something ... for poke. Maybe the something is server-sent events? I''m a little worried about doing our own websocket thing because I want to keep the complexity down in this special path.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('nZkhkY6yBwVSMHcFQ5ovP', 'doc: Document ClientNotFound error', FALSE, 1677091478000.0, 1654713850000.0, 'aboodman', 'Itâ€™s not currently on the PullResponse docs.

Also double-check that weâ€™re doing right thing for push? ', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('cxWMStEbXVwNwPSYiyRxO', '"Internal value expected" in replidraw', FALSE, 1677091478000.0, 1654063892000.0, 'aboodman', 'Running replidraw against current trunk, I see this error when I open the second tab for a URL:

<img width="508" alt="Screen Shot 2022-05-31 at 8 11 24 PM" src="https://user-images.githubusercontent.com/80388/171339164-cd686ce7-42ac-4dc6-a69e-54cbef7fd699.png">

', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('akrNUuHFpt68Mwmqz1_-9', 'Ship watch()', FALSE, 1677091484000.0, 1653699425000.0, 'aboodman', '# Background

https://www.notion.so/replicache/half-idea-for-productionizing-watch-cf3110a59db446a59848ea40f48b799b

# Problem

`watch` was an interesting experiment (https://doc.replicache.dev/api/classes/Replicache#experimentalwatch) and has produced large value in several of our customers'' projects, plus repliear. However, it also introduces undesirable complexity. Every app that uses it needs to implement the same pattern of applying the watch output to maintain a view. In repliear: https://github.com/rocicorp/repliear/blob/main/frontend/app.tsx#L313. In placemark: https://gist.github.com/tmcw/7d38380e0e2dee41ff6df18942742f65. Finally, `watch()` duplicates a subset of `scan()` but not all of it. It''s non-DRY.

# Proposal

What if we extend `ScanResult` with a `watch()` method? This way we immediately solve the duplicate interface problem. `ScanResult.watch()` then returns a stream of diff operations matching the params immediately passed to scan.

We can eliminate the view-maintenance code in React apps by a `useScan()` helper to `replicache-react` which calls `watch()` and does the view-maintenance internally. The `useScan()` hook would re-fire whenever its contents change in some way, but the identity of the values within the array would stay unchanged to facilitate use with `memo()`.

It may also be useful to add a `collect()` helper to ScanResult which does this view maintenance if other frameworks want it (needs investigation).

# Search Helpers

The above proposal only helps when the UI is directly displaying a scan, not filtering it or manipulating it in any way. In Repliear, for example, this wouldn''t completely solve our problem because of the complex filters.

However, by adding several more helper methods to `ScanResult` we can replace all the view management code in Repliear:

* `ScanResult::filter(f: ([k: string, v: ReadonlyJSONValue]) => boolean) => ScanResult`
* `ScanResult::sort(f: ([k1: string, v1: ReadonlyJSONValue], [k2: string, v2: ReadonlyJSONValue]) => number)`
* (maybe, not sure if there''s a need) `ScanResult::map(f: (k: string, v: ReadonlyJSONValue) => ReadonlyJSONValue) => ScanResult`

These method can be chained together so that in total you can say things like:

```ts
const watch = rep.scan({prefix: "foo", limit: 40, startAt: "fp"})
    .filter((k, v) => v.bar > v.baz)
    .sort(([v1],[v2]) => v1.size - v2.size)
    .map((,v) => v)
    .watch(diffOp => doSomething) // or toArray to just get results once!
```

The neat thing is that each of filter, sort, and map can be implemented incrementally so that the cost for updates after the first result is very low.

But the *really* cool thing is that you can then do in React:

```ts
const data = useWatch(scanResult)
```

... and the hook will re-fire whenever the data changes. The result will be an array, with only the values changed inside that changed. 

# Performance

A naive implementation of this would re-scan for each watch whenever the underlying data changes. But a better impl can re-use a single scan to update all watches.

# Impact

I believe that we could probably deprecate subscribe and replace it with this API. I am not familiar with anyone using subscribe for anything beyond what this API does, and this is much more efficient and easier way to do it. This would also enable frameworks like Solid which want to get access to the underlying values as they change so they can feed them into their own dependency tracking system.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('5gsbQgpFjeyn3M8Ylogf-', 'Expose pendingMutations API to reflect once available in replicache', FALSE, 1677094809000.0, 1653421121000.0, 'grgbkr', 'Replicache issue tracking this work: https://github.com/rocicorp/replicache/issues/490', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('gPhqCx-aYMqYuEYjVCm3J', 'onOnlineChange api', FALSE, 1677094809000.0, 1653420938000.0, 'grgbkr', 'Monday.com feature request.

Way to be informed when connection status changes.  Replicache has an onOnlineChange api for this purpose https://doc.replicache.dev/api/classes/Replicache#ononlinechange, need to adapt to socket connection impl of reflect.

This api would be added to the client Reflect class: https://github.com/rocicorp/reflect/blob/main/src/client/reflect.ts.   The implementation will also likely be in that file based on the existing  `private _state: ConnectionState`.   ', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('2wGcLhvqKsJE4NhB_rKg2', 'Can we replace DO stoarge with D1 (SQL db)', FALSE, 1677094810000.0, 1652991575000.0, 'grgbkr', 'Evaluate if this will meet our goals and provide customer''s with better visibility / tooling for their data store (i.e. it is currently very hard to see what is in your DO storage).

https://blog.cloudflare.com/introducing-d1/', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('JQtzzPWBxB1AeLUIV5vJs', 'Potentially move replidraw-do to Cloudflare Pages instead of vercel', FALSE, 1677094810000.0, 1652991426000.0, 'grgbkr', 'Simplifying the sample to a single cloud provider.  Need to make sure Cloudflare Pages meets our needs.

https://developers.cloudflare.com/pages', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('faKsp9DnD2o2RSWiwp7zm', 'Skip computing diffs when not needed', FALSE, 1677091484000.0, 1652901090000.0, 'arv', 'We know whether there are index scans or not and if there is no matching index scan we do not need to generate the "fake" diff.

Maybe we can find other optimizations here and skip other diff computations.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('Wplln6wUPMg-UpncZinnK', 'Cannot upgrade replidraw-do to reflect 0.5.0', FALSE, 1677094816000.0, 1652794343000.0, 'aboodman', 'When I try, and put two windows side-by-side, the source window does not respond to drag -- though the destination window shows the drag happening.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('U-ms75ISN0eWFMsfSPH7W', 'Upgrade to wrangler 2', FALSE, 1677094817000.0, 1652793175000.0, 'aboodman', 'Cloudflare released a new wrangler version, v2, which is at `wrangler` not `@cloudflare/wrangler`. I can''t easily get it to work with this code, when I try to run `npx wrangler dev` the application can''t connect to the socket on localhost. However connecting to the published version seems to work.
', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('qKdxI5MsM8FzdWz7jCd1x', 'show clear error message at startup if Replicache is being run in a non-secure context', FALSE, 1677091483000.0, 1652461662000.0, 'grgbkr', 'Replicache depends on crypto.subtle for hashing.  crypto.subtle is only available in secure contexts.  

If Replicache is run in a non-secure context (e.g. an http page), the following unhelpful error occurs

"Uncaught (in promise) TypeError: Cannot read properties of undefined (reading ''digest'')"
![image](https://user-images.githubusercontent.com/19158916/168333795-2b74f261-634f-4d0d-bf5c-eaeea7aeb94f.png)

Replicache should fail fast and with a helpful error message.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('S3CqyrqHSO6hfzNZN-VBF', 'Add keywords to package.json', TRUE, 1677090553000.0, 1652362945000.0, 'arv', 'https://www.skypack.dev/view/replicache shows missing keywords... We should add them to get better npm ranking/seo?', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('nUhUqD_bTwZiab-oN74OO', 'Do we need to prevent persist during poke?', FALSE, 1677091485000.0, 1652120617000.0, 'arv', 'This does not use the lock on poke yet. Filing an issue to review the situation there.

_Originally posted by @arv in https://github.com/rocicorp/replicache-internal/issues/95#issuecomment-1121430364_', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('850WT8f7wSQtZ2Wmjv17n', 'Docs: timestamp is not in the API docs', TRUE, 1690343278000.0, 1652090575000.0, 'arv', 'We added a `timestamp` to `Mutation` which is part of `PushRequest`. However, `timestamp` is not showing up anywhere in the docs.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('K7GhpXmlZM-bMHbJf02Mk', 'missing docs', FALSE, 1677704820000.0, 1651860216000.0, 'phritz', 'In rough order of priority:
- [ ] what replicache is / not good for
- [ ] maybe? replicache benefits for product owners
- [ ] persistence and startup
- [ ] integration guide update/overhaul
- [ ] auth (and mult-user https://github.com/rocicorp/replicache-internal/issues/66)
- [ ] we need a better discussion of diffs in https://trunk.doc.replicache.dev/server-pull, doesn''t have to be the full diff strategies doc
- [ ] paging data https://github.com/rocicorp/mono/issues/24
- [ ] shared mutators
- [ ] server requirements https://github.com/rocicorp/mono/issues/99
- [ ] isolation level https://github.com/rocicorp/mono/issues/96 (maybe goes into server requirements)
- [ ] versioning: https://github.com/rocicorp/replicache-internal/issues/83
- [ ] logging
- [ ] error handling https://github.com/rocicorp/mono/issues/100 
- [ ] garbage collecting old data https://github.com/rocicorp/mono/issues/97
- [ ] diff strategies: https://github.com/rocicorp/mono/issues/101

Other notes/feedback we have received:
- should probably have a top-level conflict resolution thing so it''s easy to find
- maybe should have a ''replicache mental model'' bit which crisply restates important elements of how replicache works for people already familiar with it (*without* explaining it). eg ''operations are memory fast''.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('kxmFJ2Dkl_jaW4tyDUq3u', 'Update to the latest beta of replicache.', FALSE, 1677094811000.0, 1651774383000.0, 'arv', 'we should be able to update to the latest beta of replicache.

_Originally posted by @grgbkr in https://github.com/rocicorp/reflect-server/pull/120#discussion_r866132680_', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('1pMwgmRZq30yxR8yWenLd', 'RFE: Make pusher/puller interface lower level', FALSE, 1677091483000.0, 1651739499000.0, 'arv', 'https://github.com/rocicorp/replicache/issues/575', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('LHh6KGh3-JhVj2samTs8-', 'useSuspense in replicache-react', TRUE, 1677091063000.0, 1651738861000.0, 'arv', 'https://github.com/rocicorp/replicache/issues/878

https://github.com/rocicorp/replicache-react/issues/19', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('9ngkocA1DYbF7C7291-jL', 'Test performance benefit of removing copies in read transactions', FALSE, 1677091482000.0, 1651738795000.0, 'arv', 'https://github.com/rocicorp/replicache/issues/885', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('SlToTgycCsnO07UfH4mBp', 'Doc: Add "Diff Strategies" section to recipes', FALSE, 1709536758000.0, 1651738735000.0, 'arv', 'https://github.com/rocicorp/replicache/issues/567', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('8q7Jjk_fvu1j3eSi00aD7', 'I think this is missing handling for indexes that are in basisIndexes but missing from this.indexes.', FALSE, 1677091481000.0, 1651693670000.0, 'arv', 'rocicorp/mono#106

I think this is missing handling for indexes that are in basisIndexes but missing from this.indexes.

Originally posted by @grgbkr in https://github.com/rocicorp/replicache/pull/974#discussion_r854478544', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('6eZ-NkNCr5Kg2hWBD3W5K', 'B+Tree key ordering', FALSE, 1677091486000.0, 1651661645000.0, 'arv', 'Right now we use `<` of `string` keys for B+Tree ordering.

For indexes our keys are `\u0000${secondary}\u0000${primary}` where `secondary` and `primary` are `string`s and `secondary` may not contain `\u0000`.

ECMAScript defines **IsLessThan** as a comparison if the code units (16 bits). https://tc39.es/ecma262/multipage/abstract-operations.html#sec-islessthan

The problem with this is that on backends we might not want to use 16 bit strings.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('PPViYslTtb0Bq1KHyMbJo', 'Need to export some of Replicache''s interfaces', FALSE, 1677094811000.0, 1651610358000.0, 'arv', '`@rocicorp/reflect-server` bundles a copy of `replicache` but it does not export things from `replicache`. For example `WriteTransaction` is not exported and `WriteTransaction` references concrete classes (such as `AsyncIterableIteratorToArrayWrapper`). This means that the types are not compatible.

1. `@rocicorp/reflect-server` should export select types from replicache (not everything ðŸ˜¢)
2. Remove concrete classes because they are not compatible.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('RSiJp12d7Cp0_-sbdh3xx', 'Cleanup: Get rid of VERSION  file', FALSE, 1677091486000.0, 1651604298000.0, 'arv', 'Let''s use the version in package.json instead of a dedicated file.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('Fj7117VgpCh38LCESNK0O', 'Add `allowNull` to `CreateIndexOptions`', FALSE, 1677091487000.0, 1651389331000.0, 'aboodman', 'See: https://github.com/rocicorp/replicache/issues/913.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('1WwWxmJrHtPBz49v4K9sl', 'include rep version in license active ping', TRUE, 1677091063000.0, 1651268133000.0, 'phritz', 'Depends on https://github.com/rocicorp/replicache/issues/845', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('B1bTC9qD6ERL5E0wh4y_a', 'time the TEST_LICENSE_KEY out after 5m, triggering kill switch', FALSE, 1677091488000.0, 1651267852000.0, 'phritz', NULL, NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('ZBfUbuN4fbheJpOEaVNbI', 'we should move perf tests to a different ec2 instance type', TRUE, 1677091064000.0, 1651183672000.0, 'phritz', 'We''re currently using t2.xlarge which has burstable CPU, meaning that it might (or might not!) get full cpu access when doing CPU intensive operations. We should probably move to something that has fixed cpu allocation. (I''m not sure how to get unvirtualized cpu in ec2, but if that''s an option then great, but if not, an m or c type might be best). Worth checking what cpu options we can specify.
', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('aSt6EUTfiaiflXux5-ooF', 'Recursion / infinite loop bug in "delete"', TRUE, 1677091065000.0, 1651049044000.0, 'arv', 'Internal bug for https://github.com/rocicorp/replicache/issues/985

```ts
// lazy-store.ts

  delete(cacheEntry: CacheEntry): void {
    const {hash} = cacheEntry.chunk;
    this._size -= cacheEntry.size;
    this._cacheEntries.delete(hash);
    cacheEntry.chunk.meta.forEach(refHash => {
      const oldCount = this._refCounts.get(refHash);
      assertNotUndefined(oldCount);
      assert(oldCount > 0);
      const newCount = oldCount - 1;
      if (newCount === 0) {
        this._refCounts.delete(refHash);
        const refCacheEntry = this._cacheEntries.get(refHash);
        if (refCacheEntry) {
          this.delete(refCacheEntry); // XXX here is the iloop! 
        }
      } else {
        this._refCounts.set(refHash, newCount);
      }
    });
  }
```

It looks like this can only happen if a Chunk references itself but that should not happen.

Let''s add some asserts in here as well as in when we create the meta array.

I''ll give Tom a special build with these asserts

@grgbkr FYI', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('RzLiAkwI-mbB_IYlWSqDu', 'Split perf-v2 bot into 2', FALSE, 1677091488000.0, 1650981842000.0, 'arv', 'Split the current perf benchmarks into two (on GH actions)

- One without the p95 runs which warns/errors on regressions
- One with the p95 runs which does not warns/errors on regressions
- The dash board will display both like today. It can just merge the JSON or run it the function over both json files.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('jxq8YIb0gEwLTUWn1XfbW', 'Fix package.json license field', FALSE, 1677091489000.0, 1650794875000.0, 'arv', 'This is not valid. There is a validation of these done by some tools.

We should do this:

```
{
  "license" : "SEE LICENSE IN <filename>"
}
```

@aboodman @grgbkr @phritz

_Originally posted by @arv in https://github.com/rocicorp/replicache-internal/pull/1#discussion_r857101588_', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('vY4VXqGpUGWhhjEJtMsZH', 'test issue', FALSE, 1677091489000.0, 1650665082000.0, 'aboodman', NULL, NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('6bnD-KvMSKUP59KqjkxZs', 'perf test output seems busted for the last ~week', FALSE, 1677091490000.0, 1650598645000.0, 'phritz', 'So I got the bundle and perf runners running in replicache-internal. You can see the bundle size check [run successfully](https://github.com/rocicorp/replicache-internal/actions/runs/2205455219) and see the data point for the recent commit `1c6460f` [in the graph](https://rocicorp.github.io/replicache-internal/bundle-sizes/). 

However for the perf test the [run ran successfully and detected a "regression"](https://github.com/rocicorp/replicache-internal/actions/runs/2205455218) and I got an email about it. But there was no data point for `1c6460f` appended to the [graph](https://rocicorp.github.io/replicache-internal/perf-v2/). The last data point is for `558d93c` which was a week ago. I thought this was a problem with replicache-internal''s setup, but it looks like [replicache''s perf graph has a similar problem](https://rocicorp.github.io/replicache/perf-v2/) -- the last commit there is `59e8869`. 

I doubt that it is coincidence that the next commit is https://github.com/rocicorp/replicache/commit/495d9b7f48e6ba67ff2cfdcecd7777477a23dcf1 which makes changes to the perf test. We don''t see any more perf test data points after that commit. 

I wonder if something changed that now when a regression is detected a perf data point is not appended because the run appears to have "failed"? Like maybe the exit code changed or something? ', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('_CnvrgvrxbYYRKbaSxKYP', 'I think this is missing handling for indexes that are in basisIndexes but missing from this.indexes.', FALSE, 1677091481000.0, 1650484834000.0, 'arv', 'I think this is missing handling for indexes that are in basisIndexes but missing from this.indexes.

_Originally posted by @grgbkr in https://github.com/rocicorp/replicache/pull/974#discussion_r854478544_', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('YRgQx77UCybmonAQy-D5L', 'Add engines field to package.json', FALSE, 1677094769000.0, 1648201762000.0, 'arv', 'To prevent nodejs versions and that trigger this miniflare bug:

https://github.com/cloudflare/miniflare/issues/215

Remove when miniflare releases a new version.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('c0unLajkmF9jsPqCAsyGr', 'Preview URLs', TRUE, 1677093054000.0, 1647910333000.0, 'aboodman', 'Once we''ve move to CF pages (#151 ) we could look into getting vercel-style preview URLs for PRs.

I asked on the Cloudflare Discord and (surprisingly) got a helpful response:

https://rocicorp.slack.com/archives/G013XFG80JC/p1647906508423839

There are at least two ways we could do this:

1. Use cloudflare''s "pages" product, which seems to have this built-in.
2. Generate new named "environments" at deploy time: https://developers.cloudflare.com/workers/platform/environments/
', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('JneOa0-sxjHCP_CWn9MKx', 'Send client-side logs to datadog too', FALSE, 1677094769000.0, 1647909438000.0, 'aboodman', 'See also: https://github.com/rocicorp/reflect-client/issues/12', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('YEyaRdVWx7Ho0FEA42R1_', '---- Prioritized Customer Requests Above This Line ----', FALSE, 1677093602000.0, 1647909156000.0, 'aboodman', NULL, NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('RNWQCv_4hbaVYMIRSFVjo', '`getLogLevel` -> `logLevel`', FALSE, 1677094770000.0, 1647672065000.0, 'aboodman', 'There is no need for this parameter to be a function, because it does not need to change over the lifetime of a worker. Changing it to a plain string reduces boilerplate for customer and also enables simpler implementation strategies inside our own code.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('pm8HuooXYxbgDWDJK4pwG', 'Make console logger always enabled', FALSE, 1677094770000.0, 1647671937000.0, 'aboodman', 'It takes four imports and a surprising amount of code to enable the DataDogLogger and the ConsoleLogger, something I think will be the most common configuration.

I''m struggling to imagine a case where one would not want console logging enabled. Can we force it to always be enabled and do the teeing internally so that the boilerplate required by customers can be reduced?', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('WvmdaY5OczICNTBRd4xPe', 'Investigate whether Cloudflare LogPull contains info on worker crashes', FALSE, 1677094771000.0, 1647373539000.0, 'aboodman', 'We currently log from inside a worker using datadog''s rest api. However this doesn''t allow us to capture information about workers themselves crashing, e.g., due to sending > 1mb of push data.

We should investigate whether this information about worker crashes is accessible via Cloudflare LogPull. If it is, this could be a good debugging aide in the future (and should be added to the recipes section of this repo''s README).

If it''s not then we should feature request to Cloudflare about adding LogPull and LogPush support to workers.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('1h3HiF7eonal9C7FKRR3o', 'Print roomID into log at server startup', FALSE, 1677094771000.0, 1647372670000.0, 'aboodman', NULL, NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('11ifzmeKegTKSINXyrdE5', 'Print roomID in connect handler', FALSE, 1677094772000.0, 1647372600000.0, 'aboodman', 'Here: https://github.com/rocicorp/reflect/blob/main/src/server/connect.ts#L61

This would help with debugging.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('XgIcGIjAf8FTov8V44Ycs', 'Accept an optional logger as a constructor argument', FALSE, 1677702981000.0, 1647372464000.0, 'aboodman', 'This way clients can arrange to send client-side logs to e.g., datadog', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('K1h3em6-Yvc0lz-8TvEvB', 'Another data loss report from Monday', FALSE, 1677094772000.0, 1647076885000.0, 'aboodman', '<img width="960" alt="Screen Shot 2022-03-12 at 1 18 00 AM" src="https://user-images.githubusercontent.com/80388/158012084-af52d8a6-948c-4ee9-acd6-2889dc7c37e5.png">

Noam said the customer did 25m of work which was lost on refresh.

This room ID doesn''t appear in the logs at all, and I don''t see anything abnormal in the logs from that period.

I asked Noam to check whether this room has any data on the server-side, haven''t heard back yet.

It''s not reproducible from what I understand. Still, this is a pretty awful bug for a customer to experience -- silently losing a big chunk of customer data.

Ideas, prioritized:
- We should capture client-side logs and send to datadog (https://docs.datadoghq.com/logs/log_collection/javascript/). Maybe there''s a clue in there.
- I think we are not logging enough information in `info` mode. For example, we should print the room ID on startup of the DO and also include the room ID in the connection request log.
- We should test ourselves whether [LogPull](https://developers.cloudflare.com/logs/logpull/) contains information about workers and durable objects for our instance. I don''t have clear info on whether or whether it should not according to the docs. If it does then maybe it will have information about platform-level exceptions. If it does *not* contain such information then we should put in a feature request with Cloudflare.
- Perhaps we should prioritize rocicorp/mono#310 more highly. I had originally thought that `onDisconnect` would be based on socket state, but maybe we should do a periodic mutation and check that it is applied and synced? That would be more foolproof and is what we were proposing Noam do but maybe we should just include it in Reflect?

Not sure what else we can do. Ideas?', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('g0a20gZfDDsfSAWbWoxpe', 'Licensing done done', TRUE, 1690343109000.0, 1646950633000.0, 'phritz', 'Original issue rocicorp/mono#133

### Strictly Required to launch
- [x] Replicache
   - [x] add browser profile id and include it in active ping, write it to db 
   - [x] use TEST_LICENSE_KEY in tests
   - [x] complain loudly if they don''t pass a key in
   - [x] complain loudly if license is invalid 
   - [x] client should take a logger from Replicache and we should ensure we are logging appropriately for licensing calls
   - [x] enable licensing functionality by default
 - [x] ToS and privacy policy
   - [x] finished
   - [x] posted to web site
   - [x] linked to from get-license
   - [x] pricing is up to date on web site and linked to from get-license
- [x] collect remaining information needed in get-license: commercial, agree to ToS, email, marketing opt out, etc. and store it in the customer table
- [x] ensure website / docs explains licensing, TEST_LICENSE_KEY, etc.
- [x] db
   - [x] compact migrations, reset db
   - [x] ensure db backup policy makes sense & do a restore so we know how it works
- [x] productionization
  - [x] licensing server has a level logger
  - [x] licensing server log collection to datadog
  - [x] know the story for server rollout and rollback
  - [x] pre-prod staging environment (full clone of prod)
- [x] walk through & polish
  - [x] with aa
  - [x] ~with a customer or two~ not gonna do it
- [x] add at least a version bit to licensing api calls, maybe a little more structure for errors too?
- [x] get an answer to https://github.com/rocicorp/replicache/issues/909 and update code if necessary
- [x] ensure admin pages are robust against bad or garbage customer-supplied input
- [x] make sure i understand the forward/backward compatibility of the api types

### Needed for done done
- [ ] productionization
   - [ ] uniform error handling in server (and in client)
   - [ ] do some db migrations that require data (not just schema) migration (this assumes a pre-prod env)
   - [ ] do a db migration rollback for practice (same)
- [ ] Replicache
  - [x] implement kill switch and trigger it on invalid license
  - [ ] send version in active ping
  - [x] time TEST_LICENSE_KEY out after N minutes, triggering kill switch
  - [ ] add timeouts to licensing api calls
- [ ] store full datetime in active pings table instead of just date
- [ ] integration test 
- [ ] monitoring (add some minimal metrics)
- [ ] alerting
  - [ ] on errors
  - [ ] on any monitoring signals we think are useful
  - [ ] on flapping, crashes, or other signals we get from heroku
- [x] billing walk through with aaron and susan
  - [x] make any improvements/tweaks required
- [ ] move admin functionality to its own server for safety
- [ ] are we using heroku HA?
- [ ] split the client and server packages once the dust settles
- [x] upgrade sample apps to 10 and ensure the sample apps pass a key in (set in the env, not in the code)
- [ ] we probably want to prune the active table so that our backups don''t get linearly expensive. logical backups will start to fail according to heroku docs around 20GB.
- [x] (aboodman) outreach to existing customers telling them to upgrade, how to get a license, and TEST_LICENSE_KEY
', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('L5yNDvtoZEn4Op6MRvGnb', '------ PLAYABLE BETA ABOVE THIS LINE ------', FALSE, 1677093606000.0, 1646772163000.0, 'phritz', NULL, NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('PwnCrgRHPjyUQC4hYvO6J', 'âœ‚ï¸âœ‚ï¸âœ‚ï¸ GA CUT LINE âœ‚ï¸âœ‚ï¸âœ‚ï¸ ðŸ›³ðŸ›³ðŸ›³ SHIP IT AFTER THIS LINE ðŸ›³ðŸ›³ðŸ›³', FALSE, 1677704847000.0, 1646770686000.0, 'aboodman', NULL, NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('jHMfshAirFdTUkTyPfdTH', 'Continuous unit testing', FALSE, 1677093607000.0, 1646761594000.0, 'aboodman', ':-/', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('Us0-8EbmbQBIYvkDkvQLl', 'Website Refresh', FALSE, 1677091491000.0, 1646707591000.0, 'aboodman', 'We''ve learned so much from our users on the march to GA - everything from how to talk about the product to what the product should be. The website should get a new coat of paint.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('8pE64EJT-yjuNPfrckhhl', 'Replidraw: âœ¨click effectsâœ¨', FALSE, 1677093608000.0, 1646707024000.0, 'aboodman', 'I would like to add water droplet like click effects to showcase 60fps.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('cOnsigzGN2MaN6wZhlpxs', 'Clean up Replidraw', TRUE, 1681243071000.0, 1646706949000.0, 'aboodman', 'Two cleanups I think we should do:

* We should use `clientID` from ReadTransaction, not pass it in. This code dates from before we had that feature of `ReadTransaction` and passing `clientID` all over the place is really messy.
* Generally the use of `useSubscribe` is just overdesigned. We should just have one subscribe at the top level like replicache-todo does and pass down raw data to the components. Use React''s `memo()` function to avoid re-renders when identity of input doesn''t change.

Somebody (aa, erik, cesar, greg) should do a pass of this code and make sure it''s a real nice example of using Reflect.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('3GlW0ZTQFt_dSJ1jVJhXD', 'Implement a smarter diff strategy for fast-forward', TRUE, 1684383883000.0, 1646706356000.0, 'aboodman', 'At connection, we "fast-forward" a client to the current state of the room. Currently this is done via brute force. Eventually we will need to do something smarter.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('arDVoFVGcnhEaAxICvuSc', 'Replace replidraw with replidraw-do publicly', FALSE, 1677093608000.0, 1646706257000.0, 'aboodman', NULL, NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('xJIY0eiS82XcikHemKc5g', 'Drive `late-mutation` and `late-poke` metrics both to 1:1000 on Replidraw (via manual testing)', FALSE, 1677093609000.0, 1646706066000.0, 'aboodman', NULL, NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('dUwt5mhJVsM0_6dnLIHOe', 'Add `late-poke` metric', FALSE, 1677093610000.0, 1646705929000.0, 'aboodman', 'We should have enough information in the poke to know whether we played it late. Add a metric to track this.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('fgZfIqf2NKYqI80U1YeAY', 'Add `late-mutation` metric', FALSE, 1677093610000.0, 1646705883000.0, 'aboodman', 'We should have enough information (via source-provided timestamps) to know when we are processing a mutation late in the process loop. Add a metric to track when this happens.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('hlhhXfhnNVXufEtCjccXA', 'Programmatic access control on server-side', TRUE, 1677093056000.0, 1646705730000.0, 'aboodman', 'Expose `UserData` returned by `authHandler` to mutators. Optionally also allow providing this on client-side to do same thing optimistically.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('GMRD7IaJH_4V0DRLP6hsE', 'Nail down Perf Envelope', TRUE, 1677093057000.0, 1646705499000.0, 'aboodman', 'Reflect''s perf envelope for alpha:
* Rooms can be up to 25MB
* 50 concurrents in a room supported, each moving in 10% of frames
* 1:1000 frames dropped e2e

Then:

* Document envelope', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('W5zvTqsrw-A_hT6R6Y2uI', 'Script in npm package to list room instances (and maybe delete?)', FALSE, 1677093498000.0, 1646705410000.0, 'aboodman', 'Thinking like:

```bash
npm run list-rooms ...
```', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('oB7iJ_cl641lfosZdXvPQ', 'Recipe for dev/preview/prod', TRUE, 1677093065000.0, 1646705368000.0, 'aboodman', 'We should use Replidraw as a way to figure out how to do this and document it.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('PE4kaK4nCKkHwH24WZopE', 'Monitoring and Alerting', FALSE, 1677704948000.0, 1646705323000.0, 'aboodman', 'Theory: expand the existing logging interface and DataDog concrete impls we have.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('-cYdezEfY1tZU2II3tN8M', 'Wrap top-level entrypoints so that unexpected exceptions go to DataDog too', FALSE, 1677704986000.0, 1646705287000.0, 'aboodman', 'Right now, I believe that some unhandled exceptions will propagate to the console but won''t make it to DataDog.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('OeqGbyKoZyURoaRcUCYQJ', 'Rate-limit reconnect', FALSE, 1677093500000.0, 1646705220000.0, 'aboodman', 'Can use similar strategy as in Replicache rate-limiting logic.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('tYL-OzLT2Nbd3wup_H5Hl', 'Publish reflect and reflect-client to NPM', FALSE, 1677093501000.0, 1646705166000.0, 'aboodman', 'Note: we are only planning to publish minimized/obsfucated builds, not source files.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('5AH5f4_WUUkjUBv1p08yS', 'Rework client API', FALSE, 1677093501000.0, 1646705122000.0, 'aboodman', 'Currently the Client API is a mishmash of Replicache and the quickly hacked together `Client` class. We need to combine into one `ReflectClient` or some such and and also hide all the stateless API details from Replicache that we don''t use.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('wnEQdT-7FW2WRTImdrBly', 'Replace zod with ???', TRUE, 1678376844000.0, 1646705030000.0, 'aboodman', 'Use superstruct because it is faster than zod. It is not as fast as
suretype but the code size of suretype is too large for our needs.

Closes rocicorp/mono#307', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('cIU1_Ipxb_LvoWwjS7Eni', 'Integrate licensing from Replicache to Reflect server', FALSE, 1677093502000.0, 1646704991000.0, 'aboodman', 'Once licensing is integrated into Replicache (rocicorp/mono#133) we will need to do the same in Reflect.

This involves:

- adding the script to generate a license to the reflect package
- accepting license key as param to both client and server
- validating the license at startup in both client and server
- pinging periodically in both client and server', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('Qbc1-oTPHc1ZBHdmYunQA', 'Enable licensing for Reflect', FALSE, 1677698531000.0, 1646704962000.0, 'aboodman', 'Spec here: https://www.notion.so/replicache/Reflect-Auth-CLI-v0-73d206a4dd8343aa91855f5bc4bac7c9.

There are basically two parts to this work:

1. Implement the `login` and `teams` commands in the `reflect` package, along with supporting schema and API changes in the licensing server (careful to not break existing Replicache usage!)
2. Plumb `licenseKey` through from `replicache` into `reflect` package', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('0sQUzcq3Xlc9ZhjX4_3av', 'v0 marketing website', FALSE, 1681146778000.0, 1646704792000.0, 'aboodman', 'https://www.notion.so/replicache/Reflect-Landing-Page-21f2981e8ce846a991c884bd76b01835', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('-R3lUiMlJGZOkojj3ifDf', 'Implement Repliear', FALSE, 1677091492000.0, 1646451304000.0, 'aboodman', 'We''ve talked for a long time about having a demo that shows off Replicache''s offline-first performance. As we created an homage to Figma for Multiplayer, I''d like to create an homage to Linear for offline-first.

I''m thinking:

* Runs on Next.js/Supabase using the shared typescript mutator pattern
* Uses the rowversion strategy on a per-repo basis
* Has a very small number of features:
  * Read/write plaintext/markdown body
  * Reporter
  * Create date
  * Last modified
  * Status
  * Append-only comments (each having an author and create date)
  * Labels
  * Search by:
    * text!
    * label
    * status
  * Sort by: last-modified, created
  * j/k navigation while in detail view
  * Styled to look like Linear
  * On startup asks to import from github to get a significant amount of data in there', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('nqDMl_RchEnwVwJ2iqLWx', 'Server-side API to invalidate authentication', FALSE, 1677093503000.0, 1646289836000.0, 'aboodman', 'An Auth API that supports invalidating auth tokens by user or room. See https://www.notion.so/replicache/Invalidating-Auth-shared-015bd173de8d45c4ab1c8d85f425f9f7', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('VhjxLu-RwPAKzUs6eSvb5', 'Expose connect/disconnect events', FALSE, 1677093504000.0, 1646203893000.0, 'aboodman', 'Two use cases have surfaced for knowing when a client disconnects:

1. On the _source client_ (the one that disconnected), it would be nice to update the UI to tell the user they are disconnected and collaborators aren''t seeing their chaangs.

2. On the _destination client_ (or maybe on server?) it would be nice to proactively remove client state that''s no longer needed, rather than having to play timeout games.

This requires some API design. For (2) the most direct approach to me seems to be an event on the _server_ that can be used to make some state changes. But that feels very special case. We could expose an event on other clients when some client disappears, but that would give client A the ability to modify client B which we might not want.

I''m also not clear if there''s some utility in a mirror connect event?', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('oFLCLE8py_AqJigfqVopj', 'It would be good to print the version of the server into the log at startup', FALSE, 1677093504000.0, 1646175391000.0, 'aboodman', 'Also to have a public API on `Client` that returns the version, so that web apps could print it up in the client at startup.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('g3ldLQv0tRPXdIxcjW7nW', 'canvas: drawing for long time crashes server', FALSE, 1677093505000.0, 1645862940000.0, 'aboodman', 'Splitting off from rocicorp/mono#301: Noam reports that even without copy/paste, he can still reproduce this crash just by drawing for a long period of time in the canvas app. Given the structure of the `drawLine()` mutation, this doesn''t seem like it would be because of the 1mb limit (because `drawLine()` only appends a point to a line, it doesn''t re-put the entire line).

I scoured the server-side logs in that bug and the only clue is the OOM reports (there are two of them in the log). So perhaps what is causing the crash in the case of drawing is unrelated to large uploads.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('d3TfT8wEJC0qqy4nnwbYw', 'Cloudflare log message: "Trace resource limit exceeded; subsequent logs not recorded."', FALSE, 1677093506000.0, 1645846385000.0, 'aboodman', 'There are several occurrences of this message in the cloudflare logs from rocicorp/mono#301. It appears that the way the logging works with CF is that all the messages from a single socket connection get grouped together and we stop receiving messages from a connection after some number of bytes.

Unclear if this affects the datadog logging. If it doesn''t then maybe we don''t care about this. But without datadog logging, this prevents us from understanding much about what''s happening in the server as we stop seeing log messages fairly rapidly.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('5Gws1JQqvka70AE11H8ov', '1mb upload crashes durable object', FALSE, 1677093506000.0, 1645846178000.0, 'aboodman', 'Splitting off from rocicorp/mono#301: we have confirmed experimentally that we can''t send a > 1mb push to CF. We should split large pushes into multiple socket messages I suppose.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('Er2XPJ-7DeyGmFpX7Wo0-', 'canvas: copy/paste of large numbers of objects fails', TRUE, 1677093068000.0, 1645666250000.0, 'aboodman', '*Note:* This bug is distilled from an earlier report and notes. Background here: https://www.notion.so/replicache/Data-Loss-During-Drawing-b6a0f836dc404cc89496b1d58aec5e33.

In the canvas app, if you scribble a bunch with the pencil tool, then copy/paste the scribble, then copy/paste all scribbles, then do that repeatedly... within about 10 or 16 copy paste cycles you notice that the copy/paste operation stops working. Meaning it doesn''t propagate to other clients open at same time.

Even odder, though, when you refresh the client, the data is lost entirely. It''s not present in the refreshed view either.

I''ve seen server logs that may or may not correspond to this event and they say the server exceeded its memory quota and was restarted. But I don''t *think* the number of mutations we are talking about are sufficient to do that?', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('w9YeAsQ-Yy9RQ3MJizBXs', 'Need to string logger through worker too', FALSE, 1677093507000.0, 1645168043000.0, 'aboodman', 'Right now the DO is extensively logged, but the outer worker not at all. When debugging things it would be super useful to have the outer worker logged too -- particularly for e.g., auth issues.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('t4tXK_VgdyN1C8RlVFImQ', 'Validate that stored user data is JSON before storing', TRUE, 1680637355000.0, 1645087311000.0, 'aboodman', 'We currently validate that stored data matched `userValueSchema` on read, but if that fails it''s already too late -- now the server won''t run :(. We should be validating on write (perhaps also on write) to prevent writing such data in the first place.
', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('2qrR0ZzEEJz8yFy5UnUqo', 'Feature request: some kind of script or tool to list room instances', FALSE, 1677093508000.0, 1644313813000.0, 'aboodman', 'Cloudflare has REST API for this, but it''s not that easy to use. Would be nice to have some kind of script in the npm package that lists rooms.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('BCiVEZ_YISmgEI1cjHifD', 'Feature request: a way to view and edit contents of room storage', TRUE, 1677093069000.0, 1644303943000.0, 'aboodman', 'Well this didn''t take long :-). One of our customers have requested a way to view/edit the contents of storage.

I would expect CF to be working on this, or absent that, somebody to have written the obvious npm package. But I didn''t find either.

Put in request with our contacts at CF and asked in their Discord. No answer yet.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('XfX-R3W5Cn7Wz7fta5Vxg', 'Collect client-side state of old clients from previous formats/schemas', TRUE, 1677091067000.0, 1643910324000.0, 'grgbkr', 'We can use the IDBDatabasesStore to find these old dbs.  
We can wait till all clients in an old db are older than 7 days and then delete the DB all at once (rather than GCing individual clients).
note: this implies we keep code that knows how to read and write the previous data format version in Replicache', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('DKq0lHnGahPQLCvMBTWnO', 'Implement server-side buffering', FALSE, 1677093508000.0, 1643707326000.0, 'aboodman', 'The current code plays mutations in the next frame after receipt. This is not always the right thing to do. There can be variability in transmission time from client to server which can result in multiple mutations from a client showing up to server in same frame. We wouldn''t want to play those together. Also in the opposite direction variability in tx time can result in the server "dropping" an input frame from a client.

The server should have a per-client buffer of mutations that is sized such that only 1/100 input frames is dropped.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('HMBd-5guc9dw6qwe1CaUk', 'Re-enable output gate (implies Implementing batched frames)', FALSE, 1677093509000.0, 1643707191000.0, 'aboodman', 'Right now in this codebase, every frame is flushed to storage individually. For cost reasons, we want to instead "turn" the game loop every fourth frame, and execute a batch of four frames all at once. Each frame will still send its own pokes, with timestamps so that they get played back on client at correct interval.

Previously this codebase had something like this, but it was behaving buggily and removed temporarily in https://github.com/rocicorp/replidraw-do/commit/aa97462d1f1857d328d72a7bbdc3d6a2eb9636d2. 

Note: We also will need to do this eventually for performance and correctness reasons. Workers are supposed to "gate" output over the web socket on writes to durable storage confirming. Right now, this gating doesn''t happen, which is a bug. But once the bug is fixed, all the sudden our output over the socket will slow way way down while waiting for storage confirms which take 20-30ms. This means that we will have to have a loop of 4 frames just to keep up with input.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('uW-teUPdWTHn35Tt9DHJ3', 'Design authentication', FALSE, 1677093509000.0, 1643660941000.0, 'aboodman', NULL, NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('ZmJzch57TWDw_TBZYo8u2', 'Demonstrate that durable objects is a good backend for Replicache multiplayer', FALSE, 1677093510000.0, 1643660443000.0, 'aboodman', '- [x] Rip out the old scheduling / game loop code and do something very simple - see how that performs rocicorp/replidraw-do#1
- [ ] Re-implement the game loop (perhaps using setInterval)
- [ ]  Load test (concurrent clients in a room, max number of writes per room)
- [ ] Verify cost scales as expected under load', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('rLXGEUaPaMah2pqe8BgYG', 'Pull "rep-client" down into Replicache proper', FALSE, 1677093511000.0, 1643660085000.0, 'aboodman', 'multiplayer Replicache is socket-based and has a stateful slightly different protocol than local acceleration Replicache.  The client side of this protocol is currently implemented in the `rep-client` directory of this repo. We don''t want every project to have to include this literally, and also the implementation of this client is quite inefficient and indirect due to the fact that it has to be implemented on top of the stateless API that replicache provides.

Replicache should directly and efficiently support the multiplayer use case without any of this client boilerplate required in replidraw-do.

What to do with the stateless interface is up for debate. I have considered:

1. Pulling it into a separate repo (ala replicache-react)
2. Ditching it entirely (users have to do it themselves)
3. Having it as an optional feature of `replicache`

Currently leaning more toward 3. But in any case the goal for this bug here is that multiplayer apps don''t need to copy/paste the client and that the client is efficient.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('Dkl2ie-SjYwyN3ma-vGSd', 'Infinite loop when restarting wrangler dev', FALSE, 1677093511000.0, 1643619617000.0, 'aboodman', 'Reproduction:

1. `wrangler dev`
2. Open tab to app
3. Kill server
4. Restart server

This happens because the persistent state for `wrangler dev` is deleted automatically when the server restarts and so from the server''s perspective it receives a mutation which is from the future for the existing client on reboot, and it can never apply it so it just keeps trying forever.

This is related to https://github.com/rocicorp/replicache/issues/335 -- the client should have been told it was unknown. But also there shouldn''t be a way for our server to iloop.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('5HahAMOKYo-llCP-RwZR4', 'Add a recipe page to docs about versioning data @arv', TRUE, 1677091068000.0, 1643107991000.0, 'arv', NULL, NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('ipoW51m5K6SD_Dfin2bke', 'Add recipe about how to detect that there are unsynced changes', FALSE, 1709536788000.0, 1643040648000.0, 'arv', 'This can be done by having the client set a dirty bit and having the server clear it.

```ts
const rep = new Replicache({
  name: ''dummy'',
  mutators: {
    todo: wrapWithDirty(async (tx: WriteTransaction, args: string) => {
      await tx.put(''x'', args);
      return true;
    }),
  },
});

function wrapWithDirty<A extends JSONValue, R extends JSONValue>(
  f: (tx: WriteTransaction, arg: A) => MaybePromise<R>,
): (tx: WriteTransaction, arg: A) => MaybePromise<R> {
  return async (tx: WriteTransaction, args: A) => {
    const rv = await f(tx, args);
    await setDirty(tx);
    return rv;
  };
}

async function setDirty(tx: WriteTransaction): Promise<void> {
  await tx.put(''/dirty'', true);
}

async function isDirty(tx: ReadTransaction): Promise<boolean> {
  return (await tx.get(''/dirty'')) === true;
}
```

This is not super obvious and it does mean that the mutators have to call `setDirty` or wrap the mutators. It does make the API a bit less convenient and we might want to expose a cleaner way.
', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('dJX4mqJ0gbmXUEBcIE2ek', 'gc has a bug when a head name is updated to the same hash', FALSE, 1677091480000.0, 1641490066000.0, 'grgbkr', 'If during a commit a head name is updated to the same hash  (through a single setHead or a series of setHead) we end up incorrectly increasing the ref count of that hash.

This can be fixed in `gc.ts` by checking that headChanges actually change the hash .  

Existing code in `gc.ts` computeRefCountUpdate:
```
  for (const changedHead of headChanges) {
    changedHead.old && oldHeads.push(changedHead.old);
    changedHead.new && newHeads.push(changedHead.new);
  }
```

fix:
```
  for (const changedHead of headChanges) {
    if (changedHead.old !== changedHead.new) {
      changedHead.old && oldHeads.push(changedHead.old);
      changedHead.new && newHeads.push(changedHead.new);
    }
  }
```

', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('sN3njrG2plDSyO-KGErre', 'add rebase speed target to perf envelope and add test', TRUE, 1677091069000.0, 1638479334000.0, 'phritz', 'MP at 60fps implies a strict constraints on rebase in the client. We should:
- add a target to the perf envelope that captures this
- add a benchmark that tracks this metric', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('987MObqQpXlQEoi2Rp9hf', 'update eslint config to enforce leading underscore for private methods.', FALSE, 1677091479000.0, 1637716073000.0, 'grgbkr', '
_Originally posted by @arv in https://github.com/rocicorp/replicache/pull/724#discussion_r755607584_

cc @arv ', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('-bJTkZ0z0hRWQc2uFxYqB', 'determine under what circumstances to accept an update in the MP world', FALSE, 1709599717000.0, 1637292977000.0, 'phritz', 'In the current world the PullResponse has the lastMutationID and cookie. We accept the new snapshot in the PullResponse if 
- [the new lmid is >= what we have](https://github.com/rocicorp/replicache/blob/16d2597163a88c65f51d6632bf1b03fd294769cf/src/sync/pull.ts#L115) AND
- [there is a patch or the cookie or lmid changes](https://github.com/rocicorp/replicache/blob/16d2597163a88c65f51d6632bf1b03fd294769cf/src/sync/pull.ts#L124) AND
- [the base snapshot hasn''t changed out from under us](https://github.com/rocicorp/replicache/blob/16d2597163a88c65f51d6632bf1b03fd294769cf/src/sync/pull.ts#L108) (indicating something else completed a pull while we were working)

Note that we''ll write a new snapshot if the server returns the same cookie or lmid as long as the patch is non-empty. The state associated with a lmid and a cookie is not unique. Intuitively this must be true because of out of band changes, but unfortunately it means that we can''t _know_ the client has the state we intend for it to have, because a cookie does not uniquely identify a state.

In the new world we''ll poke a series of updates composed of (more or less) a tuple of `(baseCookie, newCookie, lastMutationID, patch)`. We need to determine under which circumstances to accept an update, considering both the MP use case as well as the more traditional instant use cases (client does something like today''s pull, maybe without a cookie). 

I have an opinion about this that I will flesh out here, but I''m submitting this issue right now as-is so I have an issue to reference in code I am trying to merge.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('9-kGTzoWbU3ZkGIjsDu6D', 'Consider changing entries for btrees', TRUE, 1677091070000.0, 1637270184000.0, 'arv', 'Right now the entries of a BTree Node looks like `[key: string, value: Hash | ReadonlyJSONValue][]`. This is a lot of array objects.

We could either have one array for the keys and one for the values or we could have an array of alternative keys and values.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('oDBSiF8TQ-xKJoOX9OX5J', 'Reduce allocations in BTree read', TRUE, 1677091071000.0, 1637270059000.0, 'arv', 'For B+Tree read operations I think we can work directly on the chunk data instead of the Node wrappers.

Currently the code depends on "virtual dispatch" since what needs to happen for an internal node vs a data node is slightly different. That "virtual dispatch" can be manually handled with "if statements".', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('gDHgP0jMeH1b1ZKPjJg87', 'RFE: Run Perdag in a Worker', TRUE, 1677091071000.0, 1637269908000.0, 'arv', 'We can/should run the perdag in the worker. ~~That would allow us to use the native hash functions (we can precompute the hash of the chunks in the persist operation)~~ (We actually precompute the hashes outside the IDB transactions with SDD)

According to this [SO post](https://stackoverflow.com/questions/10343913/how-to-create-a-web-worker-from-a-string) you can create a worker from a string but it is not clear what CSP policies this runs under.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('R5MQzNdePV7LxZDudXBcn', 'p95 performance benchmarks are noisy, increase tolerance for p95 but not p50', TRUE, 1677091072000.0, 1636996283000.0, 'grgbkr', 'This will require a bit of a refactor, so that the p50 and p95 benchmarks can have different tolerances, currently they share one.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('e7MUe65zxiBtYx9-CM9ms', 'we should consider tracking GC perf', TRUE, 1677091073000.0, 1636677659000.0, 'phritz', 'Per https://github.com/rocicorp/replicache/pull/681#issuecomment-966725207. I don''t think GC rises to the level of core perf envelope metric, but it''s clearly a component of core per metrics (eg, rebase perf). Seems like it would be useful to have a baseline.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('SlE6AfVubu1fihXYt23Gg', 'doc: Recipe for multi user app', TRUE, 1677091073000.0, 1636136927000.0, 'arv', '- Include user id in name
- Use local storage to allow switching while offline?', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('3Lqsy1GR2WqtRXZcEYsFL', 'Perf tests runs for too long?', TRUE, 1677091074000.0, 1635372592000.0, 'arv', 'The tests were designed to run 5 iterations or 500ms seconds but I see tests running for 30s

https://github.com/rocicorp/replicache/blob/aa5d02439657400edfc55d837b57ae632de537b3/perf/perf.ts#L42

The way it is structured now, the sum is only summing up the time we are measuring. I think we should exit if we have 10 runs and a total of 5s', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('4V730VpXj_77aTZ9ChnO_', 'Replidraw r3 bugs', FALSE, 1677091492000.0, 1633969950000.0, 'aboodman', 'See https://github.com/rocicorp/replidraw/issues?q=is%3Aopen+is%3Aissue+milestone%3AR3', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('PQrfPCAXGBKNWgzrtkUSV', 'Consider using structuredClone function', FALSE, 1677091479000.0, 1633383521000.0, 'arv', 'Do some perf tests on the built in `structuredClone` function and use it instead of `json.ts` `deepClone` if faster.

It is available in Safari Developer Preview and Deno and is in a spec so it should come to other browsers soon:tm:', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('bV6F4SQcaGBW8OC2nrA2H', 'Licensing', FALSE, 1690343136000.0, 1632901452000.0, 'aboodman', '**NOTE: Licensing done done list: rocicorp/mono#119**

In order to implement our billing model, we need to track the number of unique clients each customer uses Replicache with each month. 



This implies that Replicache will ping some central server on startup with an accountid/clientid pair so that we can count. Replicache generates the clientid internally already, and the accountid could be supplied as a constructor parameter.

However, we do **not** want it to be possible to accidentally (or maliciously) use someone else''s account id  and charge their credit card. Therefore, accounts should be somehow tied to domains -- and you should only be able to construct Replicache with an accountid if the owner of that account intends Replicache to be used on that domain. (As a nice side effect this would allow account holders to know which domains are generating Replicache usage).

Another requirement is that we do **not** want to report domain names Replicache is used on to some central service. It''s common for companies to have internal, semi-secret host names, and it would be bad practice to collect those.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('rBSmSqQqxdpmP2W45ElO_', 'Replicache Performance Specs', FALSE, 1677705190000.0, 1632896484000.0, 'aboodman', 'This bug describes the current performance specifications of Replicache. This is intended to help developers understand how Replicache has been designed and tested to be used.

# Performance

## Data Sizes
- Up to 64MB per Replicache instance

## Scan
- 650 MB/s in release mode

## Reactive Loop
100 open subscriptions, 5 of which are dirty, each of which reads 10KB of data.
With 16 MB total data, all reads refresh after a write in:
- p50: 3.5ms
- p95: 6ms
With 64 MB total data:
- p50: 3.5ms
- p95: 25ms

## Populate 1MB
- With zero indexes: 25 MB/s
- With one index: 17 MB/s
- With two indexes: 13 MB/s

## Startup
- @100mb: p95 Read first 100kb < 150ms

# Correctness

## Offline

Pending but unpushed mutations are pushed to server when back online. For example, if pending mutations are applied in a tab while offline and that tab is closed (or crashes) while offline, they are sent next time the app runs (in any tab). The very last few moments of mutations (~5s worth) before tab close/crash may be lost.
', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('kBIhj35iwGtK9x7WaKBlX', 'doc: Add "paging in data" recipe', FALSE, 1709536814000.0, 1632728493000.0, 'aboodman', 'A start: https://roamresearch.com/#/app/aboodman/page/X3mhH5OCs', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('HpDqR9p-u7MaXLH9amS5o', 'doc: error handling recipe', TRUE, 1677090551000.0, 1632728420000.0, 'aboodman', 'Add a recipe demonstrating how to handle errors in mutators.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('Lfq4TTkY8hot7brCdnYXi', 'doc: server requirements', TRUE, 1690343026000.0, 1632728384000.0, 'aboodman', 'Add a page to documentation listing server requirements so you can easily determine if your db is compatible with Replicache', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('qSROcLgTbB8uqruKO2P02', 'doc: Document ExperimentalKVStore and related interfaces', TRUE, 1690342957000.0, 1632727574000.0, 'aboodman', 'https://doc.replicache.dev/api/interfaces/ExperimentalKVStore -- none of the methods or the other related experimental interfaces are documented.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('wCvNaE0GWyXeRMZXAJY-O', 'doc: add "garbage collecting old data" recipe', FALSE, 1690342899000.0, 1632727447000.0, 'aboodman', 'https://www.notion.so/Garbage-Collecting-Old-Data-6b3bb7f39a4447f7b77a465b013824b5', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('ifRidiHdNCNChjSOrwaHK', 'Doc: change isolation level of push to serialized', FALSE, 1690342830000.0, 1632726928000.0, 'aboodman', 'We should officially required snapshot isolation or higher for our push handlers. I think this is a reasonable tradeoff since Replicache sends many mutations in a batch. Also most modern databases are going to higher levels of isolation anyway.

Let''s change the integration guide to explicitly use serialized for its transactions/

https://discord.com/channels/830183651022471199/830183651022471202/884871312361807922', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('05dtxzYQLH_XJFMINALtS', 'Calling commit twice should throw', TRUE, 1677091076000.0, 1632264884000.0, 'arv', 'Also commit should be private

We have some code paths where we call commit twice.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('k-7h0a983HN22wjzjIZFJ', 'Idea: DeepFrozenJSONValue', FALSE, 1709536838000.0, 1630877666000.0, 'aboodman', 'https://github.com/rocicorp/replicache/pull/479 introduces a DeepReadonlyJSONValue, but the semantics are not quite ideal. You can pass a `JSONObject` to a parameter that is `DeepReadonlyJSONObject `, because the latter is just promising that *it* won''t mutate the data. It''s not saying that it wants a `JSONObject` that others can''t mutate.

@arv ways saying he thinks it''s possible to use the opaque type trick to get us semantics more like real immutability, where you can enforce that nobody accidentally passes you something that they will mutate behind your back.  I think that might be useful to us.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('XLvBCpfRGHsNPqIUCAHJM', 'ExperimentalKVStore: Move withRead/withWrite to Replicache', TRUE, 1677090671000.0, 1630665606000.0, 'aboodman', 'Why do clients have to implement this? It seems like given read(), write(), and release(), we can do this ourselves.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('0Gxp-kEk4sZzEmwXJVnZo', 'Test don''t show details of mismatched maps', TRUE, 1677091077000.0, 1630491141000.0, 'aboodman', 'I see errors like:

```
 âŒ maybe end try pull
      AssertionError: 2 pending but nothing to replay: expected {} to deeply equal {}
      + expected - actual
```

It looks like this has been fixed upstream in Chai, but a new release doesn''t exist yet: https://github.com/chaijs/chai/issues/1228. So all we need to do is roll Chai when a new release exists.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('4W7DAfubDy5JIJiMZ2ru6', 'Use a cursor to get consecutive KV values', TRUE, 1677091078000.0, 1628806363000.0, 'arv', 'https://jlongster.com/future-sql-web

> Because we keep a single transaction open for reads over time, we can detect when sequential reads are happening and open a cursor. Thereâ€™s a lot of interesting tradeoffs here because opening a cursor is actually super slow in some browsers, but iterating is a lot faster than many get requests. This backend will intelligently detect when several sequential reads happen and automatically switch to using a cursor.

For `dag::Store` we sometimes read/write consecutive keys:

https://github.com/rocicorp/repc/blob/main/src/dag/write.rs#L192-L200

and these keys are of the shape `''c/<hash>/d`,`''c/<hash>/m` and `''c/<hash>/r`. It seem like it would be beneficial to use a cursor in this case.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('iRNtQBs54QY3paG2NSZb5', 'Consider making subscribe cancel function a PromiseLike', TRUE, 1677091079000.0, 1619807318000.0, 'arv', 'subscribe is an async operation but we do not expose the promise so there is no (easy) way to tell when it is done.

We can make the returned function a "thenable"/`PromiseLike` by doing something like:

```ts
    const f: PromiseLike<void> & (() => void) = (): void => {
      this._subscriptions.delete(
        (s as unknown) as Subscription<JSONValue | undefined, unknown>,
      );
    };
    f.then = (a, b) => p.then(a, b);
    return f;
```


This would allow us to await the result. We can do more trickery to allow you do `const cancel = await rep.subscribe(...)` if we also want that.', NULL);
INSERT INTO "issue" ("id", "title", "open", "modified", "created", "creatorID", "description", "labelIDs") VALUES ('-UwGISGEIhOk4tbWCxVlj', 'If getPushAuth/getPullAuth set, let''s call it once before first push/pull', TRUE, 1677090669000.0, 1618609999000.0, 'aboodman', 'This is fairly minor but current behavior always results in one error. This is by design but it looks confusing/weird in the console until you remember what''s going on.

<img width="1030" alt="Screen Shot 2021-04-16 at 11 52 58 AM" src="https://user-images.githubusercontent.com/80388/115087681-52759a80-9eaa-11eb-90d0-4c6e54c624ac.png">
', NULL);


-- Inserts for comment table
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('0j2yr7gMiSbDoU4tij6fn', 'BqHDX7sPYX2E-m-JRyuGA', 1715878457000.0, 'Republishing unb0rks things, but they get reb0rked fairly quickly. A different error this time:

<img width="2503" alt="Screenshot 2024-05-16 at 09 53 16" src="https://github.com/rocicorp/mono/assets/132324914/a4d60d4b-b2d6-4bf8-9a4b-f34c88ab3e6d">

I suspect that we can only hold these transactions open for a certain time. Probably a knob in there somewhere.', 'darkgnotic');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('oh-wD1z1heLNzVsOa-cAM', 'BqHDX7sPYX2E-m-JRyuGA', 1715878795000.0, '`show idle_in_transaction_session_timeout;` says "1d", although I''m seeing the timeout happen before 5 minutes.', 'darkgnotic');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('hXl1Tcp8usaqOPzoNAGkF', 'BqHDX7sPYX2E-m-JRyuGA', 1715885392000.0, 'Data point: The connection seems to close fairly consistently after 4:30 minutes of inactivity.

<img width="1381" alt="Screenshot 2024-05-16 at 11 48 32" src="https://github.com/rocicorp/mono/assets/132324914/40b4ba9e-08e1-4b8f-bda0-694a57307976">
', 'darkgnotic');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('je8MEyGvelCYSH78yMZUd', 'fSzfZOsgffkU8XWuYie6g', 1715676224000.0, '<details>
<summary><a href="https://linear.app/roci/issue/ROC-12/zeppliear-exception-because-issue-missing-properties">ROC-12 Zeppliear: Exception because issue missing properties</a></summary>
<p>

To reproduce scroll down in Zeppliear.

[image](https://uploads.linear.app/be6d9e3c-d622-4339-b3a8-6f0d9478e889/674469ce-9dce-413c-a264-eaa5d8f8fd95/ce3d7065-a488-44ef-8731-176a21558894)

`row` is

```
{
    "id": "_0kcprVNTV",
    "issue": {
        "id": "_0kcprVNTV",
        "kanbanOrder": "0"
    },
    "labels": []
}
```

but the type of `row` is supposed to be `{issue: Issue; labels: string[]};`
</p>
</details>', 'linear[bot]');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('3eVI_f1o4a5XLYNQgt3WH', 'fSzfZOsgffkU8XWuYie6g', 1715677032000.0, 'This comes from

https://github.com/rocicorp/mono/blob/a221bec5b5e6dc9e675c1ad743c39dd7113f9bdd/apps/zeppliear/frontend/app.tsx#L129

and `filteredAndOrderedQuery` comes from:

https://github.com/rocicorp/mono/blob/a221bec5b5e6dc9e675c1ad743c39dd7113f9bdd/apps/zeppliear/frontend/app.tsx#L405-L418

so it is not clear yet why the `title` is not present', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('Jskr8GupZJx9G89ACyfp5', 'fSzfZOsgffkU8XWuYie6g', 1715678316000.0, 'IDB has:

<img width="281" alt="image" src="https://github.com/rocicorp/mono/assets/45845/c4a63c62-dbb2-4652-9f5c-bfe5b3e8042f">

let me check what the server sent', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('O9ZjFWoRSH1pueiTmuzQO', 'fSzfZOsgffkU8XWuYie6g', 1715699172000.0, 'We''ll need the operation to filter out partial rows from a query. This was started here: https://github.com/rocicorp/mono/tree/mlaw/filter-partial

but paused since we thought Zeppliear never diverged in what queries asked for so it wasn''t a top priority for the hackfest.

It also requires schema information on the client to support `*`', 'tantaman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('oE6mPmmTopC7LYDA4c0GG', 'EhVaY0SmM1SFxLt43Xq6l', 1715675572000.0, '<details>
<summary><a href="https://linear.app/roci/issue/ROC-11/limit-is-broken">ROC-11 limit is broken</a></summary>
<p>

I was hitting this when testing Zeppliear

In Zeppliear we have a limit of 200 but we end up with a case where we get to `#limitedAddAll` where the size of the BTree is 201 (changing the limit to 10 hits a case where the data.size is 11):

[https://github.com/rocicorp/mono/blob/baff399d78101ac77813803516b06a0c84fa4209/packages/zql/src/zql/ivm/view/tree-view.ts#L142-L144](https://github.com/rocicorp/mono/blob/baff399d78101ac77813803516b06a0c84fa4209/packages/zql/src/zql/ivm/view/tree-view.ts#L142-L144)

I did some debugging and the problem seems to be that the we call tree `set` without going through the *limit function* so the tree size is larger than we expect.

[https://github.com/rocicorp/mono/blob/baff399d78101ac77813803516b06a0c84fa4209/packages/zql/src/zql/ivm/source/set-source.ts#L173-L175](https://github.com/rocicorp/mono/blob/baff399d78101ac77813803516b06a0c84fa4209/packages/zql/src/zql/ivm/source/set-source.ts#L173-L175)
</p>
</details>', 'linear[bot]');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('OYOaqck_i2PTBLvxRgKun', 'EhVaY0SmM1SFxLt43Xq6l', 1715675726000.0, 'Also, swapping the order of set and delete here seems to fix it

https://github.com/rocicorp/mono/blob/a221bec5b5e6dc9e675c1ad743c39dd7113f9bdd/packages/zql/src/zql/ivm/view/tree-view.ts#L151-L154

Which makes no sense to me! Are we mutating a shared tree somewhere?', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('CULmRpNdKlEBL9WbTLVNG', 'EhVaY0SmM1SFxLt43Xq6l', 1715851935000.0, '#1804 ?', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('E6nBq3QoKlsWOO6VWPj7W', 'EhVaY0SmM1SFxLt43Xq6l', 1715855576000.0, 'Do you know of a reliable repro? As you linked, I couldn''t repro with those tests. I also changed the b-tree recently to use the immutable variants of add/remove/delete (#1825) which would preclude any of those mutation issues.', 'tantaman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('MXEdDektldZpa2VpvfUf0', 'EhVaY0SmM1SFxLt43Xq6l', 1715856096000.0, '> I did some debugging and the problem seems to be that the we call tree set without going through the limit function so the tree size is larger than we expect.

The `source` tree and `view` tree are two distinct trees.', 'tantaman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('vfpVpIdVbVcPJ7cCGDocO', 'EhVaY0SmM1SFxLt43Xq6l', 1715856425000.0, 'I just synced main, npm i, npm run build and I still get the same error on loading zeppliear with a new "room"', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('LO8YjUQxwXUcgrLoeaoO_', 'EhVaY0SmM1SFxLt43Xq6l', 1715867117000.0, 'taking a look', 'tantaman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('7jQh5kAb4z1k1XN6UbKho', 'EhVaY0SmM1SFxLt43Xq6l', 1716564431000.0, 'This ended up being fixed by #1867, correct?', 'tantaman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('Gc-56qwca54Z7tNZ-LSuJ', 'EhVaY0SmM1SFxLt43Xq6l', 1716811485000.0, 'limit is broken but in a different way #1866

I''ll close this and open a new issue.

Subsumed by #1942', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('E_12Rkuivu-oVep6Ylf92', 'KAJMDvkPkNX7ssqR6ijhZ', 1715671154000.0, '<p><a href="https://linear.app/roci/issue/ROC-10/type-generation-for-client-api">ROC-10 Type Generation for Client API</a></p>', 'linear[bot]');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('42Dz7FApAylsfZl-RLn6s', 'l4d3GWQ4feLSHJ3MHEoM2', 1715671049000.0, '<details>
<summary><a href="https://linear.app/roci/issue/ROC-9/auth">ROC-9 Auth</a></summary>
<p>

Right now we have a few paragraphs of text and a code block. We need to design and implement both authentication and authorization.
</p>
</details>', 'linear[bot]');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('iqkD4cdM0DFRzXmJGUaRi', 'TEV8ldKCbizgc7IAJurcD', 1715670373000.0, '<details>
<summary><a href="https://linear.app/roci/issue/ROC-8/test">ROC-8 test</a></summary>
<p>

test
</p>
</details>', 'linear[bot]');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('gdoilb_Ysb-Z5wtdOaddw', 'TEV8ldKCbizgc7IAJurcD', 1715670396000.0, 'test?', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('Y5rdAEig9bwlzCLkBNikZ', '2oVDh-xJvsIX5UFda_Qa2', 1716564465000.0, 'fixed by https://github.com/rocicorp/mono/pull/1839', 'tantaman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('vemlIiHhapbuJdfM85MEQ', '69hisURAjfAph8bLIhLTl', 1715013757000.0, 'I did this originally. My thought process was that an app level ping would handle more failure modes. For example it could detect deadlocks in the locking code of the do. The cf provided ping support couldnâ€™t do that.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('eC0fjTwF5kqKa6nrH3EAv', '69hisURAjfAph8bLIhLTl', 1715066130000.0, 'Let''s leave as is. It works.', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('g44TiUPWRERdUoNRz9aLl', 'cXM1rl-5nWvpAmqR2oTU5', 1714602151000.0, 'cc @tantaman ', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('iUGIMjNjm6f_1yX628Tdg', 'cXM1rl-5nWvpAmqR2oTU5', 1715192809000.0, 'Started on this here: https://github.com/rocicorp/mono/tree/mlaw/filter-partial

but it stalled out since we need schema information on the client to deal with `*`. Once we have that I can resume this work.', 'tantaman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('OezLNPssv3MMJa8GogiZy', 'cXM1rl-5nWvpAmqR2oTU5', 1718868284000.0, 'We should only cache the entire row.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('2R5El1f42MpQHiGNenLEv', 'qFXKhttg4GG6UDqNs2Enf', 1715649260000.0, '- #1803
- #1817 

Reduce still needs to be lazy on input.

Join is lazy in coming commits.', 'tantaman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('BjQMAcGQmdzhjRDS-KBVo', '6AfgoH5ICMJhe8g7Q93Nc', 1714484810000.0, 'Another option is to just go ahead an implement sharing of structure. In that case, joins will only run once.

We''ll want to be a bit smart when cleaning up graph nodes after hitting 0 references and keep them around a bit in case a new query immediately shows up wanting a recently de-referenced node.', 'tantaman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('DMdwtS4cGzD2-9zDx8Rbh', 'zA09ylXAJ5afxFeiwwXWF', 1714155513000.0, 'For (1) --

The current signature is:

```ts
EntityQuery<F extends FromSet, Return = []>
```

which, in practice, looks like the following when defining functions that take queries:

```ts
function applyFilter(q: EntityQuery<{
  issue: Issue,
  label: Label
}, {
  issue: Issue,
  label: Label
}[]>) {
}
```

That''s... difficult to get right.

A potential fix is to modify the `EntityQuery` type to:

```ts
EntityQuery<F extends union of entities?, Return = MakeReturn<F>>
```

Which cleans up user defined functions (like the applyFilter example) to:

```ts
function applyFilter(q: EntityQuery<Issue | Label>) {
}
```

> note: `EntityQuery<Issue | Label>` instead of `EntityQuery<[Issue, Label]>` since order should not matter.

Which, I think, is pretty obvious. 

Does this fix the issue with `EntityQuery<{foo: Foo, bar: Bar}>` being assignable to `EntityQuery<{foo: Foo}>`?
We do not want to former to be assignable to the latter unless we force the user to always use qualified names in `where`, `on`, `having`. The reason is that the type system will allow unqualified selectors for the latter but the implementation will break if the type of query is really the former at runtime.', 'tantaman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('Y5umYohiHtSka2ybXMZRa', 'zA09ylXAJ5afxFeiwwXWF', 1714155630000.0, 'For (2) --

I like the idea of leaning into sub-queries and making join as irrelevant as possible in the language. This, combined with co-located queries, should fix the problem.

Co-located queries helps to fix the problem since the return type of a query will not spread out into many components.', 'tantaman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('br1wyj5t1BqoF-Ui3UxoA', 'zA09ylXAJ5afxFeiwwXWF', 1714155822000.0, 'For (3) --

One option would be to default the return type of a query to the empty object rather than defaulting it to `SELECT *`.

I _think_ this would allow selects which add fields to a query to be assigned to a prior query variable.', 'tantaman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('UyYHj0bvXPndq4jfqGRf8', 'QkIQSSr1UMxdQ1XvCXRLL', 1714064314000.0, 'Looks like I forgot to deal with operators that have memory when it comes to processing historical data.

History requests should stop as soon as they hit an operator with memory. Although no queries are sharing structure right now so I''m a bit confused as to why we''d process history more than once through a pipeline.', 'tantaman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('6iR-QYzSDfy1Lu03hl2M_', 'QkIQSSr1UMxdQ1XvCXRLL', 1714065768000.0, 'ah, the source is always shared among all queries.

When removing the `queue` abstraction I removed/screwed up the code that selected the correct downstream path.', 'tantaman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('nlQfSMC1vA7UkiULE-3El', 'QkIQSSr1UMxdQ1XvCXRLL', 1714066591000.0, 'need to clean it up but the fix is here: 6e192626b168ce4197cadf0496b75ee51a1047d6 in this draft pr: https://github.com/rocicorp/mono/pull/1640 ', 'tantaman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('GIRdnwQQO5ZLIILE8nWy0', 'QkIQSSr1UMxdQ1XvCXRLL', 1714485062000.0, 'The existing count issue is fixed. Zeppliear has an unrelated count issue where we''re just doing the count query incorrectly.

I.e.,
```
SELECT count(*) FROM issue JOIN ... GROUP BY issue.id 
```

That counts the count in a group, not the total count of rows.

Should be:

```
SELECT count(distinct issue.id) FROM issue JOIN ... ;
```', 'tantaman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('Q7Ko0URRgJeGWGLZkUlo5', 'QkIQSSr1UMxdQ1XvCXRLL', 1714512255000.0, '- #1684 adds distinct', 'tantaman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('5xpGhfzqUA_IMF_mIKA5D', 'Hevl4rWrSKe8t4P99CDZn', 1713189710000.0, '1. All events still exists in `pending` so the delete will be sent downstream
2. The `set-source` is assuming everything has a unique id so is not allowing dupes. I.e., `this.#tree.add` replaces the old value with the new one', 'tantaman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('Z--ITC_3XoGPhuL3nmil0', 'fXWBarBsdlyZPdGy6jneN', 1710489883000.0, 'My bias is to keep it as is unless the support for old ff is really getting
in the way badly or else we are certain ~nobodyâ€™s using this old version of
FF anymore.

Keep in mind that when we make browser support choices weâ€™re making them on
behalf of our *customers* not ourselves. This is now a market our customer
canâ€™t target.

An advantage of Replicache is that we use pretty basic web platform APIs so
we are very very compatible.

SQLite based systems have much higher requirements. Letâ€™s not throw away
that advantage carelessly.

a (phone)


On Thu, Mar 14, 2024 at 9:48â€¯PM Erik Arvidsson ***@***.***>
wrote:

> https://www.mozilla.org/en-US/firefox/115.0/releasenotes/
>
> IndexedDB <https://w3c.github.io/IndexedDB/> is now also supported in private
> browsing <https://bugzilla.mozilla.org/show_bug.cgi?id=1639542> without
> memory limits thanks to encrypted storage on disk. The temporary keys to
> decrypt the information are held in RAM only and all stored information is
> purged at the normal end of a private browsing session from disk.
>
> They way Replicache deals with this is that it catches an exception and
> switches to an in memory store. With Firefox 115 this exception is no
> longer triggered. This means that we already use IDB in Firefox private
> browsing but there is room to simplify the code to remove this fallback.
>
> â€”
> Reply to this email directly, view it on GitHub
> <https://github.com/rocicorp/mono/issues/1476>, or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AAATUBCM4COXO74NF5H73C3YYKRTFAVCNFSM6AAAAABEXQL3MSVHI2DSMVQWIX3LMV43ASLTON2WKOZSGE4DOOJRGUYTGNY>
> .
> You are receiving this because you are subscribed to this thread.Message
> ID: ***@***.***>
>
', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('t5kyOw6gZ7BJ-F3eRf_wH', '5KQHlQsv-6j1BOXjh80ft', 1708950745000.0, 'It is not clear if `authHandler` to `onAuth` makes sense because `onAuth` implies that it gets called when something is authenticated but this thing is called to do the actual authentication.

Deferring that follow up until further discussions.', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('N9ytXNl3ZdkBMD30f0Cgf', 'PlfYFT5Cf3igeiSPgvlQ9', 1708746427000.0, 'FYI, happened again for a different user. I''ll fix the CLI to not report this as an error.

<img width="1011" alt="Screenshot 2024-02-23 at 19 46 30" src="https://github.com/rocicorp/mono/assets/132324914/434dacda-bdeb-4738-a612-e198e39dfa9e">
', 'darkgnotic');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('wTy5QDGkD1GxAUp3F9cXj', 'PlfYFT5Cf3igeiSPgvlQ9', 1708746800000.0, 'Another option is to create the team if the user calls one of these functions (which would normally not happen until they publish their first app).

This might make the most sense from a dx perspective. Then `apps list` and `keys` would work.', 'darkgnotic');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('38TFqaX6IvwKtTpT6yYpD', 'PlfYFT5Cf3igeiSPgvlQ9', 1709670117000.0, '> Another option is to create the team if the user calls one of these functions

This makes sense to me. In the future if users can be invited to teams then such users will often end up with two teams, their personal one and their work one. But this is standard with similar tools.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('qCWIzVpk59TjKVjtKn2A3', 'qo-ny8b0TQ4Q8HMzngZ5E', 1708655783000.0, 'Yeah, weird. Looking.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('jqL01PiAZ_TC7zgh5bP6z', 'qo-ny8b0TQ4Q8HMzngZ5E', 1708656653000.0, 'I believe this is a length issue.  loop-orchestrator-release-0-39-2024022-rocicorpreflectservices.reflect-server.net is accepted but loop-orchestrator-release-0-39-20240222-rocicorpreflectservices.reflect-server.net is not.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('caFSraa5WBXx9FZoEFXyZ', 'qo-ny8b0TQ4Q8HMzngZ5E', 1708668542000.0, 'Nice. I''ll add an explicit check beforehand so we don''t get these orphaned custom hostnames (since that succeeds, but the subsequent dns step fails).', 'darkgnotic');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('WeNiEgs_1hrkyOZvTgKj2', 'qo-ny8b0TQ4Q8HMzngZ5E', 1708670195000.0, 'Needs more experimentation to know if itâ€™s the total host name length it doesnâ€™t like or subdomain or what ', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('4oJAuCUTopKis7QV4anCA', 'qo-ny8b0TQ4Q8HMzngZ5E', 1708714681000.0, 'Yup.

I was unable to create loop-orchestrator-release-0-39-202402220-rocicorpreflectservices.reflect-server.dev via the dashboard, but I am able to create loop-orchestrator-release-0-39-20240222-rocicorpreflectservices.reflect-server.dev.

<img width="1268" alt="Screenshot 2024-02-23 at 10 47 38" src="https://github.com/rocicorp/mono/assets/132324914/d1d55fc1-0f02-444b-93fe-45524c572892">

It looks like loop-orchestrator-release-0-39-20240222-rocicorpreflectservices.reflect-server.net already exists, which may be why you couldn''t create it.

Next test is to see if it''s the hostname length or the full dns name length, so I tried it on replicache.dev.

 loop-orchestrator-release-0-39-202402220-rocicorpreflectservices.replicache.dev fails but  loop-orchestrator-release-0-39-20240222-rocicorpreflectservices.replicache.dev succeeds. So it''s hostname specific.

To be complete, I removed all numbers and hyphens and just tried a straight alphabetic hostname.

The 64 character hostname rocicorpreflectservicesrocicorpreflectservicesrocicorpreflectser.replicache.dev fails, but the 63 character hostname rocicorpreflectservicesrocicorpreflectservicesrocicorpreflectse.replicache.dev succeeds.

So the max hostname length is 63.
', 'darkgnotic');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('x_-50l49OkGA_uoGUznfM', 'qo-ny8b0TQ4Q8HMzngZ5E', 1708718168000.0, 'Confirmed: https://developers.cloudflare.com/dns/manage-dns-records/reference/dns-record-types/#cname

> CNAME
>
> Name: A subdomain or the zone apex (@), which must: 
> * Be 63 characters or less
> * Start with a letter and end with a letter or digit
> * Only contain letters, digits, or hyphens (underscores are allowed but discouraged)

Also, according to https://community.cloudflare.com/t/dns-record-for-cname-is-limited/491688, the total dns name must be 255 characters or less:

<img width="771" alt="Screenshot 2024-02-23 at 11 54 37" src="https://github.com/rocicorp/mono/assets/132324914/1fd4925e-a4ef-4145-82e1-6cd2331f5af8">


At the moment, we don''t have to worry about the max dns name limit of 255 characters, but it''s good to keep in mind if/when we start supporting more variants of domain names.
', 'darkgnotic');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('NuOFFTrThAaQd_nChf9jH', 'qo-ny8b0TQ4Q8HMzngZ5E', 1708719293000.0, 'So does this mean we should limit the app name to 63 chars too?', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('BqOO-NFn67qyAyFFV1vEj', 'qo-ny8b0TQ4Q8HMzngZ5E', 1708719429000.0, 'Yeah, we actually need to limit `{appName}-{teamLabel}` to 63 characters.', 'darkgnotic');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('YJg0BMkdydUz73FCpXnyb', 'qo-ny8b0TQ4Q8HMzngZ5E', 1708719626000.0, 'ooooh. Two follow-up thoughts:

1. Should we put a limit on team label too?
2. And/or, should we implicitly clamp and then add a hash to user supplied appNames that would exceed the limit?', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('_QzNiO9ixBB3C5HkGc0bc', 'qo-ny8b0TQ4Q8HMzngZ5E', 1708728641000.0, 'A fix for (2) is in. The app name will be truncated and hashed if the dns label would otherwise be too long.

But yes, we would also need to limit the lengths of team names for this to be water tight. Does github limit usernames already? If not, or if it''s close to 63 chars, do you want to do a similar dns-only truncate+hash thing for the team name? 

I guess we''d also need to decide how to deal with the case when they''re both too long (i.e. decide how many characters each name gets). ', 'darkgnotic');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('E2tNVmx7Qx64yd-Wil-NW', 'qo-ny8b0TQ4Q8HMzngZ5E', 1708744985000.0, 'Various places indicate that Github restricts its username length to 39:

https://gist.github.com/tonybruess/9405134
https://github.com/shinnn/github-username-regex
https://docs.github.com/en/enterprise-cloud@latest/admin/identity-and-access-management/iam-configuration-reference/username-considerations-for-external-authentication

So I will set the limiit to 40 when creating a team label for the future point at which we choosing or renaming team names.', 'darkgnotic');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('YmiNFs7_oX6rW_Qyejtg-', 'F2FaYE6GlVnOvb18-4_S3', 1707851314000.0, 'I was just coming here to file this bug as Tristan raised it on Discord https://discord.com/channels/830183651022471199/1206779893229031494/1206812591997976596
 
 :)
 
 I''ll respond with how I think we should fix.', 'grgbkr');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('6is4dIQ5-hTmUiXx3cEHu', 'F2FaYE6GlVnOvb18-4_S3', 1707853212000.0, 'To fix:

1.  [RoomDO''s delete handler](https://github.com/rocicorp/mono/blob/2b950d45e343e84ae0dfadd47e2f9246483c07f9/packages/reflect-server/src/server/room-do.ts#L169) should close all connections.
2.  [deleteRoom handler](https://github.com/rocicorp/mono/blob/2b950d45e343e84ae0dfadd47e2f9246483c07f9/packages/reflect-server/src/server/auth-do.ts#L263) should [delete all ConnectionRecords](https://github.com/rocicorp/mono/blob/2b950d45e343e84ae0dfadd47e2f9246483c07f9/packages/reflect-server/src/server/auth-do.ts#L1253) for the deleted room.  Note that its possible that a room-do processes a delete request but the auth-do fails to update the RoomRecord and delete the ConnectionRecords (the new logic), this is because the state is spread across two dos and thus is non-transactional.  This complicates the handling in 2 and 3 below.
3. [authRevalidateConnections](https://github.com/rocicorp/mono/blob/2b950d45e343e84ae0dfadd47e2f9246483c07f9/packages/reflect-server/src/server/auth-do.ts#L895) should correctly handle deleted rooms.  For each roomID, before sending it the request, it should check if it''s RoomRecord indicates it is deleted, if it is it should delete the connection records and move to the next roomID.  If not proceed to send the request for the current connection.  If the returned responses is 410 deleted, this indicates the room was deleted but we failed to record it in the auth do''s state, so we should fix that now, we should update the RoomRecord to indicate it is deleted and we should delete the connection records for the room.
4. The auth invalidate endpoints (authInvalidateAll, authInvalidateForRoom and authInvalidateForUser) need to be updated to deal with deleted rooms.  Similarly to 2, before sending a request to a room we should check the RoomRecord for deleted, and we should also handle 410 deleted responses from the room, updating the RoomRecords and ConnectionRecords appropriately.  *We should treat a deleted room as a successful invalidation.* 

Probably the handling in 2 and 3 can be largely shared code.', 'grgbkr');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('kBXBYywHOS8HLvc6rOY9k', 'F2FaYE6GlVnOvb18-4_S3', 1707853868000.0, 'This reminds me of a related idea that I had when I encountered this non-transactional (two storage system) interaction. The idea was something like:

1. deleteRoom sets that status of the RoomRecord (in the AuthDO) to a new state called `DELETING`. For most intents and purposes, the AuthDO treats it the same as `DELETED`.
2. An Alarm is scheduled to make the call to the RoomDO to delete itself (along with any other rooms that are in the `DELETING` state). If it succeeds, the AuthDO then marks the RoomRecord as `DELETED`. If it fails, the alarm tries again later.

Would this simplify things in terms of revalidate / invalidate? ', 'darkgnotic');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('acphQWpXCI1OsNGNcjJXu', 'F2FaYE6GlVnOvb18-4_S3', 1707855047000.0, 'BTW reading some comments in the code I think the original intention was that callers that wanted to delete a room would have to:
1. call close with roomID 
2. call invalidateForRoom with roomID
3. call delete with roomID

This seems overly burdensome on the caller.  Also if you fail to invalidateForRoom before you delete you can end up with dangling connections that are just getting errors cause all of the room''s state has been deleted.

We do strictly require that you [close a room before deleting it](https://github.com/rocicorp/mono/blob/2b950d45e343e84ae0dfadd47e2f9246483c07f9/packages/reflect-server/src/server/rooms.ts#L213).  I''m also not sure about this.  Why require closing before deleting?   A comment indicates that close is intentionally coded to not log out existing connections, but just to not allow new ones.... why?  ', 'grgbkr');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('k4au5xyNfM_JE0bkskWA2', 'F2FaYE6GlVnOvb18-4_S3', 1707856499000.0, '> This reminds me of a related idea that I had when I encountered this non-transactional (two storage system) interaction. The idea was something like:
> 
> 1. deleteRoom sets that status of the RoomRecord (in the AuthDO) to a new state called `DELETING`. For most intents and purposes, the AuthDO treats it the same as `DELETED`.
> 2. An Alarm is scheduled to make the call to the RoomDO to delete itself (along with any other rooms that are in the `DELETING` state). If it succeeds, the AuthDO then marks the RoomRecord as `DELETED`. If it fails, the alarm tries again later.
> 
> Would this simplify things in terms of revalidate / invalidate?

That is a nice way to ensure eventual consistency. 2 would retry if either the call to the RoomDO failed or the AuthDO RoomRecord/ConnectionRecords updates failed, correct?

I think it would allow a little simplification of revalidate/invalidate if we had been using this scheme from the beginning, but given having to deal with existing rooms I think its probably more complicated to move to this.  With this revalidate could just skip over DELETING and DELETED rooms.  However invalidate would still need to make calls to DELETING rooms (as its contract is that if the response is 200 the connections are closed, not that they will be eventually close), and would need to deal with `410 deleted` responses.

', 'grgbkr');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('11jj3TeKxowtkKGuEg5yd', 'F2FaYE6GlVnOvb18-4_S3', 1707872116000.0, 'Taking this off of @grgbkr ''s plate.', 'darkgnotic');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('QRj1Zzn5HhTrOX92oly28', 'F2FaYE6GlVnOvb18-4_S3', 1708032856000.0, 'As I''m working through the code, one thing I''ve noticed is that none of the invalidate methods actually clear connection state. Revalidate is the only place that does so. I assume that this is because the AuthDO doesn''t know about regular (non-invalidation) disconnects, and so it has to periodically revalidate, and thus it will eventually find out about the invalidated connections through this process.

Is there a harm in leaving it this way and only removing connections in revalidate (with the additional logic for understanding deleted rooms)? I think this would simplify things slightly by consolidating all of the bookkeeping in the revalidate step. ', 'darkgnotic');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('h5dE-Zn1UHZqROIdDqBUT', 'F2FaYE6GlVnOvb18-4_S3', 1708037065000.0, '> As I''m working through the code, one thing I''ve noticed is that none of the invalidate methods actually clear connection state. Revalidate is the only place that does so. I assume that this is because the AuthDO doesn''t know about regular (non-invalidation) disconnects, and so it has to periodically revalidate, and thus it will eventually find out about the invalidated connections through this process.
> 
> Is there a harm in leaving it this way and only removing connections in revalidate (with the additional logic for understanding deleted rooms)? I think this would simplify things slightly by consolidating all of the bookkeeping in the revalidate step.

I convinced myself that this is a correct (and elegant) way to handle this. The connection cleanup code remains largely the same, the only difference being that it handles a deleted room as having returned no connections (so that they get cleaned up), and checks that the RoomRecord is marked as deleted.', 'darkgnotic');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('OmQ-IB05tAIqCw-n_YdE_', 'F2FaYE6GlVnOvb18-4_S3', 1708109132000.0, '> BTW reading some comments in the code I think the original intention was that callers that wanted to delete a room would have to:
> 
> 1. call close with roomID
> 2. call invalidateForRoom with roomID
> 3. call delete with roomID
> 
> This seems overly burdensome on the caller. Also if you fail to invalidateForRoom before you delete you can end up with dangling connections that are just getting errors cause all of the room''s state has been deleted.
> 
> We do strictly require that you [close a room before deleting it](https://github.com/rocicorp/mono/blob/2b950d45e343e84ae0dfadd47e2f9246483c07f9/packages/reflect-server/src/server/rooms.ts#L213). I''m also not sure about this. Why require closing before deleting? A comment indicates that close is intentionally coded to not log out existing connections, but just to not allow new ones.... why?

After thinking about this a bit, I can see use cases for:
* Closing a room and leaving the existing connections open. (maybe?)
* Closing a room and then invalidating connections but keeping the room data for archival purposes.

However, I feel like `deleteRoom` could automatically close the room (in the AuthDO) similar to how we added the auto invalidate, so that the `deleteRoom` command can stand on its own without any of the previous steps.

I guess one gotcha is that in the case of a partial failure (e.g. the call to RoomDO#delete fails), the user is left with a closed room, which could be kind of unintuitive.

I dunno. Food for thought.', 'darkgnotic');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('Wq0fBZD6pIqvdw-nWbCi-', '1fsM2YJIam46LHJSXHhqN', 1707802349000.0, 'Sorry, mischaracterized the error. Will file a new Issue.', 'darkgnotic');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('AAQmjcgp0kHamPOXEAEWz', 'E8dSj9b47wGSDgzsu0DMI', 1707804512000.0, 'Actually, the request is technically incorrect because the user id is supposed to be uri encoded, so the '':'' character should be encoded as a ''%3A''.

It looks like Tristan eventually resolved the issue by changing the colon to an underscore in the user id.

<img width="1518" alt="Screenshot 2024-02-12 at 22 07 06" src="https://github.com/rocicorp/mono/assets/132324914/723031a8-77bc-4950-b25b-a0dce6149dc0">
', 'darkgnotic');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('kZQf0IcuYohJ49XUBgBTE', 'E8dSj9b47wGSDgzsu0DMI', 1707805557000.0, 'If you look at the discord thread, he was sending it URL encoded:

![CleanShot 2024-02-12 at 20 25 15@2x](https://github.com/rocicorp/mono/assets/80388/3d35d892-e447-4271-9c68-da258dfee032)

-- https://discord.com/channels/830183651022471199/1020392595450507304/1206779893229031494

Could some infra somewhere be decoding it before it gets to our code?

', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('PgTaFy69RVbs0_OcCvnw6', 'E8dSj9b47wGSDgzsu0DMI', 1707874353000.0, 'Yeah, I just confirmed that it happens to me too.

<img width="1364" alt="Screenshot 2024-02-13 at 17 30 41" src="https://github.com/rocicorp/mono/assets/132324914/cba4a63b-8c64-4d6d-ad02-df6e333a607b">
<img width="1230" alt="Screenshot 2024-02-13 at 17 30 19" src="https://github.com/rocicorp/mono/assets/132324914/526696d6-de0c-4928-9c1a-4fa4b9016304">


I think it''s GCP (which is based on Express) that''s uri decoding the "%3A". Sort of defeats the whole purpose.
I haven''t been successful in figuring out whether that can be turned off.', 'darkgnotic');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('SPNkwvU0Ge5SbP2y_mu0y', 'E8dSj9b47wGSDgzsu0DMI', 1707877114000.0, 'I found other folks who have encountered this problem (in Express, or IIS, which uses Express) but have yet to find a solution:

https://github.com/expressjs/express/issues/4825
https://github.com/expressjs/express/issues/1479

https://github.com/tjanczuk/iisnode/issues/217
https://github.com/tjanczuk/iisnode/issues/343
', 'darkgnotic');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('ZucBFeT7emXgwOlPf4R6y', 'E8dSj9b47wGSDgzsu0DMI', 1707877706000.0, 'FTR, I ran a bunch of URL encoded characters through to see which ones do and do not get decoded.

Sent:

```
connections/users/-._~%3A%2F%3F%23%5B%5D%4024%26''()*%2B%2C%3B%25%3D:invalidate
```

Received:

```
connections/users/-._~:/%3F%23%5B%5D@24&''()*+,;%25=:invalidate
```

This indicates which characters we''d have to re-encode to reverse this behavior for upstream (reflect-server) receivers.', 'darkgnotic');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('0jxurn0tyIMta5aaCejON', 'E8dSj9b47wGSDgzsu0DMI', 1707878141000.0, 'Also FTR, I checked the request headers to see if it provides the original (sent) url. It contains a partially decoded one, which is unfortunately equally useless.

```
x-forwarded-url: "/v1/apps/ln3/ddtrj/connections/users/-._~:/%3F%23%5B%5D@24&%27%28%29%2A+,;%25=:invalidate"
```', 'darkgnotic');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('zkU62Zd-PftXJCoSa-r_g', 'E8dSj9b47wGSDgzsu0DMI', 1707931953000.0, '@grgbkr and I consulted and agreed on the path forward being to move ids into query parameters. Working on this.', 'darkgnotic');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('wSHpemECDdQMqs2VQ1znS', 'NJstzvXt-8vhzO9ZirTXR', 1706916109000.0, '@cesara this is due to your change in 

https://github.com/rocicorp/mono/commit/b5ee7e383ae1f2035ec7217375361f6ab2b9c541

<img width="532" alt="Screenshot 2024-02-02 at 15 21 04" src="https://github.com/rocicorp/mono/assets/132324914/2ad0088c-7201-4627-841c-931e403c8695">

Do you recall what the motivation was? 

(These errors are now triggering alerts)', 'darkgnotic');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('o6dkdEEi81on1lX_iyUcl', 'NJstzvXt-8vhzO9ZirTXR', 1706916806000.0, '@d-llama ya this was a mistake to check-in. I needed to throw the error because the exit(1) was hiding an error on a failing test', 'cesara');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('4L19P0tlZLaZhmbDqlsKp', 'R9O4DN_T0uY-NcGNVRzah', 1707856154000.0, 'Confirmed that Tanushree bumped our quota up to 10K.

<img width="725" alt="Screenshot 2024-02-13 at 12 27 57" src="https://github.com/rocicorp/mono/assets/132324914/6f77d485-dbbc-407e-8950-8e2e53b5783d">

@aboodman are we happy with this or is there more to follow up on with CF?', 'darkgnotic');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('RJ2Dza-dATLFLiE0k1SFn', 'R9O4DN_T0uY-NcGNVRzah', 1707857509000.0, '<img width="750" alt="CleanShot 2024-02-13 at 10 51 26@2x" src="https://github.com/rocicorp/mono/assets/80388/08ff83f0-0032-4dfe-ad2d-45abfc8e617f">

Booya', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('nIvBg0m8o6mR_Nfbf794T', 'tCNDwi5uFtMpq0wNi4snY', 1706622451000.0, 'Here is what I think is happening?

reflect.net does not use auth and the handler currently requires auth. We need to make the http handler have an optional authentication header instead.

https://github.com/rocicorp/mono/blob/main/packages/reflect-server/src/server/auth-do.ts#L317', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('Rj5L2wvEyD26bDImjhDIA', 'tCNDwi5uFtMpq0wNi4snY', 1707395741000.0, 'Fixed', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('MLkV5fJKfmWIXk7iAuTAa', 'WAwQvGFjwY_dvk4clQ_vN', 1702638011000.0, 'https://github.com/rocicorp/mono/blob/main/packages/reflect-server/src/mod.ts#L16-L20

We should not export `ReflectServerBaseEnv` nor `createReflectServer`.', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('qer9hx5GXwJvF2LhEcsyR', '7baXcO5BCdpFSyo3hY4Wn', 1701396730000.0, 'FTR, the code documentation clarifies this:

```js
(property) GlobalOptions.concurrency?: number | ResetValue | Expression<number>

Number of requests a function can serve at once.

@remarks
Can only be applied to functions running on Cloud Functions v2. 
A value of null restores the default concurrency (80 when CPU >= 1, 1 otherwise).
 Concurrency cannot be set to any value other than 1 if cpu is less than 1. 
The maximum value for concurrency is 1,000.
```', 'darkgnotic');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('wGKk3zfrGtVsBkV1QNgWT', 'knakVggHchm5GL-iH2bDn', 1701202071000.0, 'For posterity, I manually ran the backup that failed:

```bash
mirror-cli $ npm run mirror backup-analytics ConnectionLifetimes

Running on reflect-mirror-prod

{"severity":"INFO","message":"Start date: 2023-11-19T00:00:00.000Z"}
QUERY: SELECT * FROM ConnectionLifetimes WHERE timestamp >= toDateTime(1700352000) AND timestamp < toDateTime(1700438400) ORDER BY timestamp FORMAT JSONEachRow
{"_sample_interval":1,"blob1":"1327adzgrUI","blob10":"","blob11":"","blob12":"","blob13":"","blob14":"","blob15":"","blob16":"","blob17":"","blob18":"","blob19":"","blob2":"loluf17a","blob20":"","blob3":"34c7b5fd-e508-4cff-839c-0252dd1aae55","blob4":"","blob5":"","blob6":"","blob7":"","blob8":"","blob9":"","dataset":"ConnectionLifetimes","double1":1700357802510,"double10":0,"double11":0,"double12":0,"double13":0,"double14":0,"double15":0,"double16":0,"double17":0,"double18":0,"double19":0,"double2":1700357838292,"double20":0,"double3":0,"double4":0,"double5":0,"double6":0,"double7":0,"double8":0,"double9":0,"index1":"","timestamp":"2023-11-19 01:37:18","severity":"INFO","message":"Validated first row"}
{"severity":"INFO","message":"Num results: 47"}
QUERY: SELECT * FROM ConnectionLifetimes WHERE timestamp >= toDateTime(1700438400) AND timestamp < toDateTime(1700524800) ORDER BY timestamp FORMAT JSONEachRow
{"_sample_interval":1,"blob1":"7b4eqY3OWih","blob10":"","blob11":"","blob12":"","blob13":"","blob14":"","blob15":"","blob16":"","blob17":"","blob18":"","blob19":"","blob2":"loxv9i52","blob20":"","blob3":"orch_public_d","blob4":"","blob5":"","blob6":"","blob7":"","blob8":"","blob9":"","dataset":"ConnectionLifetimes","double1":1700444186329,"double10":0,"double11":0,"double12":0,"double13":0,"double14":0,"double15":0,"double16":0,"double17":0,"double18":0,"double19":0,"double2":1700444212739,"double20":0,"double3":0,"double4":0,"double5":0,"double6":0,"double7":0,"double8":0,"double9":0,"index1":"","timestamp":"2023-11-20 01:36:52","severity":"INFO","message":"Validated first row"}
{"severity":"INFO","message":"Num results: 557"}
QUERY: SELECT * FROM ConnectionLifetimes WHERE timestamp >= toDateTime(1700524800) AND timestamp < toDateTime(1700611200) ORDER BY timestamp FORMAT JSONEachRow
{"_sample_interval":1,"blob1":"7b4eqY3OWih","blob10":"","blob11":"","blob12":"","blob13":"","blob14":"","blob15":"","blob16":"","blob17":"","blob18":"","blob19":"","blob2":"loxv9i52","blob20":"","blob3":"orch_public_e","blob4":"","blob5":"","blob6":"","blob7":"","blob8":"","blob9":"","dataset":"ConnectionLifetimes","double1":1700529467258,"double10":0,"double11":0,"double12":0,"double13":0,"double14":0,"double15":0,"double16":0,"double17":0,"double18":0,"double19":0,"double2":1700529874342,"double20":0,"double3":0,"double4":0,"double5":0,"double6":0,"double7":0,"double8":0,"double9":0,"index1":"","timestamp":"2023-11-21 01:24:34","severity":"INFO","message":"Validated first row"}
{"severity":"INFO","message":"Num results: 9337"}
QUERY: SELECT * FROM ConnectionLifetimes WHERE timestamp >= toDateTime(1700611200) AND timestamp < toDateTime(1700697600) ORDER BY timestamp FORMAT JSONEachRow
{"_sample_interval":1,"blob1":"OW06tVvTZG","blob10":"","blob11":"","blob12":"","blob13":"","blob14":"","blob15":"","blob16":"","blob17":"","blob18":"","blob19":"","blob2":"loztnkex","blob20":"","blob3":"/","blob4":"","blob5":"","blob6":"","blob7":"","blob8":"","blob9":"","dataset":"ConnectionLifetimes","double1":1700611152133,"double10":0,"double11":0,"double12":0,"double13":0,"double14":0,"double15":0,"double16":0,"double17":0,"double18":0,"double19":0,"double2":1700611221482,"double20":0,"double3":0,"double4":0,"double5":0,"double6":0,"double7":0,"double8":0,"double9":0,"index1":"","timestamp":"2023-11-22 00:00:21","severity":"INFO","message":"Validated first row"}
{"severity":"INFO","message":"Num results: 28377"}
QUERY: SELECT * FROM ConnectionLifetimes WHERE timestamp >= toDateTime(1700697600) AND timestamp < toDateTime(1700784000) ORDER BY timestamp FORMAT JSONEachRow
{"_sample_interval":1,"blob1":"OW06tVvTZG","blob10":"","blob11":"","blob12":"","blob13":"","blob14":"","blob15":"","blob16":"","blob17":"","blob18":"","blob19":"","blob2":"loztnkex","blob20":"","blob3":"/","blob4":"","blob5":"","blob6":"","blob7":"","blob8":"","blob9":"","dataset":"ConnectionLifetimes","double1":1700697598374,"double10":0,"double11":0,"double12":0,"double13":0,"double14":0,"double15":0,"double16":0,"double17":0,"double18":0,"double19":0,"double2":1700697617917,"double20":0,"double3":0,"double4":0,"double5":0,"double6":0,"double7":0,"double8":0,"double9":0,"index1":"","timestamp":"2023-11-23 00:00:17","severity":"INFO","message":"Validated first row"}
{"severity":"INFO","message":"Num results: 35038"}
QUERY: SELECT * FROM ConnectionLifetimes WHERE timestamp >= toDateTime(1700784000) AND timestamp < toDateTime(1700870400) ORDER BY timestamp FORMAT JSONEachRow
{"_sample_interval":1,"blob1":"OW06tVvTZG","blob10":"","blob11":"","blob12":"","blob13":"","blob14":"","blob15":"","blob16":"","blob17":"","blob18":"","blob19":"","blob2":"loztnkex","blob20":"","blob3":"/","blob4":"","blob5":"","blob6":"","blob7":"","blob8":"","blob9":"","dataset":"ConnectionLifetimes","double1":1700783993933,"double10":0,"double11":0,"double12":0,"double13":0,"double14":0,"double15":0,"double16":0,"double17":0,"double18":0,"double19":0,"double2":1700784000652,"double20":0,"double3":0,"double4":0,"double5":0,"double6":0,"double7":0,"double8":0,"double9":0,"index1":"","timestamp":"2023-11-24 00:00:00","severity":"INFO","message":"Validated first row"}
{"severity":"INFO","message":"Num results: 60902"}
QUERY: SELECT * FROM ConnectionLifetimes WHERE timestamp >= toDateTime(1700870400) AND timestamp < toDateTime(1700956800) ORDER BY timestamp FORMAT JSONEachRow
{"_sample_interval":1,"blob1":"OW06tVvTZG","blob10":"","blob11":"","blob12":"","blob13":"","blob14":"","blob15":"","blob16":"","blob17":"","blob18":"","blob19":"","blob2":"loztnkex","blob20":"","blob3":"/","blob4":"","blob5":"","blob6":"","blob7":"","blob8":"","blob9":"","dataset":"ConnectionLifetimes","double1":1700871580050,"double10":0,"double11":0,"double12":0,"double13":0,"double14":0,"double15":0,"double16":0,"double17":0,"double18":0,"double19":0,"double2":1700871652082,"double20":0,"double3":0,"double4":0,"double5":0,"double6":0,"double7":0,"double8":0,"double9":0,"index1":"","timestamp":"2023-11-25 00:20:52","severity":"INFO","message":"Validated first row"}
{"severity":"INFO","message":"Num results: 232"}
{"severity":"INFO","message":"Saving 134490 rows to 085f6d8eb08e5b23debfb08b21bda1eb/ConnectionLifetimes/2023-11-19~2023-11-26"}
 mirror-cli $ 
```

Interestingly, the resulting backup is quite a bit larger than that of previous weeks, at 1.7MB vs ~50kb:

![Screenshot 2023-11-28 at 12 03 07â€¯PM](https://github.com/rocicorp/mono/assets/132324914/8d94f37a-c294-4fcf-9afb-df113ae0f6fe)

There is also an increase in size for our other table, `RunningConnectionSeconds`, but not nearly as much:

![Screenshot 2023-11-28 at 12 04 42â€¯PM](https://github.com/rocicorp/mono/assets/132324914/a58bc436-c076-4f56-bae2-5bb92097cbf6)

Which is likely because the latter is bounded at once-per-minute entries, while `ConnectionLifetimes` produce an entry per connection and can thus increase with lots of (short) connections.

We''ll probably want to keep an eye on this and see if saving ConnectionLifetimes are worth it, given that we don''t actually use the data (due to the [overcounting issue](https://www.notion.so/replicache/Usage-Tracking-Methodologies-Limitations-e94b29188b024038bfbcc183a6d88189)).

', 'darkgnotic');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('3OLqHnYjT9DIVuWKncbiv', 'LIu0go2daaVpcFXW-rFE7', 1699987328000.0, 'More context / ideas from @aboodman here:

https://discord.com/channels/830183651022471199/1020392595450507304/1174054443508043916', 'darkgnotic');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('aLZn622g98HePhTMwGoJJ', 'LIu0go2daaVpcFXW-rFE7', 1699991926000.0, 'I''m pretty sure that Cloudflare workers executes the code once to figure out what the DOs and fetch/alarm etc there are.

esbuild does not check for undefined bindings. This is actually something that it cannot do because someone might have added "window" as a global using `eval`.

I think the task for us is to make sure we forward the stack trace for these errors to the cli.', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('5Sxipavg3Tp4P3dgHWrVZ', 'LIu0go2daaVpcFXW-rFE7', 1699992583000.0, 'I can reproduce this with this change to a sample app:

<img width="718" alt="CleanShot 2023-11-14 at 10 09 00@2x" src="https://github.com/rocicorp/mono/assets/80388/2a84201a-1dd8-45a4-b00f-11fc4e8e5013">

<img width="1089" alt="CleanShot 2023-11-14 at 10 09 22@2x" src="https://github.com/rocicorp/mono/assets/80388/9406a5cd-e8d1-45fc-bc54-eecc20eb8596">
', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('bQY9S0TwO6ROYL1G5ZtFP', 'LIu0go2daaVpcFXW-rFE7', 1699995115000.0, 'Surfacing error code 10021 should be straightforward. I''ll take this.', 'darkgnotic');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('THsx9vsH9HYnIL_Hp0Cbt', 'e4fB1x5jR7I4U8Le6SBRx', 1699883747000.0, 'I think handling this in the CLI and print an error, asking them to update reflect.config.json, is the right approach.

Checking and changing to src/reflect/index.{js,ts} seems like a step too far
', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('iOLoAv39gNU7hZWOCNkJH', 'e4fB1x5jR7I4U8Le6SBRx', 1700032404000.0, '> I think handling this in the CLI and print an error, asking them to update reflect.config.json, is the right approach.

This is already happening:

<img width="1409" alt="CleanShot 2023-11-14 at 21 08 46@2x" src="https://github.com/rocicorp/mono/assets/80388/133a432a-d992-4fc9-88aa-0dbe67d1e456">

---

There are actually a few things that go wrong here though:

1. First thing is that `init` and `create` put the `reflect` dir in different places (`/refect` vs `/src/reflect`). Thus when `init` runs we end up in a confused state where we have two reflect directories (the old one is still there!).

2. If you run `npx reflect dev` at this point, then you have the wrong mutators for the current app (you''re still looking at the `create` app, which wants to call cursor-related mutators).

---

I think the right thing here is to have both apps use the `/src/reflect` directory so that this confusion can''t happen. Then it will do the right thing:

<img width="774" alt="CleanShot 2023-11-14 at 21 13 04@2x" src="https://github.com/rocicorp/mono/assets/80388/7a65055e-7f04-4074-bace-344b830b9e69">

', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('9swUcqisY9JP7RkXH7EoN', 'e4fB1x5jR7I4U8Le6SBRx', 1700178964000.0, 'After thinking about this some, I''d like to just remove `init`. I don''t think it''s buying us much. If we can default the server entrypoint to `reflect-server/index.ts` (and allow it to be overridden via config), then I think we can get away with removing `init`.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('-9KYjoY9QyeBvCGTZbfXm', 'e4fB1x5jR7I4U8Le6SBRx', 1700178981000.0, '(and overall the setup will be easier to understand because less magic)', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('6m20zuqs4hOcQ8Dj8BEcM', 'e4fB1x5jR7I4U8Le6SBRx', 1700180866000.0, 'Actually nevermind, still wringing my hands about this.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('rAR3-oxesMWMd528Jodmi', 'e4fB1x5jR7I4U8Le6SBRx', 1700213560000.0, 'OK back to my original idea. Let''s remove `init`. I think it''s easier overall to walk user through creating the right files. See: https://reflect-docs-git-aa-idea-rocicorp.vercel.app/add-to-existing#sync for what I''m planning.

All we have to do here is remove the `init` subcommand. We''re. not going to refer to it in the docs anymore.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('49idf77eGY2PzsOzgfKbi', 'uNGIrPyPoDby2oNWz2Ayr', 1698799394000.0, 'Actually, I think there will be cases in which we want to create the app without publishing (like `reflect vars set ...`). So I think our options are:

1. Figure out how to get `reflect tail` to display the more helpful text that the server returns
2. Have `reflect tail` check if the app has a `runningDeployment`.

I lean towards (1), but will defer to you.', 'darkgnotic');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('yEvuCisIlLwu99RbTqQr6', 'uNGIrPyPoDby2oNWz2Ayr', 1698836365000.0, '`reflect tail` uses a SSE from mirror-server/Firebase. These HTTP errors should be readable (we have our own custom implementation of SSE). Let''s double check that these errors are reasonably reported.

mirror-server/firebase talks to CF using a Websocket. These errors are reported using a ws message not http headers because those cannot be read by a websocket client. When these happens we forward the error to the cli using a server sent event called error.', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('YGPnek-hF_LTH1VZLXEjA', 'uNGIrPyPoDby2oNWz2Ayr', 1698856535000.0, 'Okay, I found out where the error message is being sent; it''s in `response.text()`. Will send out a PR to surface the message.', 'darkgnotic');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('X4o0Oo0MD64Q07CZ2SN_O', 'yHifzF5hD9i0QUXh6du60', 1698748797000.0, 'Yup. Here it is.

https://github.com/rocicorp/mono/blob/main/packages/reflect-server/src/server/auth-do.ts#L319

The old format was _better_ in the sense that it was shared between tail and connect.', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('6JIfFDHP-Pxaz75m-_W0a', 'yHifzF5hD9i0QUXh6du60', 1699420616000.0, 'Weird ... I just got this again when testing `tail` in prod, on an App running 0.37.202311060940.

Is this supposed to be fixed in that version?

In my case, the Worker existed but I had never actually run the app so the room had not yet been created.

![Screenshot 2023-11-07 at 9 14 43â€¯PM](https://github.com/rocicorp/mono/assets/132324914/8ec8b3ae-d00e-4b6a-be3f-8294fe9d6659)', 'darkgnotic');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('zNpybWwVJUkBa975awIgp', 'yHifzF5hD9i0QUXh6du60', 1699445406000.0, 'That is strange. Maybe needs another publish?', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('sqaFVw12oxLbvseqADSV_', 'yHifzF5hD9i0QUXh6du60', 1699462620000.0, 'Tried again. Here''s the console:

![Screenshot 2023-11-08 at 8 55 11â€¯AM](https://github.com/rocicorp/mono/assets/132324914/c7eb7e24-5c23-4000-b58f-94b7a032d771)

And here are the logs:

![Screenshot 2023-11-08 at 8 56 24â€¯AM](https://github.com/rocicorp/mono/assets/132324914/e13abfb8-8229-42b1-8ffc-9a240313653b)

Anything else I should try?
', 'darkgnotic');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('JbDy52ZnljRwDrvI0QorM', 'yHifzF5hD9i0QUXh6du60', 1699463406000.0, '(Are you able to reproduce it?)', 'darkgnotic');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('miCgoGD-B9oQZTdQCqB5g', 'yHifzF5hD9i0QUXh6du60', 1699476971000.0, 'I see it and I see the error in the auth-do

', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('DI_qtIqmHJZsWbHANevt-', 'B58g9LAcLydOVYSkRW3_t', 1698516599000.0, 'I found the [documentation on this](https://cloud.google.com/functions/docs/bestpractices/retries).

> When retries are not enabled for a (background) function, which is the default, the function always reports that it executed successfully, and 200 OK response codes might appear in its logs. This occurs even if the function encountered an error. To make it clear when your function encounters an error, be sure to [report errors](https://cloud.google.com/functions/docs/monitoring/error-reporting) appropriately.

So we either have to enable retries or use a different mechanism for surfacing the error (which could be error reporting or it could be log levels).

I''ll need to think about whether retries are the right thing to do.', 'darkgnotic');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('udtGMaVQREAe0jjgn_zNH', 'B58g9LAcLydOVYSkRW3_t', 1698517874000.0, 'After more research, I realize that retries may be useful for some cases, but for the purpose of alerts it''s not what we want. We''re supposed to throw an Error for transient, retryable scenarios, and _not_ throw them for non-retryable errors, which is somewhat opposite of how we want to be alerted.

I looked into error reporting, though, and it appears that these errors are already being nicely collected by the error reporter:

https://console.cloud.google.com/errors?project=reflect-mirror-prod&supportedpurview=project

![Screenshot 2023-10-28 at 11 26 43â€¯AM](https://github.com/rocicorp/mono/assets/132324914/3645178d-f75d-4acd-a300-28753c1bfcea)

This is very nice (and I have actually used this in other companies ... just forgot about it  ðŸ˜‰ ). It''s also catching some errors that the alerts missed, like the `Memory Limit` errors on publish.

The key will be figuring out the right process for distinguishing warnings (like the deprecation errors we return to users) from errors that we want to be alerted on. ', 'darkgnotic');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('Ihn1tUL07qoaIPTt0E1Y6', 'B58g9LAcLydOVYSkRW3_t', 1698518550000.0, 'More good news. Error reporter does exactly what we want (with some roughness around the edges).

I was concerned about the deprecation errors rising to the top because I had converted those to warnings quite a while ago. It turns out that it''s a case of mis-bucketing; the error reporter is putting the `dev` errors into same bucket, and until this morning these were classified as errors.

![Screenshot 2023-10-28 at 11 37 14â€¯AM](https://github.com/rocicorp/mono/assets/132324914/1f77d0ef-c17c-4714-bb34-9b2249f8fa7f)

Importantly, the stuff that we classify as warnings do not get surfaced to the error reporter.  ðŸŽ‰ 

It also has some nice features like attaching bugs and setting state of errors to `Acknowledged` and `Resolved`, re-reporting as desired if something resurfaces.

I''m enabling notifications from the Error Reporter to our #mirror-prod-alerts channel. This could conceivably replace our existing error-level alerts (though I''ll keep the warning-level alerts around).

![Screenshot 2023-10-28 at 11 41 50â€¯AM](https://github.com/rocicorp/mono/assets/132324914/a0bcb241-744f-43dd-9904-dcaa16a7432a)


', 'darkgnotic');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('vKDHSoc4iDr0UqQrCoWIH', 'oSH_7bnghHj3YPNvrVbNb', 1698458882000.0, 'This happens even when rolling back to a previously "healthy" release. So it may be specific to `reflect-server.dev`.

Going to worker urls from the browser is fine. I''m kind of hesitant to push a new mirror server to prod though (which is currently working fine).  ðŸ¤” ', 'darkgnotic');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('97k4rc2TIPvpS8n4QIYL9', 'oSH_7bnghHj3YPNvrVbNb', 1698459859000.0, 'After pushing to prod, it happens there too.  ðŸ˜¦ ', 'darkgnotic');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('lxVxzAnRUAFO1JS4K3I6R', 'oSH_7bnghHj3YPNvrVbNb', 1698461189000.0, 'Updating `firebase-functions` to the latest package did not help.', 'darkgnotic');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('yHJjHrr4BCMa-HGnZr3oc', 'T_LUVleECWfFHLT9ybQwt', 1698976557000.0, 'For posterity, I''ve run some experiments to verify the behavior for how Cloudflare enforces its [5KB limit on Environment Variables](https://developers.cloudflare.com/workers/platform/limits/#environment-variables). 

The limit appears to be for the value only:

This works:

```ts
   bindings: {
      vars: {
        [''E''.repeat(1024)]: ''H''.repeat(5120),
      },
```

But this results in an error:

```ts
   bindings: {
      vars: {
        [''E''.repeat(1024)]: ''H''.repeat(5121),
      },
```

```json
  code: 10054,
  error_chain: [
    {
      code: 10054,
      message: ''workers.api.error.text_binding_too_large''
    }
  ]
```

In addition, there appears to be an undocumented limit of 2712 bytes for the key name.

This works:

```ts
bindings: {
      vars: {
        [''B''.repeat(2712)]: ''A''.repeat(5120),
      },
```

But this results in an error:

```ts
bindings: {
      vars: {
        [''B''.repeat(2713)]: ''A''.repeat(5120),
      },
```


```json
code: 10100,
  error_chain: [
    {
      code: 10100,
      message: ''workers.api.error.binding_name_too_large''
    }
  ]
```

Finally, the limit does appear to apply to the UTF-8 encoded byte length. 

For two-byte characters:

```ts
// Works:
bindings: {
      vars: {
        [''M''.repeat(2712)]: ''Â£''.repeat(2560),
      },

// Fails:
bindings: {
      vars: {
        [''M''.repeat(2712)]: ''Â£''.repeat(2561),
      },
 ```

For three-byte characters:

```ts
// Works:
bindings: {
      vars: {
        [''M''.repeat(2712)]: ''æ–‡''.repeat(1706),
      },

// Fails:
bindings: {
      vars: {
        [''M''.repeat(2712)]: ''æ–‡''.repeat(1707),
      },
```', 'darkgnotic');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('U2yYV4cLXfxbd-Gr_GRaT', 'T_LUVleECWfFHLT9ybQwt', 1698976672000.0, 'In summary, Cloudflare''s limits are:

* Max length of UTF-8 encoded variable name: 2712 bytes
* Max length of UTF-8 encoded variable value: 5120 bytes

The policy that we''ve implemented is:
* Max length of UTF-8 encoded name + value: 5120 bytes

Which falls safely within Cloudflare''s constraints.', 'darkgnotic');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('jSvPrArs6fB71ZYpl5Mqo', '8tS30s-ZGcWJSgCCcAqw9', 1698114183000.0, 'Actually, perhaps can just ignore it on the server side. Then we don''t have to add extra logic to the cli. ', 'darkgnotic');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('8b2c7Za9FyIH1hiBEVNj1', 's8-JbMuhMcMaGMpQAts5E', 1698013559000.0, 'Maybe consider `ERR_RUNTIME_FAILURE` too?

Going to lump these into the same issue. Feel free to separate if that''s more appropriate.

https://console.cloud.google.com/monitoring/alerting/incidents/0.n3pfgkk59jih?project=reflect-mirror-prod

![Screenshot 2023-10-22 at 3 24 46â€¯PM](https://github.com/rocicorp/mono/assets/132324914/d2cf4763-6a82-4ec2-8bce-781a939066f0)
', 'darkgnotic');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('J198ib-3zmDtaDCemojcC', 'x2DlBWMg86apkaKv4hG_6', 1697645373000.0, '@cesara ', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('_eF9oIQssyNwDjENhTg4F', 'QUYqA_yGLqpHYJZ5fmXR1', 1697190425000.0, 'Closing. Didn''t show any measurable perf gain.', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('j1ROaaoprI8qe2GBFrjpl', 'WbcOiV7djvjQ0ygCFCmDJ', 1696999779000.0, 'FWIW, I thought that the problem might be due to the large number of HTTP requests that `npm install` performs during a `reflect create`, so I tried a similar scenario with a fresh `reflect init`, but does not result in the same issue.

```
 analytics-test $ rm -rf node_modules reflect.config.json package-lock.json 
 analytics-test $ node ~/roci/mono/mirror/reflect-cli/out/index.mjs --stack=sandbox init
Installing @rocicorp/reflect
npm WARN deprecated sourcemap-codec@1.4.8: Please use @jridgewell/sourcemap-codec instead

added 531 packages, and audited 532 packages in 25s

75 packages are looking for funding
  run `npm fund` for details

found 0 vulnerabilities

You''re all set! ðŸŽ‰

To start the Reflect dev server:

npx @rocicorp/reflect dev
 analytics-test $ 
 ```', 'darkgnotic');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('pyl-FE33LUn_VGrHdMzim', 'NqY5fk7zVCFKzxKBt7N69', 1696318268000.0, 'We need to be deliberate if we want server support. The code we had was using localStorage to do the broadcasting and that is not available on servers either.

FWIW, server environments are starting to support more and more of the browser APIs.

### BroadcastChannel
- https://nodejs.org/api/worker_threads.html#class-broadcastchannel-extends-eventtarget
- https://docs.deno.com/deploy/api/runtime-broadcast-channel
- https://bun.sh/blog/bun-v0.7.2


### localStorage
- nothing for nodejs
- https://docs.deno.com/runtime/manual/runtime/web_storage_api
- nothing for bun', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('y27qDCdALUxhMFVyYSJDa', 'NqY5fk7zVCFKzxKBt7N69', 1696319609000.0, 'Fair. But does the bug report of BroadcastChannel not found on Safari 15.4 make sense to you? Noam is even saying he sees this in Safari 16.6 somehow.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('d6dl7sXaWXj3Ztg3-9zkL', 'NqY5fk7zVCFKzxKBt7N69', 1696319626000.0, '(I''m getting more information, just wondering right now if this jogs any ideas for you)', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('aN7LdNRrn9sP0H0GnCGIA', 'NqY5fk7zVCFKzxKBt7N69', 1696328160000.0, 'No ideas why Safari would not have it.

I''m thinking we could not broadcast the message if BroadcastChannel is not available. We currently use a channel for 2 things:

1. In case there is a newer client group so that the other tab can reload. This is so that offline usage can sync through IDB.
2. Informing ohter tabs that persist is done. This is once again to allow other tabs to pick up the changes faster.

If we have a noop channel for these in problematic browsers I think everything will continue to work but the multiple tab scenario sync will be slower.

@grgbkr What do you think?', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('0aGm-9MMRCGNIA30juHg4', 'NqY5fk7zVCFKzxKBt7N69', 1696492634000.0, 'This seems like a reasonable compromise to me.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('FbOIrquAUrsa7sjy0ZLZA', 'D43Q0DwS-qD5VOGcKntt6', 1695945454000.0, 'Landed with helpful feedback / discussion with Aaron in https://rocicorp.slack.com/archives/C013XFG80JC/p1695926653293939

TL;DR, dist-tags are:
* `@latest`
* `@rec`: minimum non-deprecated version
* `@sup`: minimum supported version

<img width="677" alt="Screenshot 2023-09-28 at 1 10 37 PM" src="https://github.com/rocicorp/mono/assets/132324914/8f749859-7925-4eb7-89d3-cbfcc0544f8e">

', 'darkgnotic');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('vyIXI3pw-Mms4icG1q3el', '9GXlK-CM2benNeZJ4T0Y-', 1695301351000.0, 'Two options:

1. Remove the CJS modules from replicache 13. They do not work
2. Update all the deps to have both esm and cjs

My vote is to do 1.', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('8Wg1iXRAIJe5MThH7dmI_', '9GXlK-CM2benNeZJ4T0Y-', 1695301430000.0, 'Let''s try 1 for a bit and see how it flies.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('5mm0JxBJGs417HkttCNVt', 'Hv6m4oCF_ILqiPYWw5cLL', 1695188457000.0, 'Another relevant thread is https://rocicorp.slack.com/archives/C013XFG80JC/p1695078215793509, where we consider a push model in which RoomDOs with connections periodically push their connection sets to the AuthDO via an Alarm. This would obviate the ping-and-wait-for-wake-up fuzziness.

I think technically this would spread out the revalidations and cause the AuthDOs to be awake more, but I imagine it''s a negligible increase in total execution time when the RoomDOs are active, and execution time will still zero out when there are no active connections.', 'darkgnotic');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('9DfmclJfyqelbt2O-akT9', 'Hv6m4oCF_ILqiPYWw5cLL', 1695190065000.0, 'And as a general principle, we should strive for designs/protocols that minimize the I/O (and execution variability) that happens in a lock.  (Reminds me of #567 as another example of how protocol design can streamline critical sections)', 'darkgnotic');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('5PVFRTmLzMacYAFSS6oXU', 'Hv6m4oCF_ILqiPYWw5cLL', 1695321052000.0, '> And as a general principle, we should strive for designs/protocols that minimize the I/O (and execution variability) that happens in a lock. (Reminds me of #567 as another example of how protocol design can streamline critical sections)

Agreed.  I was aware at the time of the short comings of the locking in AuthDO, but I was satisficing on the design.   I agree that the timestamp based approach in https://github.com/rocicorp/mono/issues/567 is the way to go. ', 'grgbkr');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('T0UwXD5o6bfCQS-h4rC_x', 'FU2janrOctZZvFLLvMtPs', 1694159645000.0, 'I think subscribe is the wrong abstraction. Maybe watch is a better one since there is not `body` to execute here.', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('bB2zIWFvf05DjrWeutlV3', 'FU2janrOctZZvFLLvMtPs', 1709537043000.0, 'We actually did this!', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('Ob0fMHN-5BFmFU1mGXF0C', 'qNtxQnpxgL89FkPLw7b5C', 1693511148000.0, 'We should definitely do a pass over how we expose errors. Right now we have a lot of unhandled exceptions that are surfaced.', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('G5NzYZQxEpulYM4RsnX61', 'qNtxQnpxgL89FkPLw7b5C', 1693511338000.0, 'It looks like we are limited to 99 domain records:

https://dash.cloudflare.com/085f6d8eb08e5b23debfb08b21bda1eb/reflect-server.net/dns/records?recordsPage=2

<img width="777" alt="Screenshot 2023-08-31 at 12 47 41 PM" src="https://github.com/rocicorp/mono/assets/132324914/6120028d-ef26-48b8-aed8-fcbb4ff8dfc8">

I didn''t find a place in the dashboard where the limit can be increased.', 'darkgnotic');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('45kZH-SJVvr-J6WVALGfl', 'qNtxQnpxgL89FkPLw7b5C', 1693514510000.0, 'I will loop in cloudflare.

On Thu, Aug 31, 2023 at 9:49â€¯AM d-llama ***@***.***> wrote:

> It looks like we are limited to 99 domain records:
>
>
> https://dash.cloudflare.com/085f6d8eb08e5b23debfb08b21bda1eb/reflect-server.net/dns/records?recordsPage=2
> Screenshot.2023-08-31.at.12.47.41.PM.png (view on web)
> <https://github.com/rocicorp/mono/assets/132324914/6120028d-ef26-48b8-aed8-fcbb4ff8dfc8>
>
> I didn''t find a place in the dashboard where the limit can be increased.
>
> â€”
> Reply to this email directly, view it on GitHub
> <https://github.com/rocicorp/mono/issues/902#issuecomment-1701690289>, or
> unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AAATUBAUXXTFOTP6VGYEHMDXYDTDJANCNFSM6AAAAAA4GR2TG4>
> .
> You are receiving this because you were mentioned.Message ID:
> ***@***.***>
>
', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('ur4Ao2_0H3iq5a_7f1v0R', 'qNtxQnpxgL89FkPLw7b5C', 1693514539000.0, '+Aaron Boodman ***@***.***>

On Thu, Aug 31, 2023 at 10:41â€¯AM Aaron Boodman ***@***.***>
wrote:

> I will loop in cloudflare.
>
> On Thu, Aug 31, 2023 at 9:49â€¯AM d-llama ***@***.***> wrote:
>
>> It looks like we are limited to 99 domain records:
>>
>>
>> https://dash.cloudflare.com/085f6d8eb08e5b23debfb08b21bda1eb/reflect-server.net/dns/records?recordsPage=2
>> Screenshot.2023-08-31.at.12.47.41.PM.png (view on web)
>> <https://github.com/rocicorp/mono/assets/132324914/6120028d-ef26-48b8-aed8-fcbb4ff8dfc8>
>>
>> I didn''t find a place in the dashboard where the limit can be increased.
>>
>> â€”
>> Reply to this email directly, view it on GitHub
>> <https://github.com/rocicorp/mono/issues/902#issuecomment-1701690289>,
>> or unsubscribe
>> <https://github.com/notifications/unsubscribe-auth/AAATUBAUXXTFOTP6VGYEHMDXYDTDJANCNFSM6AAAAAA4GR2TG4>
>> .
>> You are receiving this because you were mentioned.Message ID:
>> ***@***.***>
>>
>
', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('V-oztBlbCaIrqTie5jviw', 'qNtxQnpxgL89FkPLw7b5C', 1694564907000.0, 'FYI, I think both we and Tanushree misunderstood the problem here. (The hint was that she referred to some kind of domain limit *per worker*, which was not our problem).

I think the limit of 100 Custom domains is just part of our Free plan:

https://developers.cloudflare.com/pages/platform/limits/#custom-domains

> Custom domains
> Based on your Cloudflare plan type, a Pages project is limited to a specific number of custom domains. This limit is on a per-project basis.
> 
> Free | Pro | Business | Enterprise
> -- | -- | -- | --
> 100 | 250 | 500 | 500
> 

(I realize that this is part of the "Pages" documentation but my hunch is that this is where the limit comes from.)

 Upgrading to Pro or Business would cost $20 and $200 per month, respectively:

<img width="1336" alt="Screenshot 2023-09-12 at 5 20 29 PM" src="https://github.com/rocicorp/mono/assets/132324914/865e9840-1978-43cf-a0f0-a9971ff1496f">

The silver lining here is that we should be able to overcome our limit by upgrading our plan, so we''re not blocked on migrating to Workers for Platforms.

I''m still digging into mapping out a game plan for WfP, but it won''t change the fact that we''ll need to pay for more domains, as WfP has similar Plan-based limits on hostnames:

https://developers.cloudflare.com/cloudflare-for-platforms/cloudflare-for-saas/plans/

<img width="1328" alt="Screenshot 2023-09-12 at 5 24 39 PM" src="https://github.com/rocicorp/mono/assets/132324914/649f7457-014d-4eae-9556-37a21094fe14">


 ', 'darkgnotic');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('JCyE0zuDeXeKhX62UY_z1', 'qNtxQnpxgL89FkPLw7b5C', 1694631544000.0, 'After playing around with this, it turns out that I was wrong.

* reflect-server.dev (Free Plan, Rocicorp DEV account): Max of 100 worker custom domains
* reflect-server.net (Free Plan, Rocicorp LLC account): Max of 300 worker custom domains
* replicache.dev (Enterprise Plan, Rocicorp LLC account): Max of 300 worker custom domains

So indeed our limit is what Tanushree bumped us too, even for the zone that''s officially on the "Enterprise Plan".

So moving to Workers for Platforms should indeed allow us to scale to many more hostnames. The first 100 are free, and the default max is 5000, but Enterprise customers can remove that 5000 limit by contacting sales:

https://developers.cloudflare.com/cloudflare-for-platforms/cloudflare-for-saas/plans/', 'darkgnotic');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('JRpyCuEEX1SIHmHg7tHFZ', 'qNtxQnpxgL89FkPLw7b5C', 1697222471000.0, 'Problem is understood now. The migration to WFP addresses this issue.', 'darkgnotic');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('cCF95lR5rRPhTCbVKWp7-', '_cXlBrP9gPy7bPU9e29Qq', 1692382992000.0, 'Two things seem to work, though neither of them ideal.

1. Reference a different version for `@rocicorp/reflect` in the `mirror-cli/package.json`. Then npm picks up the code from the registry.
2. Download the tarball for the package and reference it as `file:reflect-...tgz` in `mirror-cli/package.json`. This allows you to use the version that''s in the repo, while using the code actually published in npm. But it takes a bit of work.

It would be nice to have a magic solution that makes `mirror-cli` always reference the canonical npm package.  ðŸ¤” ', 'darkgnotic');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('aMyIGr-AK2FTFI8IulfIq', '_cXlBrP9gPy7bPU9e29Qq', 1692848163000.0, 'I think a way to do this could be to download the tarball from npm directly and extract it: https://stackoverflow.com/questions/33530978/download-a-package-from-npm-as-a-tar-not-installing-it-to-a-module', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('vGH0RRHtrFCZg45PpJWiU', '_cXlBrP9gPy7bPU9e29Qq', 1692907314000.0, 'Let''s download the file from npm', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('kTUMpUzF5rFrA_ry5zjGE', '_cXlBrP9gPy7bPU9e29Qq', 1694778540000.0, 'I looked at this a bit. Downloading the tarball is fine, but then when we try to build from it we need to `npm install` because of the deps. Not a big deal since this is only for our internal usage.', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('0x28PcWByHzlS0qeRxBya', '_cXlBrP9gPy7bPU9e29Qq', 1694779516000.0, '...and if have to use `npm install` to get the deps we might as well use npm add `@rocicorp/reflect` to get the files.', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('GOOup0xzR9y3EQeyi-gpv', '_cXlBrP9gPy7bPU9e29Qq', 1694810971000.0, 'One thing that I have in my client is an option to build from source (to be able to push non-published servers for development or debugging). It''s basically a flag that asserts that the path does _not_ have /node_modules in it, as well as a fake version to upload the module as.

It would be nice to be able to preserve that capability if possible. I''ll send you a PR so that you have a better idea.', 'darkgnotic');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('_14kvdI2Jyi_oHymkZguI', 'tTqGbFV7s5es4M19NaRn3', 1691542302000.0, 'See also #367, #808, #807.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('7127Kf2pGuE6rBHhvZMNq', 'pKyFIZHzJ5xaEmJJpStDa', 1691466519000.0, 'Actually now that I think of it, I remember that Erik and I decided that Valita was fast enough to have on by default, but just that we could add an "escape hatch" flag for disabling it if user really wanted to go as fast as possible. I can''t remember if this decision applied to both client and server or if we actually added the hatch.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('vuIVirl7i_7M6uogb3R3P', 'pKyFIZHzJ5xaEmJJpStDa', 1691478872000.0, 'That seems to fit with what I remember too.

We did not add the escape hatch yet.

I think the next step is to identify where validation is happening and decide what knobs to provide.', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('esEGiKi6kw6YncXwMUYme', '2y79vz_jBCdGgRqvegYyE', 1691396851000.0, 'Thanks', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('uZcl4gmDb9wQKPqLuWICZ', 'O1U5xVLfsbRBoIziTaqtt', 1690585245000.0, 'Thanks @grgbkr 

With the ability for sandbox to have its own env vars, sandbox.reflect.net can use the `reflect-mirror-staging` (perhaps renamed to `reflect-mirror-sandbox`) FIrebase project so that reflect-cli + cloud-function development does not require running a local instance of the login page.', 'darkgnotic');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('CvmbAekqeisHIz9jrxwZG', '8MfQ0wLfkAtOot-3yYlcG', 1690493573000.0, 'cc @arv @grgbkr ', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('UKsGHenduRMrElKkWvQp0', '8MfQ0wLfkAtOot-3yYlcG', 1710163847000.0, 'fixed by https://github.com/rocicorp/mono/pull/1463', 'tantaman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('mhR4OvRXj_HedgKMHOZyP', 'yqygBXzMFKPGv1gk9ByR1', 1709545367000.0, 'I believe closely related to #754', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('_hsd0fG5BE0m1LhmaHHWh', 'yqygBXzMFKPGv1gk9ByR1', 1709577933000.0, '~~Looks like this is already fixed (see console output in the screenshot below) --~~

![Screenshot 2024-03-04 at 1 43 00â€¯PM](https://github.com/rocicorp/mono/assets/1009003/83d5656e-cd15-401e-a1a7-a72d0c699f67)

~~Assuming that the correct behavior is to throw away null cookies, which it must be since `null` indicates the _first_ cookie: https://doc.replicache.dev/reference/server-pull#cookie~~

Ignore me. Was able to repro in a new tab.
', 'tantaman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('Qm-iBsItnWhjw2SEpLMvJ', 'Pm9ARiE3x3zpUTVSjzciR', 1689238462000.0, 'I like the idea of deleting all local state in debug mode!', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('byk2x_rEM7nm7EM9M_flz', 'Pm9ARiE3x3zpUTVSjzciR', 1689266714000.0, 'Why doesn''t refreshing fix it?  The new client should not get assigned to the disabled client group, but perhaps we have a bug here https://github.com/rocicorp/mono/blob/main/packages/replicache/src/persist/clients.ts#L508.  

', 'grgbkr');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('OUMR9hGQXytQR_J9Dv7fP', 'Pm9ARiE3x3zpUTVSjzciR', 1689277403000.0, 'I didn''t understand that''s what this code is trying to do. I don''t think it''s what I''m seeing though, will confirm.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('cJW9hdkyROPYJUGVkyzMv', 'mGoIMzuUM0tNhoILcqLsx', 1689238713000.0, 'We used to use localStorage as a fallback. We removed it to make things simpler. There is no reason we cannot add back that fallback path.', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('WZdAbM47RilOEsY1hlsMH', 'nE8sHktFu8R_UNrb9LG0y', 1689882914000.0, '- [ ] Mirror server generates a REFLECT_AUTH_API_KEY when an app is created. This key gets sent to the client when the app is created and printed to the console. It is also stored in firebase in the apps collection so that we can set the secret when we publish to cloudflare.
  - [ ] Should we store this in the app config (reflect.config.json)?
- [ ] Provide a way to reset/get a new REFLECT_AUTH_API_KEY in case the key has been compromised.
- [ ] For dev mode we can use a dummy REFLECT_AUTH_API_KEY.
- [ ] We should remove the authentication for calls from the main worker to the DOs since these are safe and can only come from the same CF worker script.', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('o9mv_udalEDFm678DY_hq', 'nE8sHktFu8R_UNrb9LG0y', 1695410765000.0, 'I had a conversation with Greg about this a while back, and the preference we concluded was to handle this value as a secret and avoid storing it insecurely (which includes storing it in plainly in Firestore, as that data can be exposed in leaked backups, etc.).

What I''d prefer to avoid, however, is storing a lot of secrets in the Secret Manager because it''s extra datastore management and is [relatively expensive](https://cloud.google.com/secret-manager/pricing) (at least, compared to the other GCP costs,  which for our usage is pretty much free).

The rough idea I had in mind is to store a single master key in the Secret Manager, and then store a random plaintext in Firestore for each app. The REFLECT_AUTH_API_KEY for the app would be the plaintext encrypted with the master key. On that scheme we can implement key replacement by replacing the plaintext, or key rotation by storing multiple plaintexts (and having reflect accept multiple keys).

The downside to this approach is that leaking the master key puts everyone''s keys at risk, but only if the plaintexts are also exposed. I think the way to address this is to have each plaintext be associated with the master key version, and if the latter is leaked, we would create a new master key/version and rotate in a new plaintext with that version. Then we''d notify everyone to switch to their new resulting API_KEY.

This is not a high priority at the moment, but I wanted to jot down my thoughts so that I don''t forget.', 'darkgnotic');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('wrSV-lSxNUhvvOeTrWqeV', 'nE8sHktFu8R_UNrb9LG0y', 1698424739000.0, 'I''ll take this as it it has some synergy with #1150', 'darkgnotic');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('l00PFJs1BQ8emgft4_Tp-', 'nE8sHktFu8R_UNrb9LG0y', 1698759114000.0, 'I feel like this is not done.

We do not yet have a way to get the REFLECT_AUTH_API_TOKEN so that we can invoke the REST API.

Straw proposal:

```
npx reflect api-token 
npx reflect api-token --rotate
```

I also think we might want to expose the actual REST endpoints as conveniences on the reflect commands. See https://github.com/rocicorp/reflect-todo/blob/main/doc/server-api.md for a list of the existing endpoints', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('xZDOXRbohD1C4CU0mCp9q', 'nE8sHktFu8R_UNrb9LG0y', 1698765042000.0, 'Good point. For real down-time free rotation, we would technically need to add `reflect-server` support for two keys (old and new) while clients are updating, but we can probably get away with one-key-at-a-time rotation.', 'darkgnotic');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('Mk6AyCLt7s8MP9vRCJO3s', 'nE8sHktFu8R_UNrb9LG0y', 1698765285000.0, 'Also, if we did want to rename the header, now would be the time to do it. 

 @grgbkr @arv what do you think?', 'darkgnotic');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('iwkzghBOxWfQa3sE9bXT1', 'nE8sHktFu8R_UNrb9LG0y', 1698879323000.0, 'Yes. Let''s rename it ', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('ORmG2iWN6h1dl5ys7mYvL', '6X2YCjgiwHiZWh2-YbeM6', 1687814590000.0, 'The problem is that we were using `[string, string][]` for the HTTP headers. The fetch spec allows this but it seems like React Native is having trouble with this. It isn''t clear if they have fixed this or not (their .d.ts includes the tuple form).

I changed the license code to use `Record<string, string>` in `replicache` but we would need a release for this to work out of the box.', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('-hyQOvIFhTwHhnn3Bwtol', 'LqxDeWIa4yuHRNl-iM4lM', 1687497970000.0, 'To unblock deploys I''m changing the build command  from:
`npm run build --prefix=../.. && ./publish-if-production.sh`
to:
`npm run build --prefix=../..`

https://github.com/rocicorp/mono/pull/639


We should try to get this working again.  For now we will need to manually wrangler publish from our machines.', 'grgbkr');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('eB-L178RznYGaRHX5aJmK', 'LqxDeWIa4yuHRNl-iM4lM', 1687545765000.0, 'im also sometimes seeing this error when publishing from my machine.  ', 'grgbkr');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('qPAJS44SDq6ObwKI4B0sd', 'LqxDeWIa4yuHRNl-iM4lM', 1687556528000.0, 'I think this may have started with my change that pulls in the datadog libraries, which increased the code size to 2000+ kb. According to [Worker startup time](https://developers.cloudflare.com/workers/platform/limits/#worker-startup-time):

```
 Script size can impact startup because thereâ€™s more code to parse and evaluate.
```

The amount of actual code that we use in the libraries is not actually that large though, so I''m wondering if we''re not effectively tree-shaking the new code.

I tried adding the [`commonjs()`](https://github.com/rollup/plugins/tree/master/packages/commonjs#readme) plugin to our rollup config, but that didn''t help (perhaps because rollup is only used for the d.ts files). Maybe @arv can figure out whether we can improve the tree shaking with the commonjs libraries we''re pulling in.

The other option, of course, is to roll back the datadog lib change and handroll the api / monitoring code, but if we can solve this at the toolchain level it would improve our ability to pull in 3rd party libs.', 'darkgnotic');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('XGvK19xB9WkG6HEC4dSJQ', 'LqxDeWIa4yuHRNl-iM4lM', 1687772186000.0, '```
$ cd apps/reflect.net
$ wrangler publish --dry-run --outdir distx
$ ls -l distx/
total 14096
-rw-r--r--  1 arv  staff      117 Jun 26 11:00 README.md
-rw-r--r--  1 arv  staff  2788488 Jun 26 11:00 index.js
-rw-r--r--  1 arv  staff  4421961 Jun 26 11:00 index.js.map
```

Going back to the change before 897ceacffdb964bd4d96706d459acca411e6401a:

```
$ git co 897ceacffdb964bd4d96706d459acca411e6401a~1
$ npm run build
$ cd apps/reflect.net
$ wrangler publish --dry-run --outdir distx2
$ ls -l distx2/
total 2728
-rw-r--r--  1 arv  staff      117 Jun 26 11:16 README.md
-rw-r--r--  1 arv  staff   262344 Jun 26 11:16 index.js
-rw-r--r--  1 arv  staff  1125319 Jun 26 11:16 index.js.map
```

The server code size increased 10x 

Cloudflare claims the code size limit is 10MB and 1MB on free accounts https://developers.cloudflare.com/workers/platform/limits/#worker-size

There is also a [startup time limit of 200ms](https://developers.cloudflare.com/workers/platform/limits/#worker-startup-time). It seems plausible that those 2.5MB of code the datadog library takes too long to parse and initialize.

Next step... Try some dead code elimination', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('N3D-dfiZvuyqHkEpp2JCg', 'tZNGGFnoeAlCgEZoimkt2', 1687500317000.0, 'https://codemirror.net/examples/collab/', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('r14T8RXlv8PvWs7UC1TFY', '40G7H1-7njAN3flAvj-Vm', 1687213365000.0, '<img width="918" alt="image" src="https://github.com/rocicorp/mono/assets/19158916/c3fb0f8a-3f3f-4b5f-ba7b-51c5b4d8b69c">
', 'grgbkr');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('R3j-gPD7nKPWlXrSmyBE2', '40G7H1-7njAN3flAvj-Vm', 1687247873000.0, '> 4. A suspicion: Is it possible you are missing waiting on a promise somewhere, and the mutator is actually completing before the call to tx.get call in rehashBlockPaths that is leading to the ChunkNotFoundError?

I was thinking the same thing.

Are we missing a check for is `closed` somewhere along the path?', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('-b6CzznGQXcseRkIDJeLH', '40G7H1-7njAN3flAvj-Vm', 1687248457000.0, 'It would also be important to know if they are using more than one instance of `Replicache`. All the issues we have seen in the past have been due to us not keeping things alive correctly related to persist/refresh.', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('DgZbWNFImNSTc8sn-H1JV', '40G7H1-7njAN3flAvj-Vm', 1687366990000.0, 'Meeting with the customer that reported this, Julian Benegas, today.', 'grgbkr');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('27CFsTtn77zv8avtv0auh', '40G7H1-7njAN3flAvj-Vm', 1687398822000.0, 'From talking with Julian I have learned that.

- The issue is not a missing await on a promise, the ChunkNotFoundError is happening before the mutator''s returned promise resolves.
- Customer was encountering ChunkNotFoundError''s in Replicache 12.0.1 as well.
- In this repro with deleteBlocks, actually several calls to tx.get are hitting ChunkNotFoundErrors, but they are being collapsed to one error by Promise.all. 
- The issue seems to be related with doing parallel work in mutators.  The customer has encountered ChunkNotFoundErrors in a few mutators and has found that replacing `Promise.all` with a for loop that awaits sequentially often avoids the ChunkNotFoundError.  In this specific repro with deleteBlocks, the error can be avoided by replacing
``` 
await Promise.all([
   ...(isBlockWithChildren(block)
     ? block.value.children.map(async (c) => {
         await deleteBlockAndNestedBlocks(tx, {
           id: c.id,
           idSeed,
           isoNow,
           path: path + ''/'' + c.id,
           skipRehashing, 
         })
       })
     : []),
 ])
```
  with 
```
  if (isBlockWithChildren(block)) {
    for (const child of block.value.children) {
      await deleteBlockAndNestedBlocks(tx, {
        id: child.id,
        idSeed,
        isoNow,
        path: path + ''/'' + child.id,
        skipRehashing, 
      })
    }
  }
```

The code for the deleteBlocks mutator that leads to this error is below.

```
/* -------------------------------------------------------------------------------------------------
 * DELETE
 * -----------------------------------------------------------------------------------------------*/

export type DeleteBlockParams = {
  id: string
  path: string
  isoNow: string
  idSeed: string
  skipRehashing?: boolean
}

export const deleteBlock = async (
  tx: WriteTransaction,
  { id, path, isoNow, idSeed, skipRehashing }: DeleteBlockParams
) => {
  const normalizedPath = normalizeStringPath({ path, edgeId: id })
  const blocksInPath = [...normalizedPath]

  const rootBlockId = blocksInPath[0]
  invariant(rootBlockId)
  const blockId = blocksInPath.pop()
  const parentBlockId = blocksInPath.pop()

  if (!blockId || !parentBlockId) throw new Error(''Invalid path'')

  const [thisBlock, parentBlock] = await Promise.all([
    blockBaseOps.get(tx, blockId),
    blockBaseOps.get(tx, parentBlockId),
  ])
  if (!thisBlock) throw new Error(`Block with id ${blockId} not found`)
  if (!parentBlock || !isBlockWithChildren(parentBlock)) {
    throw new Error(`Parent block not found or not valid`)
  }

  if (isComponentBlock(thisBlock)) {
    await detachInstancesOfComponent(tx, {
      isoNow,
      idSeed,
      blockId: thisBlock.id,
      path,
    })
  }

  await deleteBlockAndNestedBlocks(tx, {
    id,
    path,
    isoNow,
    idSeed,
    skipRehashing,
  })

  // if parent block is component, need to re-sync all instances of it
  if (isComponentBlock(parentBlock)) {
    await syncAllComponentInstances(tx, {
      componentBlockId: parentBlock.id,
      idSeed,
      isoNow,
      rootBlockId,
    })
  }
}

/**
 * Self explanatory.
 *
 * **IMPORTANT:** Assumes you''re not passing a child key and then its parent.
 * You''ll need to handle that filtering before passing the blockIds here, else you''ll break stuff.
 * See the tree primitive for an example implementation on how selected keys are filtered by parent/child relationship.
 */
export async function deleteBlocks(
  tx: WriteTransaction,
  {
    rootId,
    blockIds,
    isoNow,
    idSeed,
  }: { rootId: string; blockIds: string[]; isoNow: string; idSeed: string }
) {
  console.log(''starting deleteBlocks'')
  const allBlocks = await blockBaseOps.list(tx)
  // 1. build tree
  const treeManager = await buildTree(tx, rootId, allBlocks)

  // 2. get paths for each block
  const blockPaths = blockIds.map((bId) => {
    const block = treeManager.getNode(bId)
    if (!block) throw new Error(`Block with id ${bId} not found`)
    const path = treeManager.getPathForKey(bId, ''string'')
    return { path, bId }
  })

  // 3. call `deleteBlockOnPath` on each one.
  // unfortunately, this needs to be synchronous, as we need to delete the blocks in order
  for (const { bId, path } of blockPaths) {
    await deleteBlock(tx, {
      id: bId,
      path,
      isoNow,
      idSeed,
    })
  }
  console.log(''returning from deleteBlocks'')
}

/**
 * Deletes block and all its nested blocks, without worrying about paths or hashes.
 * WARNING: This function should be paired with another function that updates the parent block''s hash and value.
 */
export async function deleteBlockAndNestedBlocks(
  tx: WriteTransaction,
  {
    id,
    path,
    isoNow,
    idSeed,
    skipRehashing,
  }: {
    id: string
    path: string
    isoNow: string
    idSeed: string
    skipRehashing?: boolean
  }
) {
  const normalizedPath = normalizeStringPath({ path, edgeId: id })
  const block = await blockBaseOps.get(tx, id)
  if (!block) throw new Error(`Block with id ${id} not found`)

  if (isComponentBlock(block)) {
    await detachInstancesOfComponent(tx, {
      isoNow,
      idSeed,
      blockId: block.id,
      path,
    })
  }

  // 1. delete
  await blockBaseOps.delete(tx, id)
  // deleting test

  // 2. rehash (remove reference from parent)
  if (!skipRehashing) {
    // root/test/a
    // root/test/b
    // root/test/c
    await rehashBlockPaths(tx, { paths: [normalizedPath.join(''/'')] })
  }

  // 3. delete orphan children
  await Promise.all([
    ...(isBlockWithChildren(block)
      ? block.value.children.map(async (c) => {
          await deleteBlockAndNestedBlocks(tx, {
             id: c.id,
             idSeed,
             isoNow,
            path: path + ''/'' + c.id,
             skipRehashing, 
           })
         })
      : []),
  ]);
}

/* -------------------------------------------------------------------------------------------------
 * Re-hash Paths
 * -----------------------------------------------------------------------------------------------*/

export const rehashBlockPaths = async (
  tx: WriteTransaction,
  { paths }: { paths: string[] }
) => {
  const normalizedPaths = paths.map((p) => {
    return normalizeStringPath({
      path: p,
      format: ''array'',
    })
  })

  // merge these paths into the shortest possible quantity
  // for example, if we have [''a'', ''a/b'', ''a/b/c'', ''a/b/c/d'']
  // we can only have [''a/b/c/d''], and that should cover all the re-hashing we need to do
  const mergedPaths = normalizedPaths.reduce<string[][]>((acc, path) => {
    if (acc.length === 0) {
      acc.push(path)
      return acc
    }

    const existingPathIndex = acc.findIndex((p) =>
      path.join(''/'').startsWith(p.join(''/''))
    )

    if (existingPathIndex !== -1) {
      acc.splice(existingPathIndex, 1, path)
    } else {
      acc.push(path)
    }

    return acc
  }, [])

  // store old hashes, to decide if we send an update
  const blockHashMap = new Map<string, string | null>()
  // store all blocks that will get updated
  const blockCache = new Map<string, Block | null>()

  const getBlockOrCache = async (id: string) => {
    if (blockCache.has(id)) {
      return blockCache.get(id) ?? null
    }
    const block = await blockBaseOps.get(tx, id)
    blockCache.set(id, block ?? null)
    if (!blockHashMap.has(id)) blockHashMap.set(id, block?.hash ?? null) // store old hash first time
    return block ?? null
  }

  await Promise.all(
    mergedPaths.map(async (path) => {
      await Promise.all(
        path.map(async (id) => {
          await getBlockOrCache(id)
        })
      )
    })
  )

  // re-hash all blocks (but don''t update yet, cause some blocks might be re-hashed more than once! e.g; root block)
  for (let index = 0; index < mergedPaths.length; index++) {
    const path = mergedPaths[index]
    invariant(path)

    let previousBlock: { id: string; hash: string } | undefined = undefined
    let previousBlockDeleted = false

    while (path.length > 0) {
      const currentBlockId = path.pop()
      invariant(currentBlockId)
      const block = await getBlockOrCache(currentBlockId)

      if (block && isBlockWithChildren(block) && previousBlock) {
        if (previousBlockDeleted) {
          // remove the reference to the previous block
          block.value = {
            ...block.value,
            children: block.value.children.filter(
              (child) => child.id !== previousBlock?.id
            ),
          }
        } else {
          // update the hash of the child reference to the previous block
          block.value = {
            ...block.value,
            children: block.value.children.map((child) => {
              if (child.id === previousBlock?.id) {
                return { ...child, hash: previousBlock.hash }
              }
              return child
            }),
          }
        }
      }

      if (!block) {
        // block was deleted
        // so parent will need to remove its reference to this block
        previousBlockDeleted = true
        previousBlock = { id: currentBlockId, hash: '''' }
      } else {
        previousBlockDeleted = false
        // else normal case: hash it, and store it as the previous block
        const hash = hashBlock(block)
        block.hash = hash
        blockCache.set(block.id, block)
        previousBlock = { id: block.id, hash }
      }
    }
  }

  // update all blocks that have changed
  const voidPromises: Promise<void>[] = []
  for (const [id, block] of blockCache.entries()) {
    if (!block) continue
    if (blockHashMap.get(id) !== block.hash) {
      voidPromises.push(blockBaseOps.update(tx, block))
    }
  }

  await Promise.all(voidPromises)
}
```', 'grgbkr');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('9eGCECBijQZoUPOxEFA2M', '40G7H1-7njAN3flAvj-Vm', 1687399009000.0, 'I have been trying to create a reduced repro by writing mutators that do similar things to the above (a mix of deletes and reads done in parallel), but so far have not had luck.

', 'grgbkr');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('10jhJdusqvr5QIB-5uQ9w', '40G7H1-7njAN3flAvj-Vm', 1687420952000.0, 'Did you figure out if they have multiple Replicache instances?', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('MW4CiBdocX4NwECvT2wsO', '40G7H1-7njAN3flAvj-Vm', 1687861844000.0, 'Here is a reproducible test: https://github.com/rocicorp/mono/pull/657. We are getting a `_splice` of a mutable node during the `get`.

Possible solutions:

### No mutable nodes

The motivation of allowing nodes to be mutable was for performance and memory usage. If we can prove that it is safe to mutate the Node then a `_replaceChild` (for example) is `O(1)` instead of `O(n)` where `n` is the number of nodes at that level. If we have these as immutable we have to copy the entries and create new Nodes which puts more pressure on the GC.

### Read Write Lock

Right now the BTreeWrite has a lock on the write operations. For read, we tried to prevent having an RWLock by checking if the tree changed and then start over in the case of a change.
', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('N3f44h8i53ZAmlZvCMtbG', '40G7H1-7njAN3flAvj-Vm', 1687881882000.0, 'I benchmarked things with isMutable always false. This means that we never mutate existing nodes and create new ones for partition etc.

https://docs.google.com/spreadsheets/d/1UPeZX3Xy84ZPkfMYDPAE-305PtAynotZJI7uZdBGjGM/edit?usp=sharing

[populate tmcw](https://docs.google.com/spreadsheets/d/1UPeZX3Xy84ZPkfMYDPAE-305PtAynotZJI7uZdBGjGM/edit#gid=0&range=B33) and [other populate](https://docs.google.com/spreadsheets/d/1UPeZX3Xy84ZPkfMYDPAE-305PtAynotZJI7uZdBGjGM/edit#gid=0&range=B10:B11) tests are impacted a lot.', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('Id1GeMpfHlU8BymrznQ0G', '40G7H1-7njAN3flAvj-Vm', 1687965787000.0, '#657 has 2 different approaches. Both have negative impact on the populate performance tests

My thinking is that we should make the nodes immutable because that gives me confidence that things are working correctly.

To alleviate the performance regression we can actually remove the lock in `put` (and `delete`/`clear`). The lock on `put` is there because:

```ts
  /**
   * This rw lock is used to ensure we do not mutate the btree in parallel. It
   * would be a problem if we didn''t have the lock in cases like this:
   *
   * ```ts
   * const p1 = tree.put(''a'', 0);
   * const p2 = tree.put(''b'', 1);
   * await p1;
   * await p2;
   * ```
   *
   * because both `p1` and `p2` would start from the old root hash but a put
   * changes the root hash so the two concurrent puts would lead to only one of
   * them actually working, and it is not deterministic which one would finish
   * last.
   */
```

What we can do instead is that we can detect if the root hash changed and if it did we start over. That approach is already used in `scan`. The case where this gets slower is when you do a lot of parallel `put`s.', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('MVQZKyb14Afddhdn3Px-3', 'sOyiw5y37HpoZt4VhLydX', 1709536236000.0, 'I don''t think we should do this anymore, because we should have first-class search instead.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('7ey7xIRQZg1JDlcTlXYdl', 'xJ10iyYDC5StBYPnkuEyz', 1686860093000.0, 'Below is the code for drawing which Noam shared with me.
What initially jumps out at me is the cost of the calls to validateSchema and concat grow linearly with the size of the drawing.

```
export async function drawLine(tx: WriteTransaction, { id, point }: { id: string; point: Point }): Promise<void> {
  const lastLine = await getDrawing(tx, id);
  if (lastLine) {
    // add point
    lastLine.points = lastLine.points.concat([point.x, point.y]);
    await putDrawing(tx, { id, drawing: lastLine });
  }
}
export async function getDrawing(tx: ReadTransaction, id: string): Promise<Drawing | null> {
  const jv = await tx.get(key(id));
  if (!jv) {
    console.log(Specified shape ${id} not found.);
    return null;
  }
  return validateSchema(drawingSchema, jv);
}
export function putDrawing(tx: WriteTransaction, { id, drawing }: { id: string; drawing: Drawing }): Promise<void> {
  const next = { ...drawing as ReadonlyJSONObject, lastModifiedTimestamp: getUnixTimestampUTC() };
  return tx.put(key(id), next);
}
```', 'grgbkr');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('jM36erTyPwi8K6eljkO2a', 'lx-VPi5fzZ_jTQ5GPxZvb', 1686824634000.0, 'Probably just cargo culture?', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('Adv5glGp9eH2qOjDxdQob', 'lx-VPi5fzZ_jTQ5GPxZvb', 1686840666000.0, 'Removed in https://github.com/rocicorp/mono/pull/613', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('l0FKht-4A-mkXDsVfOpZT', '2eEq-R1RACTLoNgkHPJBb', 1686606438000.0, 'There really needs to be a sad trombone reaction emoji.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('ADB_nC8fmc8x4kBSfz9rL', 'hXkFidPZMhENhrjGCf8mi', 1686564053000.0, 'We have the issue in the unified package.', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('_v6LZGNgH4kN9M5ES59Oy', '8_3okmjon6nIUdLz2E4TX', 1686841266000.0, 'Done with 3ad1befafb7ea041aa25ccbd0ee615eebc022156', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('vOLFCEfJJctMt33gZWpiw', '4Q5SGl4eAHqkV2xCIsjmX', 1685005348000.0, 'This sounds fantastic to me.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('kbrXf_OEIGyPKr_1hTReZ', '4Q5SGl4eAHqkV2xCIsjmX', 1686240532000.0, 'Epic. So excited to try this.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('QkLtSBrLjddNspmcQaAEN', 'KkwyzF_g9DQ3AEMx501MW', 1690882968000.0, 'Strawperson:

* Add notion of special reserved "system" keyspaces (this would also be useful for other theorized features, such as local-only keys)
* The system keyspace starts with `"-/"`. (This is a breaking change but whatevs)
* The initially supported system keyspace is the "presence" keyspace: `"-/p/<client-id>"`.
* The Reflect system maintains two invariants for the presence keyspace:
  1. Client C1 can always access its own presence keys (online, offline, whatever)
  2. Client C2 can only access C1''s presence keys when C1 is connected

In other words, the server only sends C1''s presences keys to C2 when C1 is connected. When C1 is disconnected, the server sends deletes to C2 for C1''s presence keys. But the server always sends C1''s presence keys to C1.

---

Let''s test this strawperson against the goals:

> associate state with clients/users that are connected

Presence state is associated with clients by definition. Per-user state would have to be implemented by app code. Presence state could indicate which user it is for, and then some counter or timestamp could be used to track which client a user is currently at.

We could use this same pattern to provide user presence in the future if necessary.

> automatically delete this state when clients disconnect

Yes. And further, doesn''t delete it locally which is required for cursors to work consistently while disconnected.

> doesn''t get confused by mutation recovery

@grgbkr will have to verify this, but I think we are good.

Mutations to presence state sent by mutation recovery *will write* to presence state for disconnected clients on the server, as normal. However, writes to presence state for disconnected clients won''t be visible to other clients.

> integrates naturally with persisted state

This namespace idea is originally nate white''s, and although using strings in the keys feels a little hacky, it integrates beautifully with all of the existing API.

Also note that this presence state as proposed here *is normal Reflect state*. It gets persisted to IDB as normal, it gets written to durable objects, normally, etc. This means that cross-tab presences while offline will just automatically work.

The deletion of presence state when a client disconnects isn''t a function of the state being ephemeral on the server, it''s a function of a specific delete process that runs when the client disappears.

It is true that presence state often doesn''t need to be written so aggressively, both on client and server, but that can be handled separately...

> don''t bother persisting this state locally

We do persist the state locally. And as above, maybe this is a good thing (so cross-tab presence works).

> don''t bother resending changes related to this state when reconnecting from offline

We would send when reconnecting. However, sending too much unneeded data when reconnecting can be handled separately by #769. Almost all presence mutations would typically be droppable under evenflow, but we still preserve the ability to have non-droppable presence mutations.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('AL44gJ56UfSe3svpWt9wC', 'KkwyzF_g9DQ3AEMx501MW', 1690883205000.0, 'Open question:

Should the server enforce access control to presence? If we do not, it seems like an easy thing for developers to screw up. I believe that we can enforce that after a given clientID is written, it is only mutated by the same userID that originally wrote it.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('4DmTImiFLdBW1joQOWO-w', 'KkwyzF_g9DQ3AEMx501MW', 1690917539000.0, 'Possible additional goal:
- When a client is offline, it should not see presence state of other clients (with possibly the exception of other local tabs in the same profile, i.e. clients in the same client group).   If my client is offline, it cannot have accurate up to date information about the presence of other clients, better to not show any presence info than to show stale/inaccurate presence info.', 'grgbkr');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('ISAvsWd3PGJACRm2u4jS0', 'KkwyzF_g9DQ3AEMx501MW', 1690933559000.0, 'In retrospect I don''t think this works perfectly with DD31, but I''m not sure if the idea is salvageable. ', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('625RM0A9YBez8PcmhEoX6', 'KkwyzF_g9DQ3AEMx501MW', 1691089812000.0, '> Possible additional goal:
> 
> * When a client is offline, it should not see presence state of other clients (with possibly the exception of other local tabs in the same profile, i.e. clients in the same client group).   If my client is offline, it cannot have accurate up to date information about the presence of other clients, better to not show any presence info than to show stale/inaccurate presence info.

I think this makes sense ideally, but it doesn''t seem like a very high priority. There will likely be other UI changes apps want to make to run offline, this just being one. They can detect when they are offline and hide the presence UI if they want to already. I think we should skip for v1 of presence.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('ugALT9gvDZcbKgK5VMuX0', 'KkwyzF_g9DQ3AEMx501MW', 1693246905000.0, '> Open question:
> 
> Should the server enforce access control to presence? If we do not, it seems like an easy thing for developers to screw up. I believe that we can enforce that after a given clientID is written, it is only mutated by the same userID that originally wrote it.

Or even stricter, a mutation can write `-/p/<clientIdX>`, iff the mutation''s clientID is `clientIDX`.', 'grgbkr');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('tLpGAr1op35SZIGi4U6Po', 'KkwyzF_g9DQ3AEMx501MW', 1693247066000.0, '> In retrospect I don''t think this works perfectly with DD31, but I''m not sure if the idea is salvageable.

I really like this proposal and spent a lot of time this weekend thinking about how to salvage it from the complexity of shared client state via client groups... and I''ve got nada.', 'grgbkr');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('3dDrBtqqR5k1anUwWCHGl', 'KkwyzF_g9DQ3AEMx501MW', 1693252371000.0, 'Actually here is an alternative proposal: https://www.notion.so/replicache/Present-Clients-7deb6e93cba0435a82feab0a8bd3bdce', 'grgbkr');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('XT7zPNOKQyNc91EdmwW_r', 'KkwyzF_g9DQ3AEMx501MW', 1693253668000.0, 'lol

On Mon, Aug 28, 2023 at 9:53â€¯AM Greg Baker ***@***.***> wrote:

> Actually here is an alternative proposal:
> https://www.notion.so/replicache/Present-Clients-7deb6e93cba0435a82feab0a8bd3bdce
>
> â€”
> Reply to this email directly, view it on GitHub
> <https://github.com/rocicorp/mono/issues/526#issuecomment-1696306240>, or
> unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AAATUBHPUJ477KENT3XQSNTXXTZJ3ANCNFSM6AAAAAAYKEXGYM>
> .
> You are receiving this because you authored the thread.Message ID:
> ***@***.***>
>
', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('c3l3_82hPlc3lJUfLGPpV', '-pwHLdQfYmU6LhWG0GqBf', 1684480160000.0, '@arv can you do this as part of the connection loop work?', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('YReQkAhXV5znUs-_OCpv7', '-pwHLdQfYmU6LhWG0GqBf', 1685447865000.0, 'There is also the concept of we got disconnected and now we are reconnecting. At the moment we do not call `onOnlineChange` when this happens, we only call it after we have failed once.

If we have a tristate (disconnected, connecting and connected) it would only seem fair that we report the state as connection during a reconnect.

But let''s think about what we actually want to report:

State | ConnectionState | Error Count | What we want to report
-- | -- | -- | --
Startup | disconnected | 0 | online
Connecting | connecting | 0 | previous state
Connected | connected | 0 | online
Auth Error | disconnected, connecting | 0 | previous state
Auth Error, repeated | disconnected, connecting | 1 | offline
Connect timed out | disconnected | 1 | offline
Ping timed out | disconnected | 1 | offline
Tab hidden (with timeout) | disconnected | 0 | offline
Tab shown | connecting | 0 | previous state?

I think we could keep things the way they are with slight tweak. The value of online can be `!tabHidden && errorCount === 0`. We would also "set" this when we set the connectionState to Connecting. That means that when we startup we would be online. When we come back from a hidden tab we will also treat that as being online.

', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('9P2tr1Quvy4sJO94C3aEi', '-pwHLdQfYmU6LhWG0GqBf', 1685448047000.0, 'https://github.com/rocicorp/mono/issues/503

I think if we expose the connecting state, the most honest thing would be to only expose the current `connectingState` but I think that would lead to bad UI.

We could have `onOnlineChange` take 2 arguments, the "online" heuristic as described in the previous comment as well as the `connectedState`.', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('55vPdtUIFW1swkuDHT1Jq', 'Jg_Lc8p-Qo5DJARqYAZa-', 1684041952000.0, 'cc @d-llama @aboodman ', 'grgbkr');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('tjXjK66Dqj9eRIGFYSYTX', 'Jg_Lc8p-Qo5DJARqYAZa-', 1684042213000.0, 'â€œExcept for the testsâ€ is a red flag. We should think critically about what
the tests are really doing for us and be prepared to abandon them where
necessary.

We have metrics, and we have our own site to test on. Also with js itâ€™s
easier to test at higher abstraction levels.

Be bold! Letâ€™s write the code the right way and not let the testing tail
wag the dog.

On Sat, May 13, 2023 at 7:26 PM Greg Baker ***@***.***> wrote:

> cc @d-llama <https://github.com/d-llama> @aboodman
> <https://github.com/aboodman>
>
> â€”
> Reply to this email directly, view it on GitHub
> <https://github.com/rocicorp/mono/issues/505#issuecomment-1546811349>, or
> unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AAATUBFG3EVEH7UFFQ44SSTXGBUGXANCNFSM6AAAAAAYA53LVM>
> .
> You are receiving this because you were mentioned.Message ID:
> ***@***.***>
>
-- 
a (phone)
', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('vVN25xDRh5FbMFUYH1-6N', 'Jg_Lc8p-Qo5DJARqYAZa-', 1684042263000.0, 'Fixing this also feels like something that will pay big dividends in
velocity.

Iâ€™m fine if we have some short bustage in exchange.

On Sat, May 13, 2023 at 7:29 PM Aaron Boodman ***@***.***>
wrote:

> â€œExcept for the testsâ€ is a red flag. We should think critically about
> what the tests are really doing for us and be prepared to abandon them
> where necessary.
>
> We have metrics, and we have our own site to test on. Also with js itâ€™s
> easier to test at higher abstraction levels.
>
> Be bold! Letâ€™s write the code the right way and not let the testing tail
> wag the dog.
>
> On Sat, May 13, 2023 at 7:26 PM Greg Baker ***@***.***>
> wrote:
>
>> cc @d-llama <https://github.com/d-llama> @aboodman
>> <https://github.com/aboodman>
>>
>> â€”
>> Reply to this email directly, view it on GitHub
>> <https://github.com/rocicorp/mono/issues/505#issuecomment-1546811349>,
>> or unsubscribe
>> <https://github.com/notifications/unsubscribe-auth/AAATUBFG3EVEH7UFFQ44SSTXGBUGXANCNFSM6AAAAAAYA53LVM>
>> .
>> You are receiving this because you were mentioned.Message ID:
>> ***@***.***>
>>
> --
> a (phone)
>
-- 
a (phone)
', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('rj71kp6870FalR2vmXnoo', 'mcyn0Mc1lRIh88TtaUxgA', 1685448060000.0, 'Closing in favor of https://github.com/rocicorp/mono/issues/517', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('7KDZSldPW3KwZx0m6jEuK', 'rDkzXqfJWPsX2mLK1iDCU', 1684606504000.0, 'I looked at this, should be quite easy to extract a base class out of the current MetricManager that is shared btwn client and server. Don''t typically like class inheritance but this seems like a nice place for it because the current pattern of having the individual metrics be fields of MetricsManager so that lifetime and access are tied together is nice.

Once this exists, there is a separate question of reporting. Currently everytime the client sends metrics to the server, the server dumbly turns right around and forwards to datadog. This won''t last long (#189). But if we add server metrics in a naive way then we''ll double the number of metrics RPC from our server to datadog instantly.

We should probably fix #189 at same time as this bug and queue up both client and server metrics in the server for awhile before sending.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('EvJ-t57hej3StX92DYxEI', 'rDkzXqfJWPsX2mLK1iDCU', 1684606713000.0, 'Also note: a good place to actually add the code to tickle the metric is by extending `timed` shared utility to optionally take a Gauge as an argument. It is probably common that a thing we wanted to time for logging should also have a metric.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('AKZq45_zILwRGT_8f3t00', 'rDkzXqfJWPsX2mLK1iDCU', 1684960866000.0, '> I looked at this, should be quite easy to extract a base class out of the current MetricManager that is shared btwn client and server. Don''t typically like class inheritance but this seems like a nice place for it because the current pattern of having the individual metrics be fields of MetricsManager so that lifetime and access are tied together is nice.
> 

Is it possible that the server can use the current MetricsManager class out of the box (no subclassing / inheritance) by just supplying an appropriate `MetricsManagerOptions.reporter`? Trying to confirm my reading of the code ...', 'darkgnotic');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('mPbxr2k3e1SqpVDl7s7lG', 'rDkzXqfJWPsX2mLK1iDCU', 1684962336000.0, 'The problem is that the `MetricsManager` class has hard-coded into it specific metrics -- metrics which make sense on the client but not server.

You could reuse the existing class, perhaps by having it contain a union of metrics needed by client and server, but that feels sort of odd to me. I guess there is no major harm to it I can think of.

The idea of subclassing is only to allow the client and server to have distinct set of metrics (and subclassing in particular for no particular reason -- composition would also work).', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('_sNOFynE4ARpVtPcukFGM', 'rDkzXqfJWPsX2mLK1iDCU', 1684966074000.0, 'Okay, I see the calls to `this._register()`. So we would need abstract that out of the class and either configure them via inheritance (baked into the class) or by composition (options). Got it. ', 'darkgnotic');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('ruF99etQa2j7My6YYw3wH', 'rDkzXqfJWPsX2mLK1iDCU', 1684976985000.0, '> Also note: a good place to actually add the code to tickle the metric is by extending `timed` shared utility to optionally take a Gauge as an argument. It is probably common that a thing we wanted to time for logging should also have a metric.

Next question: I see that we use a Gauge for recording connection times. While it at first seemed odd to me, I understand now that we''re trying to track two things at the same time: (1) the number of open connections along with (2) the time they took to connect.

However, for timing of short-lived events like a LogFunction or auth_time, I was thinking that it makes more sense as a Distribution, at least according to the [DD docs](https://docs.datadoghq.com/metrics/#metric-types-and-real-time-metrics-visibility). Does that make sense or am I thinking about this the wrong way? (I recall rpc timing metrics at our previous companies being distributions.)', 'darkgnotic');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('8sEvj6BW2L5hbkEjaBoLL', 'rDkzXqfJWPsX2mLK1iDCU', 1684983407000.0, 'Here are some things that Fritz wrote on this subject:

https://www.notion.so/replicache/metrics-0b848461ffce4312bdb645d65adb7da2

https://github.com/rocicorp/mono/issues/186

I think the decision to use a gauge was driven by what datadog has api support for. But also an interesting consideration is we want to be super careful to not overcount in situations like retry loops. That is why we have this setup where we store the _last value_ for some measure and report that every time period. This makes a lot of sense to me and gives me confidence in what we are seeing for the short time we''ve had metrics but I am a complete novice here.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('gZy0oK2JvuOmTuqJJ_jyW', 'rDkzXqfJWPsX2mLK1iDCU', 1685039985000.0, 'Got it. This is great context. Thank you!', 'darkgnotic');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('yl_Zq6lFWoJ53kMEEzyqO', 'Dj6LEOA9zuXzoV6c9klaa', 1683652936000.0, 'In a previous project we used Firebase [custom claims](https://firebase.google.com/docs/auth/admin/custom-claims) to attach user-level roles (such as `admin` and `readonly-admin`) to privileged accounts for debugging user issues. These claims are accessible (after authentication) both from the client-side and the server-side, as they are encoded in the user''s JWT.

As such, it would be elegant to have the application pass the same user `context` object (containing `userID`, `claims`, and whatever else the application desires to make its authorization decisions) into the initialization of the client, and from the auth response of the `authenticateAndAuthorize` callback in the worker. Then the mutator logic can be the same on the client and server side (the former of which assumes what behavior is allowed, and the latter of which does the actual enforcement). In this way replicache / reflect provide a conduit for arbitrary information from the application''s auth code to its mutation code, in both the client and server environments.

Is that the general idea?', 'darkgnotic');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('LujMgExvOR01S26xSFa1-', 'Dj6LEOA9zuXzoV6c9klaa', 1683656219000.0, 'Yes! We discussed enabling customers to pass this `Context` into the Reflect constructor to enable the mutator logic to be "isomorphic". I go back and forth on whether this is a net win for dx or features or not, I think we''d have to try it.

On the one side of course it sounds good to let the mutators do the same thing both client side and server side.

OTOH, it''s trivial to write:

```ts
async function fooButOnlyIfAdmin(tx: WriteTransaction) {
  if (tx.context?.isAdmin) {
    await foo();
  }
}
```

The nice thing about Reflect is because it has authoritative server the client doesn''t _need_ to do the same thing as the server. If this is an edge case that should not be triggered except by malicious users, then it doesn''t matter what the ux is.

I think we should start by just exposing context on the server-side and see how it feels to write authenticated code.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('Z1rZ2XPpuZQxT8Ns5qzSf', 'Dj6LEOA9zuXzoV6c9klaa', 1683657737000.0, 'Cool. I''m still reading up on docs and haven''t gotten to actual client code or deployment examples, but I assume that a customer will need to separately (1) deploy their mutators into Cloudflare (whether that be onprem or managed by us) and (2) pass their mutators into a Reflect client. So what I hear you saying is that, while being able to use the same mutator code in both places is convenient, it is not necessary because there are separate management paths for code running in the client and code running in the server. ', 'darkgnotic');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('FZLBeFSr3NVDM-Nl_U2VU', 'Dj6LEOA9zuXzoV6c9klaa', 1683658224000.0, 'They do need to pass their mutators to both the client and server. But what I''m saying is different: since the Replicache/Reflect is an authoritative server system, the mutators are allowed to do something different on the server. You can have a mutator that access to additional information on the server (ie auth info) and it simply computes a different answer than it did on the client. This is a feature. We don''t need to bend over backwards to have the mutators always compute the exact same thing on the client, they don''t have to be pure.

This might actually help assimilate the core of the sync protocol: https://doc.replicache.dev/concepts/how-it-works#the-replicache-sync-model. It discusses some of these ideas in more detail.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('kxAtnkn3-nqxMfDMjWyvP', 'Dj6LEOA9zuXzoV6c9klaa', 1683681342000.0, 'I do appreciate the fact that Replicache/Reflect provide an authoritative server system, and that developers have the option to do something different on the server than on the client.

I also think, though, that part of the elegance of the Replicache/Reflect design, with mutators run in both environments, is that developers _can_ write their code without thinking about where it will be run. The dx win, to me, is an isomorphic API for server-run and client-run mutators (but certainly not requiring isomorphic implementations).

Just one dev''s opinion though.  ðŸ˜„ 
', 'darkgnotic');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('RytEBdxKqU7M3P0woiS0u', 'Dj6LEOA9zuXzoV6c9klaa', 1683685278000.0, 'Thanks a lot for the feedback. Let''s just try it! I''ve learned *not* to trust my instincts on this kind of thing... I am often surprised how bad my guesses are, when just using it makes it clear what feels right and doesn''t.

I think that providing `.context` only on the server-side is strictly less work than providing it on client and server. We can write some code that requires authorization in samples and I bet we will pretty quickly realize if it is not working.

WDYT?', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('72fBYpJ8EzewGGKgIx1UU', 'Dj6LEOA9zuXzoV6c9klaa', 1683693171000.0, 'For sure! Sorry, I was just weighing in on what your were "[going back and forth on](https://github.com/rocicorp/mono/issues/492#issuecomment-1540653342)" and not trying to imply that we _must_ do it one way or the other. Certainly, the server-side functionality is the only prerequisite for the desired functionality. And after learning more about DD31, I can imagine that adding a Context to the Reflect constructor could complicate client grouping (e.g. what do we do for clients with the same "name" but different Contexts?).', 'darkgnotic');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('KP7l4wT3TrnMZwphdpALQ', 'Dj6LEOA9zuXzoV6c9klaa', 1686093873000.0, 'API proposal:

```ts
interface WriteTransaction {
  // ...
  readonly auth?: AuthData|undefined;
  // ...
};

type AuthData = {
  readonly userID: string;
} & ReadonlyJSONObject;
```', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('RtqDvrzL6_-8zd3ii2DnS', '7rq_RVRKkIXMkuWfAUgQo', 1683231143000.0, 'The real error is: 

"Closing socket with error, {kind=VersionNotSupported, message=unsupported version}"

The "accepting connecting to send error" are completely explained by this.  Both have the same number of occurrences and occur in pairs (in the code we expect one to be logged immediately after the other).

One issue is:
"accepting connection to send error" is logged at level error, while "Closing socket with error, {kind=VersionNotSupported, message=unsupported version}" is logged at info.  Both should be logged at info.', 'grgbkr');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('o1SghFArqMM-I_IZrhFAJ', '7rq_RVRKkIXMkuWfAUgQo', 1683231879000.0, 'I recall monday had a spike and slow fade out of these VersionNotSupported errors last time they updated versions as well.  ', 'grgbkr');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('v1M_JT5ay3ox5zQDdIOj4', '7rq_RVRKkIXMkuWfAUgQo', 1683238312000.0, 'I do not believe this is a real problem.  Monday has said they only soft users to update when there is a version mismatch.  I expect these errors to subside as users refresh their pages.', 'grgbkr');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('_Scz2bQZRYtgIDWvpdyj1', '7rq_RVRKkIXMkuWfAUgQo', 1683579930000.0, 'Closing this LMK if you disagree @grgbkr ', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('mSuwMr_Lz679UwNo38-iu', 'AkjVslOD_m3m-18wS4cuE', 1683246659000.0, 'Most are logged by the Worker, fewer are logged by the AuthDO, and very few are logged by the RoomDO.

<img width="1408" alt="image" src="https://user-images.githubusercontent.com/19158916/236356847-25cd385c-6b37-4687-919d-edcbdd014ef0.png">
', 'grgbkr');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('rRkxglLBRnVSE_u76H8VO', 'eXY6YjHp1QVbQ00wEzGf_', 1682920872000.0, '#454 caused this regression. It was reverted in https://github.com/rocicorp/mono/pull/480 as a temporary fix.

We still need to re-land 454.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('02wvRmLvJF4lJ68vgTZsB', 'HUUC4Hxvx78QSMFt13C21', 1684744555000.0, '@grgbkr I vaguely remember we consciously decided to leave this for some reason. Do you remember the details?', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('SpY4a5kLrNOLnuc8uqRGe', 'tlbJPIoPQUVH08OyCz1r6', 1681172301000.0, 'Moving this to a checkbox on existing bug (#350 )', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('btFJ5O0qONCj0p5vierH3', 'bcHDVOcyyXBwF2PQg8kmp', 1680638935000.0, 'And if we decide to soften offline, then #367 can go back to beta.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('EUjBckDfYOx91yGgd4Z-a', 'Gq69iFiLUO9DqZ0LEMug8', 1681462993000.0, 'IIRC, the code supports this behavior but the TS types do not.', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('hyOWKPuT7es9ErlgaL7WN', '1A3uLjcAPJnZChekrNVXC', 1680568206000.0, 'Note that this really wants `warn` to exist on our logging package. There have been a few cases of this. We should add it.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('rgor1WLiwgU597D0yFV80', '1A3uLjcAPJnZChekrNVXC', 1681462932000.0, 'I''m not sure why we cannot throw in the case of nested transaction? Maybe the problem is to detect that we are nesting them in an async context?', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('7PAJm3Khu-neYcF_8o4fK', '1A3uLjcAPJnZChekrNVXC', 1681487381000.0, 'We want it to be possible to open overlapping transactions. Like if a user click rapidly and each click fires a mutation, since they are async, this can easily lead to overlapped writes. What we don''t want is transactions to be waiting on each other in a cycle. But we have no way to know which tx are waiting on which AFAICT.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('lRdcvSz-knkiVVqgh0wyX', '1A3uLjcAPJnZChekrNVXC', 1681739913000.0, 'I think you are missing my point. We would like to prevent a transaction from trying to open another transaction. More specifically a read transaction cannot open a write transaction and a write transaction cannot open another read or write transaction.

The thing that makes this hard to detect is that transactions are async. Potentially we can use a custom PromiseLike and wrap the then resolve/reject callbacks with a context but it requires some research.', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('RQFjEJ8MBHL2wcFmgq_ny', '1A3uLjcAPJnZChekrNVXC', 1681758688000.0, 'Ah, I think we''re saying the same thing in different words. I agree with your wording of what we are trying to prevent.

I did not even consider that it was possible to engineer something to prevent this using promises. But I guess that it should actually be since at the end of the day this is a promise chain and we can restructure to say that what we are trying to prevent is a chains like `{(waiting on a write tx from rep 1) ... (waiting on another write tx from rep 1)}`.

Neat idea.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('1zVP4yu4Ec270oIOW1KWa', '_rW2bAe2HIoIcPhGRuyOL', 1680568034000.0, 'This turned out to be a misunderstanding of how to use the API. Turning this bug into a doc bug: #456.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('sufTG_sN2BHlm09xd8qGT', 'XlJCP3zurxQelxQz0MLr7', 1680336047000.0, 'cc @arv - can you please review this and tell me which parts are right and wrong?', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('qe0RyRUIV9Bde4gSbvwVO', 'XlJCP3zurxQelxQz0MLr7', 1680343340000.0, 'ISSUE 1 is not correct. I forgot that it''s the client that measures ping timeouts not the server.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('nJbs1C-YJLmLR-buxF8nv', 'XlJCP3zurxQelxQz0MLr7', 1680510756000.0, 'ISSUE 4 is also not correct as `visibilityWatcher` actually keeps watching while the ping is outstanding. The next time you wait on it, it resolves immediately if already in that state.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('ZjvNN75e5HVlHMvqZ435s', 'XlJCP3zurxQelxQz0MLr7', 1680510787000.0, 'https://github.com/rocicorp/mono/pull/454 demonstrates ISSUE 2 and 3 and also that 4 is *not* present.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('VQKE6ssGlCKQ7pavr3Yu9', 'XlJCP3zurxQelxQz0MLr7', 1680683818000.0, 'The goal of `#nextMessageResolver` was to abort the ping timeout when we get a valid message. In other words, no need to send pings when we just received a message. Changing this to use an [abort signal on the `sleep`](https://github.com/rocicorp/mono/blob/ef6f15feae567541267df8ae4ad3109c98f9fa88/packages/replicache/src/sleep.ts#L10) function might make more sense.

ISSUE 3: You are right that we are not dealing with invalid/error messages after sending ping, waiting for pong. I don''t have a suggestion at this point.

ISSUE 2: Agreed. We do not disconnect on invalid messages and unexpected exceptions but we incorrectly increment the error count and set online to false. Ignoring these errors seems better.', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('X9thHpjiyCEtqrXc9r3mV', 'UpKiMRI46OhfsrBm1hSXv', 1680341644000.0, 'Fixed by #451 ', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('nKL9z7Q1WojkRNGfa_KqZ', 'iEvVSbWm1EYBpeeEeVX0p', 1680568370000.0, '@arv can you flesh this out a little more it''s not clear what this bug is about.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('pl7sK_adBUiIXLqcPZ0Yn', 'iEvVSbWm1EYBpeeEeVX0p', 1680598126000.0, 'This is all related to refresh being broken.

#30 #434 ', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('OuxkPbyBGS7RLHsA0MTii', 'yz1Z9BASQqi0BW2ZxaEMv', 1679821705000.0, 'Erik can you take this - you can see how to add tags to metrics in #440. We should do a request to some `/canary` http endpoint that the server exposes concurrent with connection, and include its status in the metrics tags (plus if it errors, log the result at error level or success at debug level).', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('ejobgX6cm_jISDQK-xnVj', 'yz1Z9BASQqi0BW2ZxaEMv', 1681148214000.0, '@aboodman do we want the worker to answer the canary request or do we want it to route to the roomDO ?  If to the roomDO should the canary request do implicit room creation? If we allow them to do implicit room creation then they need to be authenticated, right?', 'cesara');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('lXmCXnWmQ-wVZa9XnSB-g', 'yz1Z9BASQqi0BW2ZxaEMv', 1681167754000.0, 'The purpose of the canary request is to compare http connectivity to socket connectivity. Again, the concrete case we had was one where the ssl certificate wasn''t configured properly and the http request had a clear error. So I think just handling the request by the worker is fine.

Please log the result of the request either way (at debug level).', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('hTfLXrTXPnrOxxjXf5MZt', '64kYNhcr9MRaLzLWAWwRY', 1679611909000.0, 'cc @grgbkr -- I checked this out with Jesse. For some reason with current trunk builds, and only on --local mode, mutations stay pending forever. The server never decides to run them.

If you reboot the server then it *does* find the mutations and run them.

This doesn''t happen with current npm build, nor does it happen in trunk builds without --local.

Smells very much related to clock changes to me. Updated wrangler but didn''t help. We tried running --experimental-local, but it doens''t seem to be working at all right now (server crashes at startup with some npm inssue).

We will have to figure out something for alpha because we just decided to recommend people use --local so we don''t have to fix https://github.com/rocicorp/mono/issues/388#issuecomment-1476942996.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('XwqQibo0_24Cwx9nKzhv1', '64kYNhcr9MRaLzLWAWwRY', 1679613807000.0, 'Ok, here''s the repro branch: https://github.com/rocicorp/reflect-todo/tree/mono-issue-436-repro

To reproduce, pull this branch then:

- `cp .env.example .env` (if you have no `.env`)
- `cp .dev.vars.example .dev.vars` (if you have no `.dev.vars`)
- `npm i`
- `npm run dev-worker`

Then in a separate console

- `./create-room.sh`
- `npm run dev`
- open http://localhost:5173/

Note that you can create todos, and they don''t appear until the server confirms them ([this is the commit that forces server confirmation](https://github.com/rocicorp/reflect-todo/commit/e565f700cfb310ea97eae440b82d88b298e1ae0f)).
Now to reproduce the issue, stop both consoles, then

- `npm run dev-worker-local`

and in a new console,

- `./create-room.sh`
- `npm run dev`

Now open http://localhost:5173/ and see that when you try to add a TODO it isn''t created.

Observe that the mutator is sent to the server, it just doesn''t run it. 

<details>
  <summary>Sample server logs from one such run</summary>

```
handling message ["push",{"timestamp":5971.400000095367,"clientGroupID":"a2c67419-a064-4a4e-95cd-d1d3a6531d8a","mutations":[{"timestamp":5969,"id":2,"clientID":"355e72ec-d18b-4bd1-a0aa-77fec40bd345","name":"createTodo","args":{"id":"TqfJ6NK11kL8JPVaoTP_S","text":"test","completed":false}}],"pushVersion":1,"schemaVersion":"","requestID":"355e72ec-d18b-4bd1-a0aa-77fec40bd345-ceabd626-0"}] waiting for lock
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=66ej5mmxtra received lock
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=66ej5mmxtra msgType=push requestID=355e72ec-d18b-4bd1-a0aa-77fec40bd345-ceabd626-0 handling push {"timestamp":5971.400000095367,"clientGroupID":"a2c67419-a064-4a4e-95cd-d1d3a6531d8a","mutations":[{"timestamp":5969,"id":2,"clientID":"355e72ec-d18b-4bd1-a0aa-77fec40bd345","name":"createTodo","args":{"id":"TqfJ6NK11kL8JPVaoTP_S","text":"test","completed":false}}],"pushVersion":1,"schemaVersion":"","requestID":"355e72ec-d18b-4bd1-a0aa-77fec40bd345-ceabd626-0"}
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=66ej5mmxtra msgType=push requestID=355e72ec-d18b-4bd1-a0aa-77fec40bd345-ceabd626-0 inserted 1 mutations, now there are 2 pending mutations.
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=66ej5mmxtra handling processUntilDone
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=66ej5mmxtra already processing, nothing to do
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw processNext - starting turn at 1679613135160 - waiting for lock
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw received lock at 1679613135160
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw process pending
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw processing 0 of 2 pending mutations with 0 forced misses
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw processing room clientIDs [[0,"355e72ec-d18b-4bd1-a0aa-77fec40bd345"]]  pendingMutations []
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw currentVersion 10
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw pokes from fastforward []
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw processing frame - clients ["355e72ec-d18b-4bd1-a0aa-77fec40bd345"]
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw prevVersion 10 nextVersion 11
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw processed 0 mutations
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw Checking for disconnected clients. Currently connected ["355e72ec-d18b-4bd1-a0aa-77fec40bd345"] Stored connected ["355e72ec-d18b-4bd1-a0aa-77fec40bd345"]
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw built pokes 0
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw clearing pending mutations
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw processPending took 0 ms
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw processNext - starting turn at 1679613135160 - waiting for lock
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw received lock at 1679613135160
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw process pending
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw processing 0 of 2 pending mutations with 0 forced misses
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw processing room clientIDs [[0,"355e72ec-d18b-4bd1-a0aa-77fec40bd345"]]  pendingMutations []
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw currentVersion 10
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw pokes from fastforward []
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw processing frame - clients ["355e72ec-d18b-4bd1-a0aa-77fec40bd345"]
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw prevVersion 10 nextVersion 11
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw processed 0 mutations
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw Checking for disconnected clients. Currently connected ["355e72ec-d18b-4bd1-a0aa-77fec40bd345"] Stored connected ["355e72ec-d18b-4bd1-a0aa-77fec40bd345"]
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw built pokes 0
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw clearing pending mutations
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw processPending took 0 ms
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw processNext - starting turn at 1679613135160 - waiting for lock
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw received lock at 1679613135160
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw process pending
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw processing 0 of 2 pending mutations with 0 forced misses
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw processing room clientIDs [[0,"355e72ec-d18b-4bd1-a0aa-77fec40bd345"]]  pendingMutations []
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw currentVersion 10
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw pokes from fastforward []
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw processing frame - clients ["355e72ec-d18b-4bd1-a0aa-77fec40bd345"]
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw prevVersion 10 nextVersion 11
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw processed 0 mutations
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw Checking for disconnected clients. Currently connected ["355e72ec-d18b-4bd1-a0aa-77fec40bd345"] Stored connected ["355e72ec-d18b-4bd1-a0aa-77fec40bd345"]
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw built pokes 0
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw clearing pending mutations
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw processPending took 0 ms
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw processNext - starting turn at 1679613135160 - waiting for lock
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw received lock at 1679613135160
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw process pending
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw processing 0 of 2 pending mutations with 0 forced misses
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw processing room clientIDs [[0,"355e72ec-d18b-4bd1-a0aa-77fec40bd345"]]  pendingMutations []
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw currentVersion 10
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw pokes from fastforward []
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw processing frame - clients ["355e72ec-d18b-4bd1-a0aa-77fec40bd345"]
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw prevVersion 10 nextVersion 11
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw processed 0 mutations
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw Checking for disconnected clients. Currently connected ["355e72ec-d18b-4bd1-a0aa-77fec40bd345"] Stored connected ["355e72ec-d18b-4bd1-a0aa-77fec40bd345"]
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw built pokes 0
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw clearing pending mutations
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw processPending took 0 ms
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw processNext - starting turn at 1679613135160 - waiting for lock
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw received lock at 1679613135160
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw process pending
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw processing 0 of 2 pending mutations with 0 forced misses
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw processing room clientIDs [[0,"355e72ec-d18b-4bd1-a0aa-77fec40bd345"]]  pendingMutations []
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw currentVersion 10
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw pokes from fastforward []
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw processing frame - clients ["355e72ec-d18b-4bd1-a0aa-77fec40bd345"]
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw prevVersion 10 nextVersion 11
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw processed 0 mutations
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw Checking for disconnected clients. Currently connected ["355e72ec-d18b-4bd1-a0aa-77fec40bd345"] Stored connected ["355e72ec-d18b-4bd1-a0aa-77fec40bd345"]
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw built pokes 0
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw clearing pending mutations
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw processPending took 0 ms
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw processNext - starting turn at 1679613135160 - waiting for lock
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw received lock at 1679613135160
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw process pending
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw processing 0 of 2 pending mutations with 0 forced misses
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw processing room clientIDs [[0,"355e72ec-d18b-4bd1-a0aa-77fec40bd345"]]  pendingMutations []
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw currentVersion 10
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw pokes from fastforward []
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw processing frame - clients ["355e72ec-d18b-4bd1-a0aa-77fec40bd345"]
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw prevVersion 10 nextVersion 11
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw processed 0 mutations
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw Checking for disconnected clients. Currently connected ["355e72ec-d18b-4bd1-a0aa-77fec40bd345"] Stored connected ["355e72ec-d18b-4bd1-a0aa-77fec40bd345"]
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw built pokes 0
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw clearing pending mutations
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw processPending took 0 ms
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw processNext - starting turn at 1679613135160 - waiting for lock
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw received lock at 1679613135160
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw process pending
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw processing 0 of 2 pending mutations with 0 forced misses
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw processing room clientIDs [[0,"355e72ec-d18b-4bd1-a0aa-77fec40bd345"]]  pendingMutations []
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw currentVersion 10
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw pokes from fastforward []
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw processing frame - clients ["355e72ec-d18b-4bd1-a0aa-77fec40bd345"]
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw prevVersion 10 nextVersion 11
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw processed 0 mutations
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw Checking for disconnected clients. Currently connected ["355e72ec-d18b-4bd1-a0aa-77fec40bd345"] Stored connected ["355e72ec-d18b-4bd1-a0aa-77fec40bd345"]
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw built pokes 0
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw clearing pending mutations
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw processPending took 0 ms
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw processNext - starting turn at 1679613135160 - waiting for lock
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw received lock at 1679613135160
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw process pending
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw processing 0 of 2 pending mutations with 0 forced misses
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw processing room clientIDs [[0,"355e72ec-d18b-4bd1-a0aa-77fec40bd345"]]  pendingMutations []
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw currentVersion 10
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw pokes from fastforward []
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw processing frame - clients ["355e72ec-d18b-4bd1-a0aa-77fec40bd345"]
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw prevVersion 10 nextVersion 11
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw processed 0 mutations
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw Checking for disconnected clients. Currently connected ["355e72ec-d18b-4bd1-a0aa-77fec40bd345"] Stored connected ["355e72ec-d18b-4bd1-a0aa-77fec40bd345"]
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw built pokes 0
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw clearing pending mutations
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw processPending took 0 ms
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw processNext - starting turn at 1679613135160 - waiting for lock
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw received lock at 1679613135160
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw process pending
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw processing 0 of 2 pending mutations with 0 forced misses
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw processing room clientIDs [[0,"355e72ec-d18b-4bd1-a0aa-77fec40bd345"]]  pendingMutations []
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw currentVersion 10
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw pokes from fastforward []
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw processing frame - clients ["355e72ec-d18b-4bd1-a0aa-77fec40bd345"]
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw prevVersion 10 nextVersion 11
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw processed 0 mutations
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw Checking for disconnected clients. Currently connected ["355e72ec-d18b-4bd1-a0aa-77fec40bd345"] Stored connected ["355e72ec-d18b-4bd1-a0aa-77fec40bd345"]
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw built pokes 0
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw clearing pending mutations
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw processPending took 0 ms
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw processNext - starting turn at 1679613135160 - waiting for lock
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw received lock at 1679613135160
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw process pending
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw processing 0 of 2 pending mutations with 0 forced misses
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw processing room clientIDs [[0,"355e72ec-d18b-4bd1-a0aa-77fec40bd345"]]  pendingMutations []
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw currentVersion 10
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw pokes from fastforward []
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw processing frame - clients ["355e72ec-d18b-4bd1-a0aa-77fec40bd345"]
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw prevVersion 10 nextVersion 11
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw processed 0 mutations
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw Checking for disconnected clients. Currently connected ["355e72ec-d18b-4bd1-a0aa-77fec40bd345"] Stored connected ["355e72ec-d18b-4bd1-a0aa-77fec40bd345"]
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw built pokes 0
RoomDO doID=000000018710ae9d518d94b5482913caf0f2f5fe4efc5aeaf1d06b1b0e4d0cda roomID=room-id requestID=ami2kf3dej clientIP=127.0.0.1 wsid=ii0oe6T_F5rN9e2BaWOIc client=355e72ec-d18b-4bd1-a0aa-77fec40bd345 wsid=ii0oe6T_F5rN9e2BaWOIc msg=hsobsokdgiw clearing pending mutations
```
</details>
', 'jesseditson');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('lF3o9j6byZ5UMamOmaBMx', '64kYNhcr9MRaLzLWAWwRY', 1679616473000.0, 'From those logs it is clear that the clock is frozen (all the timestamps are 1679613135160).   I''m not sure we can work around this.', 'grgbkr');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('5xQ_o04UKrgkpWLQvvRnw', '64kYNhcr9MRaLzLWAWwRY', 1679619025000.0, 'I think it may be more feasible to address: #388   We need to do #388 if we do https://github.com/rocicorp/mono/issues/178, because then clients can end up ahead of the server in production.', 'grgbkr');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('-yDckhvBDrQUYoGSc6XvN', '64kYNhcr9MRaLzLWAWwRY', 1679624776000.0, 'Another option is to get --experimental-local working. It''s hard to believe they''re just shipping it completely broken, we must be missing something.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('zqNv2NhsnM0GYBqcDk_8A', '64kYNhcr9MRaLzLWAWwRY', 1679677136000.0, 'I was able to get past the npm error with --experimental-local by clearing my npm cache.  However, then I hit an error that persisted DOs are not supported.


```
greg replidraw-do [grgbkr/dd31-60fps]$ npm cache clean --force
npm WARN using --force Recommended protections disabled.
greg replidraw-do [grgbkr/dd31-60fps]$ wrangler dev --experimental-local
 â›…ï¸ wrangler 2.9.1 (update available 2.13.0)
------------------------------------------------------
Your worker has access to the following bindings:
- Durable Objects:
  - roomDO: RoomDO
  - authDO: AuthDO
[NPXI] @miniflare/tre not available locally. Attempting to use npx to install temporarily.
[NPXI] Installing... (npx --prefer-offline -y -p @miniflare/tre@3.0.0-next.8)
[NPXI] Installed into /Users/greg/.npm/_npx/f763b2efd540e32a/node_modules.
[NPXI] To skip this step in future, run: npm install --save-dev @miniflare/tre@3.0.0-next.8
âœ˜ [ERROR] local worker: DurableObjectsError [ERR_PERSIST_UNSUPPORTED]: Persisted Durable Objects are not yet supported

      at Object.getServices
  (/Users/greg/.npm/_npx/f763b2efd540e32a/node_modules/@miniflare/tre/dist/src/index.js:5289:13)
      at #assembleConfig
  (/Users/greg/.npm/_npx/f763b2efd540e32a/node_modules/@miniflare/tre/dist/src/index.js:6221:45)
      at async #init
  (/Users/greg/.npm/_npx/f763b2efd540e32a/node_modules/@miniflare/tre/dist/src/index.js:6070:20)
      at async Mutex.runWith
  (/Users/greg/.npm/_npx/f763b2efd540e32a/node_modules/@miniflare/tre/dist/src/index.js:2296:16)
      at async startLocalWorker
  (/Users/greg/github/replidraw-do/node_modules/wrangler/wrangler-dist/cli.js:124794:11) {
    code: ''ERR_PERSIST_UNSUPPORTED'',
    cause: undefined
  }
```', 'grgbkr');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('tIpKF30gGljyoC6-dX_Oo', '64kYNhcr9MRaLzLWAWwRY', 1679683211000.0, 'FWIW, I''ve also been seeing (and I believe it may have been like this since I started on this project) that `Script modified, context reset` will run when I change files that are not dependencies of `worker/index.ts` - this means that when I change any frontend file, the reflect db will be wiped, and baseCookie will be unrecognized.

This was less of a blocker when doing pure FE iteration, but when working on bots, it makes it pretty rough. I''ve also been seeing exceptions on reconnect, so most of the time even reloading will wipe the db locally.

I''ve seen some other reports of this (https://community.cloudflare.com/t/script-modified-context-reset-during-developement/384304) and believe it to be on cloudflare''s side, and I haven''t put any time into cleanly reproducing the reload crash, so I don''t think anything here is directly actionable, just mentioning it so we all know what the DX is at the moment.

My workaround for now is to just use the reflect libs from npm in dev, and use the tarballs in prod, since things are ok there, so I''m not blocked.', 'jesseditson');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('ry5eZGE6dfeV30VfIX4Rd', '64kYNhcr9MRaLzLWAWwRY', 1679684568000.0, '> However, then I hit an error that persisted DOs are not supported.

Ah I should have guessed this. I looked into the open source impl of the worker platform and it also doesn''t support persistent DOs. So this makes sense. So that path is dead for now.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('xRnw1obTsp5td48owHnKf', '64kYNhcr9MRaLzLWAWwRY', 1680164278000.0, 'Since we decided not to do anything here, closing this.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('CH8vqa2rhNbpUFS0Iio-5', 'swvXBTdDXne3CB_d7L1zi', 1680558408000.0, 'We have some issues related to splitting the persist and refresh implementaions over multiple transactions.

Both refresh and persist has some issues in case of transactions failing. Since we are splitting the logic over multiple transactions a rollback on failure does not work and we end up in invalid state.

Refresh:
- perdag write
- memdag write
- perdag write

Persist should be safe because it does:
- perdag read
- memdag read
- memdag write (in one of the two branches)
- perdag write

This is not what we are seeing in the reproduced test case.



', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('LINIbw5Eyj6VCw_1zZBRK', 'swvXBTdDXne3CB_d7L1zi', 1680558810000.0, 'What we are seeing is that we have interleaved persist/refresh.

https://www.notion.so/replicache/ChunkNotFound-Repro-ddeb6e1db3684c59bfd3d2163cb3eeff#dc4b1b3bcc614a268eeb42d178c18340', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('s3FU4VJ_OftTHedWKCT4X', 'swvXBTdDXne3CB_d7L1zi', 1680728025000.0, 'One thing I thought would work was to wrap persist and refresh in a exclusive lock. But even with that we get:

```
4713.js:280245  name=reflect-anon-jTxPHFi6N1IgssS7AAw7k Error during refresh from storage ChunkNotFoundError: Chunk not found 82120de598a74cb083e8a1a354b75df9000000006595
    at mustGetChunk (5fbb21d5-abd1abff75732ec5.js:4187:9)
    at async GatherNotCachedVisitor.visitBTreeNode (5fbb21d5-abd1abff75732ec5.js:4273:19)
    at async Promise.all (:3000/index 27)
    at async GatherNotCachedVisitor._visitBTreeInternalNode (5fbb21d5-abd1abff75732ec5.js:4286:5)
    at async GatherNotCachedVisitor.visitBTreeNodeChunk (5fbb21d5-abd1abff75732ec5.js:4281:7)
    at async GatherNotCachedVisitor.visitBTreeNode (5fbb21d5-abd1abff75732ec5.js:4276:5)
    at async GatherNotCachedVisitor.visitCommitChunk (5fbb21d5-abd1abff75732ec5.js:4213:5)
    at async GatherNotCachedVisitor.visitCommit (5fbb21d5-abd1abff75732ec5.js:4209:5)
    at async 5fbb21d5-abd1abff75732ec5.js:6415:9
    at async using (5fbb21d5-abd1abff75732ec5.js:3162:12)
log @ 326-ae18cf0b689d4713.js:2
```

```
4713.js:280245  name=reflect-anon-jTxPHFi6N1IgssS7AAw7k Error during persist ChunkNotFoundError: Chunk not found 82120de598a74cb083e8a1a354b75df9000000003635
    at mustGetChunk (5fbb21d5-abd1abff75732ec5.js:4187:9)
    at async BTreeWrite.getNode (5fbb21d5-abd1abff75732ec5.js:2334:22)
    at async InternalNodeImpl.set (5fbb21d5-abd1abff75732ec5.js:1979:26)
    at async 5fbb21d5-abd1abff75732ec5.js:2629:24
    at async run (326-ae18cf0b689d4713.js:280178:16)
    at async Write.put (5fbb21d5-abd1abff75732ec5.js:3845:5)
    at async WriteTransactionImpl.put (5fbb21d5-abd1abff75732ec5.js:1432:5)
    at async addSplatter (index-8d0d173e8edba1d6.js:2708:9)
    at async rebaseMutation (5fbb21d5-abd1abff75732ec5.js:4340:3)
    at async rebaseMutationAndPutCommit (5fbb21d5-abd1abff75732ec5.js:4344:14)
log @ 326-ae18cf0b689d4713.js:2
```

One take away from this still failing when we put a single lock around them is that it is not the interleaving of persists/refreshes that causes the trouble.

It could still be the interleaving of the memdag with mutations and pull...

', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('zv07GerjsnvmFyhdToKqQ', 'EAZ1GpjTKqMcMDuvn_Tcb', 1683333564000.0, 'We''re now deciding to leave this in case subset wants it, until we completely fix every last correctness issue.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('AN9hXGaZGD34KbXxWjEg7', 's3Pnkr55mei8sd-aBlx-u', 1679430593000.0, 'Is this for replidraw-do? I thought I tried that one already.', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('ZK4mqUx4GNSqWhhiLIkFR', 's3Pnkr55mei8sd-aBlx-u', 1679430920000.0, 'yeah replidraw-do, maybe i need to rebase.

On Tue, Mar 21, 2023 at 1:30â€¯PM Erik Arvidsson ***@***.***>
wrote:

> Is this for replidraw-do? I thought I tried that one already.
>
> â€”
> Reply to this email directly, view it on GitHub
> <https://github.com/rocicorp/mono/issues/427#issuecomment-1478540004>, or
> unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AESFPBHES3Q76DHHPLMXYBLW5IFUZANCNFSM6AAAAAAWC5EV6U>
> .
> You are receiving this because you authored the thread.Message ID:
> ***@***.***>
>
', 'grgbkr');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('SlnAd9zvHyUqeNPIt2i_k', 's3Pnkr55mei8sd-aBlx-u', 1679490347000.0, 'I was able to reproduce this using `npm run build`. There is some webpack bug triggering this.
- Changing from swc to babel did not help
- Changing esbuild for reflect to not minimize helped/
- Changing esbuild to treat `@badrap/valita` helped.

## Plan

Change the following dependencies to be external and real dependencies:
- [x] Move build.js to shared
- [x] Update reflect to use build.js 
- [x] Make the following external
  - `@badrap/valita`
  - `@rocicorp/logger`
  - `@rocicorp/resolver`
  - `@rocicorp/lock`
- [x] Update package.json to mark these as dependencies', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('7N8NmMRRFEopFKZg-T7_f', 's3Pnkr55mei8sd-aBlx-u', 1680036741000.0, 'Jesse is still seeing this on paint-fight.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('8D0Ksny4H1FAx3MxRGRip', 'WsyA82AUCyjc6Ttf8LqmX', 1679303817000.0, 'Duplicating the code might also have semantic issues. Like instanceof not working as expected etc.', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('7LOxaYlsBQfWBF83vy9rq', 'WsyA82AUCyjc6Ttf8LqmX', 1679322842000.0, 'Here is some code from the compiled bundle of replidraw-do:

```
;// CONCATENATED MODULE: ./node_modules/@rocicorp/reflect/out/reflect.js
// ../../node_modules/@rocicorp/logger/out/logger.js
var TeeLogSink = class {
  constructor(sinks) {
    this._sinks = sinks;
  }
  log(level, ...args) {
    for (const logger of this._sinks) {
      logger.log(level, ...args);
    }
  }
  async flush() {
    await Promise.all(this._sinks.map((logger) => logger.flush?.()));
  }
};
```

```
// ../replicache/out/replicache.js
var Xt = class {
  constructor(e) {
    this.qe = e;
  }
  log(e, ...n) {
    for (let r of this.qe)
      r.log(e, ...n);
  }
  async flush() {
    await Promise.all(this.qe.map((e) => e.flush?.()));
  }
};
```

```
;// CONCATENATED MODULE: ./node_modules/@rocicorp/logger/out/logger.js
/**
 * A [[LogSink]] implementation that logs to multiple sinks.
 */
class logger_TeeLogSink {
    constructor(sinks) {
        this._sinks = sinks;
    }
    log(level, ...args) {
        for (const logger of this._sinks) {
            logger.log(level, ...args);
        }
    }
    async flush() {
        await Promise.all(this._sinks.map(logger => logger.flush?.()));
    }
}
```

- One copy comes from the replicache bundle
- One copy comes from the reflect bundle
- And one final copy from replicache-do', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('1z208dsDIEkI_cMOBSfgO', 'LW3sBrJffl51JCGIHx5Lj', 1709536329000.0, 'Not necessary as it''s in rails.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('JTtk9D-AA8pyeMTTtloca', 'h19U5VVL9VG6VPHx7aqEi', 1680568644000.0, 'I kind of prefer it simpleminded as it is. Let''s way and see if anyone complains.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('9FE3qeeUDOzKs0VrugoQj', 'GgUGVzGishusa7eiVsVsr', 1678698590000.0, 'Really? I thought we had a test for this... checking...

https://github.com/rocicorp/mono/blob/main/packages/replicache/src/replicache-subscribe.test.ts#L428

It is not as fine grained as other subscriptions since we cannot determine if the emptiness changed based on the diff. We always call these subscription bodies.

We could improve this be doing the emptiness check outside the subscription body to determine if that changed.', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('dt9xlVMldaFno2dWgQg4Q', 'GgUGVzGishusa7eiVsVsr', 1678735158000.0, 'Huh, I think my test case was wrong. I am seeing that it works now too. Weird.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('L1r6orw7AVRd_Xj3wE7Aw', 'ojwQdGdrvD6sxgoe0tXWq', 1678579085000.0, '@grgbkr thoughts on this?', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('X-R7vuWqCqPj7KB_mU6ce', 'ojwQdGdrvD6sxgoe0tXWq', 1679619266000.0, 'Yes I think we should do this.', 'grgbkr');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('uzsHyk2U6HhmHC4peqzWt', 'ojwQdGdrvD6sxgoe0tXWq', 1683341969000.0, 'The `userID` field from the client is passed in the connection string to the server, so we should be able to fairly easily match it up against what comes back from the auth handler.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('MVEY2MxBrM-u7H3NaFUKS', 'ojwQdGdrvD6sxgoe0tXWq', 1683402496000.0, 'Fixed by https://github.com/rocicorp/mono/commit/2c1a49a822e5c3d5cbe3f13d2851191dd48af3a1', 'grgbkr');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('JJUvTeOW307XTytOxeyr-', 'Nt4PKu_sMVV62EWLGLosW', 1678717752000.0, 'https://github.com/rocicorp/mono/pull/394', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('csdA3rqa0Nw6CMZTceiJP', 'Nt4PKu_sMVV62EWLGLosW', 1679211662000.0, 'I think we probably want to dump local state when this occurs. It will be quite common due to #363 and dev mode, and makes the dx terrible.

This can create lost writes which is also bad, but during the alpha period I think it is more important to demonstrate the promise than to be perfectly robust.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('_EnDHJNbWMzN_RIAx-XQd', 'Nt4PKu_sMVV62EWLGLosW', 1679303403000.0, '> I think we probably want to dump local state when this occurs

What other part of the local state would be useful here? Pending commits? The BTree? The Client object?', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('MTTIUXkt3k4XfFlZNZffH', 'Nt4PKu_sMVV62EWLGLosW', 1679304007000.0, 'Oh sorry, what I mean is delete/drop local state. Basically delete all the Replicache data ðŸ˜¬.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('qLBq45Dc4chuabkaUPsyU', 'Nt4PKu_sMVV62EWLGLosW', 1679343285000.0, 'Big picture here, the goal is that on something like `reflect-todo`:

1. we can change the example to just instantiate Reflect with a hard coded room and it will work (that''s #363)
2. even if you kill the dev worker and reboot it, it will work

Right now I believe that step 2 will print an error to the JS console telling you to clear cache which is OK, but better for the alpha would be to just drop localstate and start over. I think?', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('ILOCAMTteFIX4u-5Yd3wi', 'Nt4PKu_sMVV62EWLGLosW', 1679344028000.0, 'It is tricky to do this transparently under the cover (i.e. close the
replicache client being wrapped, delete local replicache state, and create
a new replicache client, transparent to the user of the reflect client).
In particular its hard to get the subscribe/watch behavior correct.

I think roughly the best we can do is to delete local replicache state and
call a callback that by default reloads the page.  We could maybe do one
better, by transparently closing/deleting/reopening, if no watches have
been fired, but it seems likely to be buggy.




On Mon, Mar 20, 2023 at 1:14â€¯PM Aaron Boodman ***@***.***>
wrote:

> Big picture here, the goal is that on something like reflect-todo:
>
>    1. we can change the example to just instantiate Reflect with a hard
>    coded room and it will work (that''s #363
>    <https://github.com/rocicorp/mono/pull/363>)
>    2. even if you kill the dev worker and reboot it, it will work
>
> Right now I believe that step 2 will print an error to the JS console
> telling you to clear cache which is OK, but better for the alpha would be
> to just drop localstate and start over. I think?
>
> â€”
> Reply to this email directly, view it on GitHub
> <https://github.com/rocicorp/mono/issues/388#issuecomment-1476870559>, or
> unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AESFPBHJHCYD2FYEMDELD43W5C3EDANCNFSM6AAAAAAVV4ZJZI>
> .
> You are receiving this because you are subscribed to this thread.Message
> ID: ***@***.***>
>
', 'grgbkr');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('sdbkVa-L1ygNpk33_9BdE', 'Nt4PKu_sMVV62EWLGLosW', 1679346795000.0, 'I wonder if instead we should just recommend `wrangler --local --persist`. Development is the main problem. @jesseditson how has that mode been for you?', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('cSCYXdHnJuF_pSDnLkUVJ', 'Nt4PKu_sMVV62EWLGLosW', 1679352409000.0, 'That mode has worked fine! The only caveat is that it prints a recommendation to use an experimental new flag that does not work for me.', 'jesseditson');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('tr2gD2AH0Sx_ZZclw8iCF', 'Nt4PKu_sMVV62EWLGLosW', 1679359407000.0, 'OK let''s just do that for now. @arv nothing to do here for now.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('piHSN61oErglAgBTo-pFt', 'lmadQG9qgjCJ-cH0iAUrC', 1678309036000.0, 'Another way to ensure this is to build-dts and look for DD31 in there (and SDD)', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('k9fu_xCKkor0bQXh6m9_H', 'XeWRpSJn2BkjvCyC6Bd_6', 1678309104000.0, 'I added a known issue regarding this to the last release note FYI:
https://www.notion.so/replicache/reflect-0-13-1-reflect-server-0-22-0-7ebcdf937978409285d31b8eb1e80f2d?pvs=4#28f01adb761b41769f02962dbdce8257

On Wed, Mar 8, 2023 at 7:06â€¯AM Greg Baker ***@***.***> wrote:

> When reconnecting the CPU is pegged by rebasing mutations. This is
> because, the pusher logic pushes up mutations individually and then they
> come down in a series of pokes. Resulting in something like
> 1000 pending, poke contains 50, rebasing 950
> 950 pending, poke contains 50, rebasing 900
> 900 pending, poke contains 50, rebasing 850
> ... and so on
>
> This is improved somewhat by the 60fps buffering and playback logic, as
> the mutations from multiple of the reflect pokes above will often get
> merged into a single replicache poke.
>
> This is related to: #378 <https://github.com/rocicorp/mono/issues/378>
>
> â€”
> Reply to this email directly, view it on GitHub
> <https://github.com/rocicorp/mono/issues/384>, or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AAATUBB46FANUKFMYPP6QATW3C4AFANCNFSM6AAAAAAVUAV7VQ>
> .
> You are receiving this because you are subscribed to this thread.Message
> ID: ***@***.***>
>
', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('Se_4OUNhNNCjLrpKKzufG', 'v6dFwAmRkI7Ik95jkz04l', 1678270133000.0, 'Why is it annoying? Composable API seems better than specialized APIs.', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('bAoMeeD9GaCSfLdAy3o_L', '-z7a9Y0Lggy_cMzNNEf7m', 1678136763000.0, 'I think we should still hook the `online` event and use that as a hint to reconnect too, as even 5s is an annoying few beats to wait when you know you''ve just reconnected (especially in demos).', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('iXhsV9sjeXTIHGFWfsYPn', 'gPVhbmVjtQnpYc9lhqqOp', 1678222257000.0, 'Before you do this check with Aaron for last check about whether we''re really doing it.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('YoZQG7EEX27nsHy84Tbxx', 'gPVhbmVjtQnpYc9lhqqOp', 1684868566000.0, 'Raising priority since we also want this for the next Replicache release.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('-2PLFN5nF5pYgW1XmL62U', 'gMuZ1tweNI0m8O0VTFRnz', 1686045440000.0, 'A confusing thing I just ran into:

```
import {version} from ''@rocicorp/reflect'';
console.log(version);
```

prints `13.0.0-beta.0` because that is the version `replicache` exports.

', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('-WUgzSvarT7i3Pz8ju_tT', 'MPdM2gW91IhLM9Riata5f', 1677789465000.0, 'cc @arv ', 'grgbkr');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('9E79CTNoVzYCLrZ128EC_', '5Lzwt-BLI28FPj8fed01J', 1690343382000.0, 'This is working now.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('_pZjRISaqEhuTA0x_EqEb', '5Lzwt-BLI28FPj8fed01J', 1690363763000.0, 'âœ… ', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('MvCAgPeC7GGjZU95e0Ydl', 'eXWUPvyzDn7vHecgPEZI0', 1677762278000.0, '@aboodman @grgbkr ', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('PgGWg0PpO8eYh3M3K59iK', 'eXWUPvyzDn7vHecgPEZI0', 1677783512000.0, 'The jurisdiction flag that is passed into Reflect applies to the _room_. It is saying "I would like the data for _this room_ to be in EU". it''s solving the problem of "some of my (Monday.com''s) customers are EU entities and they need their data to be housed only in EU".

There is an interesting separate question of the auth DO. The AuthDO is shared among all rooms for a single customer (ie Monday) and the data within it is Monday''s data, not Monday''s customer''s data. Monday might someday ask us "hey I''m an EU state. I want my data to only be in EU.". This would have minor performance tradeoffs because it would put the auth do further away from some cutomers than it would otherwise have to be.

So far nobody has asked for this feature though.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('ynTI9_a9GnKB9rTEIAtMa', 'eXWUPvyzDn7vHecgPEZI0', 1677789788000.0, 'I was worried that the auth data might be considered user data.', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('ZWA1dOwKi9eAdLCJMdOcw', 'sL61Cb893TvSZQ-FNG-B6', 1680639014000.0, 'We should do a build at end of milestone and get these two customers onto it!', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('J26YeiB0DvY--y2ushNVv', 'Zo7n_XGcUrs5eU9Z_awvz', 1678321587000.0, 'We discovered another thing that should be available globally beside `env`: the roomID. There are probably going to be a few of these, we should probably define like a `StartParams` or similar that gets passed to `createReflectServer` which we can add things to over time.

See: https://rocicorp.slack.com/archives/C013XFG80JC/p1678321518575779?thread_ts=1678315633.516129&cid=C013XFG80JC', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('Usxz9FH_8Ri0xihTQ-TWH', 'Zo7n_XGcUrs5eU9Z_awvz', 1678322773000.0, 'Here is an idea I started playing with earlier and is coming back to me again:

```ts
async function createReflectServer(opts: CreateReflectServerOptions) {
  const rs = new ReflectServer({
    // Different signature and use from the `auth` field of `Reflect` but similar idea.
    // Too clever to overload?
    auth: async () => {
    },
    mutators,
    logLevel,
    logSink,
  });
  
  // rs is a fully functioning Reflect-like thing ðŸ¤¯.
  // - you can call `mutate.foo` on it (it will get queued in the game loop)
  // - you can call `subscribe` and get notifs when things change (ie to sync with
  //   external systems, maintain computed state, whatever)
  // - you can set timers or call fetch in the global scope and call mutators later!
  // - you can use libraries against it that are designed for the `Reflect` or
  //   `Replicache` interface.

  // The runtime arranges to call your mutators when messages come in.
  // The return type of `createReflectServer` is `extends ReflectServer<T>`, so
  // you can also *extend* Reflect and add your own state. 
  return rs;
}
```

We could implement this incrementally by having the return type of `createReflectServer` be as in https://github.com/rocicorp/mono/issues/352#issue-1605585020, and later add `ReflectServer` which happens to implement that interface later.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('O2fMq_xSSZDTHERF6c0bT', 'Zo7n_XGcUrs5eU9Z_awvz', 1678348901000.0, 'An interesting thing here is the way features compose. Example: we do not need `onDisconnect` to take a `WriteTransaction` (and then to deal with the fact that we don''t have a mutationID). We can just have `onDisconnect` be a normal callback and the user can do anything they want in there. If they want to mutate data, they just call `rs.mutate.disconnect(...)`.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('-6vSic5gmd1RFCntxN3Nq', 'Zo7n_XGcUrs5eU9Z_awvz', 1678354623000.0, 'I like exposing things like query and mutate but I am a bit concerned about making things less clear. For the server, these mutations are very different from the mutations the client makes. These do not have a client id and they do not have a mutation id etc. They do not get rebased and how do you order/queue these with the mutations coming from the clients? It just adds a lot of new concepts that have to be well thought through and that we need to teach.', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('qS1-qstZ3vAzgXrlSaFve', 'Zo7n_XGcUrs5eU9Z_awvz', 1678380355000.0, '> An interesting thing here is the way features compose. Example: we do not need `onDisconnect` to take a `WriteTransaction` (and then to deal with the fact that we don''t have a mutationID). We can just have `onDisconnect` be a normal callback and the user can do anything they want in there. If they want to mutate data, they just call `rs.mutate.disconnect(...)`.

But then the disconnect mutator is passed a WriteTransaction. Does that WriteTransaction have a mutationID?', 'grgbkr');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('QoAZZVCaXclFlWuJLR8-3', 'Zo7n_XGcUrs5eU9Z_awvz', 1678381738000.0, '> These do not have a client id and they do not have a mutation id etc.
> But then the disconnect mutator is passed a WriteTransaction. Does that WriteTransaction have a mutationID?

We could give them the special clientID `server` or similar then give them mutationIDs as normal.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('KrfFQviT4GcKtjMNALf_8', 'Zo7n_XGcUrs5eU9Z_awvz', 1679305830000.0, '> Another thing to check into is whether in CF, the env can change without restarting the context. I bet that it cannot. But if it can then we need to make the env field in the constructor a getter so it can return latest value (or maybe a function to make it clear it''s dynamic).

[The bindings assigned to the Worker. As long as the environment has not changed, the same object (equal by identity) is passed to all requests.](https://developers.cloudflare.com/workers/runtime-apis/fetch-event/#parameters:~:text=The%20bindings%20assigned%20to%20the%20Worker.%20As%20long%20as%20the%20environment%20has%20not%20changed%2C%20the%20same%20object%20(equal%20by%20identity)%20is%20passed%20to%20all%20requests.)

So it seems like they can change or at least that CF wants to leave this open to allow it to change in the future.', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('_7enDaLHl19CKqoBWuG_1', 'Zo7n_XGcUrs5eU9Z_awvz', 1679313468000.0, 'There are a lot of parts here. Let me focus on the `createReflectServer` part. This is what our user calls today. In a future setup with a saas we will restructure this for better ergonomics, but for now, our customer is calling us and then "handing" the `worker` to CF. That means `createReflectServer` cannot take an "env". Instead we inverse the flow slightly to take a function instead.

```ts
export function createReflectServer<
  Env extends ReflectServerBaseEnv,
  MD extends MutatorDefs,
>(
  getOptionsFunc: (env: Env) => ReflectServerOptions<MD>,
): {
  worker: ExportedHandler<Env>;
  RoomDO: DurableObjectCtor<Env>;
  AuthDO: DurableObjectCtor<Env>;
}
```

and an example usage:

```ts
const {worker, RoomDO, AuthDO} = createReflectServer(env => {
  console.log(env);
  return {
    mutators,
    authHandler,
  };
});
export {worker as default, RoomDO, AuthDO};
```

We can use a `WeakMap` to store the options per Env so we do not call this more than once per isolate and env.', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('f8RivsyeIXFVayrBrxXKG', 'Zo7n_XGcUrs5eU9Z_awvz', 1679358919000.0, 'https://github.com/rocicorp/mono/issues/352#issuecomment-1476093117 LGTM.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('f_Hc748nC2m-J04JFV2fT', 'Zo7n_XGcUrs5eU9Z_awvz', 1679389617000.0, 'For the global, we could do a global function `getEnv(): Promise<Env>` but I''m a little bit worried of "dead locks". Conceptually I think the isolate could have different Envs so a global might not be a good fit.', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('lr98cXUWcM_o24_Eu3D_U', 'Zo7n_XGcUrs5eU9Z_awvz', 1679466312000.0, '> We can use a WeakMap to store the options per Env so we do not call this more than once per isolate and env.

@arv I don''t think we need the WeakMap. I think it is totally fine and expected to call `getOptions` once per construction of `Reflect`. I would find it very confusing if this didn''t happen actually.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('l3ljUVBb85ZlEIIY1oN8J', 'Zo7n_XGcUrs5eU9Z_awvz', 1679466393000.0, 'It should just be:

```
createReflectServer(env => {
  // Gets called once when the server starts.
  return {
    ...
  };
});
```', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('-1u5yljHG63hK7rGF4nfe', 'Zo7n_XGcUrs5eU9Z_awvz', 1679475043000.0, '> // Gets called once when the server starts.

It cannot be called when the server starts. It gets called from `fetch` and `scheduled` gets called by CF worker as well as for when the RoomDO and AuthDO gets instantiated.

For typical usage it gets called 4 times if we do not have a WeakMap.

I agree that it should get called once per createReflectServer. Let me see what I can do.', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('I22Eqwh73bwz4B3wyeIJ4', 'Zo7n_XGcUrs5eU9Z_awvz', 1679478287000.0, 'Argh fetch. This CF API is so frustrating.

In this case I understand why you did it that way.

> I agree that it should get called once per createReflectServer. Let me see what I can do.

I was being lazy/imprecise with my writing when I said: `// Gets called *once* when the server starts.` Sorry that''s happened a few times, I''ll try to be more precise in the future since we''re async.

Often, the worker, roomDO, and authDO can be in separate isolates or on different machines. So in that case, it is not possible for the options callback to get called only once per `createReflectServer`.

What i really meant was "Gets called once per worker/DO, the first time the worker/DO needs the options".

Given all this i think what you did is the best balance of forces. I''ll revert this part of my recent PR.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('XRdOm6z_TGnrij45jT_x7', 'Zo7n_XGcUrs5eU9Z_awvz', 1679478357000.0, 'And I don''t think any other work is necessary here by you right now - let me know if I''m still confused.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('k09Rua5xmU9QN1pFuwgmp', 'Zo7n_XGcUrs5eU9Z_awvz', 1679482783000.0, 'I got a PR that does this once per call to createReflectServer.

What isn''t clear to me is how this interacts with isolates. If it wasn''t for isolates the options object would be shared between these 4 cases. The options object is part of a closure. I assume CF has to call createReflectServer once per isolate and that it uses one isolate for the worker fetch, one for worker scheduled, one for RoomDO and one for AuthDO.

', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('Z6f4CXKUmDxVoBiFxAjyo', 'Zo7n_XGcUrs5eU9Z_awvz', 1679485292000.0, 'I instrumented the code to see how often this got called. The caching I put in place in #430 allows reusing the options between fetch calls.

We get one isolate for Worker, RoomDO and AuthDO respectively', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('_cGchsNOTFANS8hU4YJZ8', 'Zo7n_XGcUrs5eU9Z_awvz', 1679487755000.0, 'CF uses one isolate per machine/script/version. In CF nomenclature a "script" is the thingy that wrangler.toml describes. So each unique version of one of those on a machine has its own isolate.

In the simple case where there''s one just user then yes, the worker, authdo, and roomdo will all be in same isolate.

But if there are multiple users in a room, the worker and roomdo can easily be in different isolates since CF will spawn a new worker close to where user 2 is, even if user 1 already has their own worker.


', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('4vZAPa6vJ2r64KVO4a_2H', 'Zo7n_XGcUrs5eU9Z_awvz', 1679490008000.0, 'Calling this done with #430 ', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('Kcx7EX0Q4eKWkBdLaSsRl', 'nuPkE2aUsvOlgl3Plc7F3', 1685126377000.0, 'Here is a useful datapoint. It took about 15s to connect to puzzle-000000:

https://github.com/rocicorp/mono/assets/80388/c7ed8942-7351-4dca-a08b-4b8e98f98d9e

', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('9syQZiquUrysL4KZ9lZri', 'a2XvjZLnYV40X-tvhkML0', 1678401450000.0, 'Cesar is going to do this!', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('ndvu1JYCaWA0c1sWYvBtM', 'a2XvjZLnYV40X-tvhkML0', 1680634271000.0, '@cesara these demos don''t currently work on iphone. I don''t see any message in the server console, and receiver doesn''t play either.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('19CmxLZDO0U0yEzTS3_Iw', 'a2XvjZLnYV40X-tvhkML0', 1681819114000.0, 'Closing this in favor of burn down notion.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('xr9NfpKyQCHK5XC3xTrGr', 'MFfdqRANMgtlQa43HXWQJ', 1677671726000.0, '@grgbkr wdyt?', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('FEezv2lV6GBVhOz4zkSeE', 'MFfdqRANMgtlQa43HXWQJ', 1709536376000.0, 'I think maybe we implemented this @arv ?', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('mc50b88wGpln6SosDDF_z', 'MFfdqRANMgtlQa43HXWQJ', 1709546003000.0, 'No. I don''t see anything in `close` that waits for persist.', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('EDo-pIg5NSA4D9_IQR90N', 'MFfdqRANMgtlQa43HXWQJ', 1709580453000.0, 'Agree this makes sense, and am a little surprised we aren''t doing this
already.

Should we also be doing it on visibilitychange?

On Mon, Mar 4, 2024 at 2:53â€¯AM Erik Arvidsson ***@***.***>
wrote:

> No. I don''t see anything in close that waits for persist.
>
> â€”
> Reply to this email directly, view it on GitHub
> <https://github.com/rocicorp/mono/issues/348#issuecomment-1976173593>, or
> unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AESFPBFTGEMQKSCATOAW2L3YWRACBAVCNFSM6AAAAAAVL6NGR2VHI2DSMVQWIX3LMV43OSLTON2WKQ3PNVWWK3TUHMYTSNZWGE3TGNJZGM>
> .
> You are receiving this because you were mentioned.Message ID:
> ***@***.***>
>
', 'grgbkr');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('OwomCUG0Em089bDdGdp3K', 'pq-diA4zFzhkW7VJsn3Qr', 1677617489000.0, 'Fixed in #344', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('9RPgHOPV-RYCTAO3Ug7kT', '0rJxp4rrxgBHvH_hxZjSm', 1677694982000.0, 'Greg''s current idea: https://rocicorp.slack.com/archives/C013XFG80JC/p1677560755693459?thread_ts=1677555317.057609&cid=C013XFG80JC

my current idea is:
add schemaVersion to ReflectServerOptions, if the schemaVersion differs from what is stored on startup call a customer provided schemaVersionChangeHandler passing it a WriteTransaction (or a WriteTransaction factory).  Don''t accept connections until the schemaVersionChangeHandler completes
add schemaVersion as a param to socket connect, and reject connections if the schemaVersion doesn''t match server''s current version
keep the existing behavior on the client of giving new schemaVersion''s a new idb so they do a full sync', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('HLTblKNOZbSu8uA_76t3v', '0rJxp4rrxgBHvH_hxZjSm', 1677867863000.0, 'We should also consider if we should support undo-migration/down-migration/migration-rollback (see https://flywaydb.org/documentation/tutorials/undo, https://www.prisma.io/docs/guides/database/developing-with-prisma-migrate/generating-down-migrations).

In addition or alternatively we could support snapshotting before a migration, and allow rollback to the snapshot (if a migration ends up being destructive, undo-migration wont be able to recover the data).   ', 'grgbkr');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('V_ieGW9sl1Q1nhI4eI2Cl', 'niqzkiznUTO33HDXdZ328', 1677706276000.0, 'Duplicate of #178, I''m losing it.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('SMnG_KC-VkWkgmixGzvcK', 'GnmhxPQj--yAjshNQR5xj', 1677036195000.0, 'Next steps:

* Verify this still happens on trunk / latest wrangler. If it does happen with latest wrangler, then:
  * File a bug with cf. Unhandled rejections should be printed to console in dev mode.
* Check whether this error goes to logpush in production. If it doesn''t, file error with CF. Unhandled rejections should go to logpush.
* Check whether this error goes to `unhandledrejection` event handler (and has a defined `reason`). If it goes there, add such global handler to our code and send to `lc.error`. If it does not, file bug with CF.

* On 0.21.1 branch, check whether this goes to logpush. If it does *not*, add a try/catch around this block and update build that monday is going to deploy.
', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('G3enxLD2CJX9fFMcaBRt6', 'GnmhxPQj--yAjshNQR5xj', 1677071506000.0, 'I''m seeing the following:

```
ERR RoomDO doID=f9623b0ceef5dc8b0b0593bad751d8be97f31c32e798009f9ae47cad753c4cb4 roomID=mDmS3P Unhandled promise rejection: XXX
```

And here is the diff:

```diff
diff --git a/packages/reflect-server/src/server/room-do.ts b/packages/reflect-server/src/server/room-do.ts
index 14c5695b..23e846ee 100644
--- a/packages/reflect-server/src/server/room-do.ts
+++ b/packages/reflect-server/src/server/room-do.ts
@@ -106,6 +106,10 @@ export class BaseRoomDO<MD extends MutatorDefs> implements DurableObject {
       .addContext(''doID'', state.id.toString());
     this._lc.info?.(''Starting server'');
     this._lc.info?.(''Version:'', version);
+
+    addEventListener(''unhandledrejection'', event => {
+      this._lc.error?.(''Unhandled promise rejection:'', event.reason);
+    });
   }

   private _initRoutes() {
@@ -387,6 +391,7 @@ export class BaseRoomDO<MD extends MutatorDefs> implements DurableObject {
   }

   private async _processNext(lc: LogContext) {
+    void Promise.reject(''XXX'');
     lc.debug?.(
       `processNext - starting turn at ${Date.now()} - waiting for lock`,
     );
```', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('aETUovvHPzOtir_DRNEOH', 'GnmhxPQj--yAjshNQR5xj', 1677085149000.0, 'On what version of reflect/wrangler?', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('cMFglYfBHrg_DP311xnWJ', 'GnmhxPQj--yAjshNQR5xj', 1677152697000.0, '@arv - in addition to the code change you have in progress, can you do the following:

1. Can you tell me whether, without any code changes, does trunk Reflect reproduce this bug (does it fail to send the unhandled rejection to console output)?
  - If so, we should file a bug, but I''m not sure actually where to do that. Will ask.
2. Can you tell me whether, without any code changes, does trunk Reflect send this unhandled rejection to logpush?
  - If not, we should file a bug
3. Can you tell me whether, without any code changes, does v0.21.1 Reflect send this unhandled rejection to logpush?
  - If not, we probably need to make a patch release of 0.21.1 that includes this error handling', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('oeyPf_LDtnQw-h98R35N0', 'GnmhxPQj--yAjshNQR5xj', 1677162217000.0, 'Here are my observations:

1. Unhandled rejections are not logged by default in a DO
2. Unhandled rejections are not logged by default in a Worker
3. Nothing is reported in logpush

I checked tail (on CF dashboard and locally) as well as logpush (on DD)', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('G4xQEe7yo0IUjrGTrtjTW', 'GnmhxPQj--yAjshNQR5xj', 1677618223000.0, 'Added code on our side to handle unandled rejections:

9c8cfb703dd9efee911f20e18c711cdbee343296

I haven''t followed up with cloud flare what the intended behavior is and if they could expose this in their logs at least.', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('_FOAVwtwmQkINRCqF-yw3', 'GnmhxPQj--yAjshNQR5xj', 1677670632000.0, 'Posted question to Cloudflare''s Discord.', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('COX_Avik5fClLNqxsrrt2', 'GnmhxPQj--yAjshNQR5xj', 1677670785000.0, 'Kenton said that things that are more toward the side of bug like this could be better posted in https://github.com/cloudflare/workerd.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('BYDCyrZmROROrPHG0yeEe', 'GnmhxPQj--yAjshNQR5xj', 1677698244000.0, '@arv once you post this to cf issue tracker we can consider this closed!', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('iDFVlxBGIhApPDnx0Ms1b', 'GnmhxPQj--yAjshNQR5xj', 1677754391000.0, 'https://github.com/cloudflare/workerd/issues/412', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('wus7uWGKZ5zvEZVuZ_G3e', 'g-7mAvE-qMLJgg-v0ZzQi', 1677790410000.0, 'FWIW here''s what I do now: https://github.com/rocicorp/reflect.net/commit/1fae51626983a171869d2702289a133c02c954af', 'jesseditson');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('d06c4nDnwxUo8_tHknkmk', '_Wef2qqAq4jCHbnSGRoaP', 1677002734000.0, '@arv do you agree?  if so I can make this change quick.', 'grgbkr');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('7KfGkLpuO6aK-FwpmSerb', '_Wef2qqAq4jCHbnSGRoaP', 1677065352000.0, 'I agree', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('Q5YCIX_Nk4GhpnzbJR9i3', '_Wef2qqAq4jCHbnSGRoaP', 1678115377000.0, 'When passing `--platform=neutral`, `nanoid` ends up importing nodejs specific modules and bundling fails.

We didn''t have this kind of problem in replicache since it didn''t have any runtime dependencies.', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('av4cTNQsNRcFGKPqeRch3', 'w3k2LCZJxewgS1dARkA8Y', 1677164918000.0, 'Done in https://github.com/rocicorp/mono/commit/b25e7940af349190effa21e9b82009e9dd7e24ed', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('Gblegc2dRrbG_Y8PduGx5', 'w3k2LCZJxewgS1dARkA8Y', 1677165206000.0, 'Not sure this is working.

It says:

```
â€¢ Remote caching enabled
```

```
@***/reflect:check-format: cache miss, executing 902c3594961702cf
replicache:check-format: cache miss, executing 8973c6d29bd20ec4
@***/reflect-server:check-format: cache miss, executing 9b514e8f3872a890
```

Let''s check on this later', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('AShRg0Uoyfy_ej-nzeGt8', 'xM_4rGRsys3jKT0CiGJCP', 1676581227000.0, 'I think there was a `nodeConsoleLogSink` somewhere for this purpose, since we want to have a version that doesn''t stringify for environments that are fancy (ie browsers).

But on second thought maybe it''s better to just do this in one place and accept that it won''t be perfectly optimal in browsers.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('6bDBP_m3mmwopZblbMSFd', 'xM_4rGRsys3jKT0CiGJCP', 1676645118000.0, 'Yeah, maybe it is not worth it.
', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('lg7rcmizdEYwLH8aC1mzZ', 'xM_4rGRsys3jKT0CiGJCP', 1677695694000.0, 'Agree let''s simplify and just not have the fancy expandy logging in browsers so that we can have just one console logger.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('qkgQwq6PhpiRL5HRGfYOY', 'zV0bdg5YJbbylZqmUhrQt', 1677698286000.0, 'The logs part of this has been done. The metrics part will roll into the metrics bug.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('uV61e0E2sNLOOE-tFCrWD', 'BLUsC86cjEIHYkXuiYnTU', 1677666133000.0, '**delete** service does not seem right to me', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('dIBzvfFTq_3OR5Zn9tDDV', 'BLUsC86cjEIHYkXuiYnTU', 1677666987000.0, '1. I verified that migrate works.
2. Deleted the worker
3. `wrangler publish` again

Everything seems to work fine.', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('j51RkibbFwTsWwscZnNqf', 'qQJ73nkTKuavBoXO4aFti', 1677695758000.0, 'Seems like this is internal and has no customer impact. If so can we take out of the milestone @arv?', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('zKWzqpHV5MsGlYFzXo-jT', 'qQJ73nkTKuavBoXO4aFti', 1677695775000.0, 'Taking out optimistically, LMK if you disagree.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('KgfWUkZH20olNpST3JNzU', 'WrDhErVP1Ef_KNtlUSzjb', 1677695806000.0, 'I think this is done @grgbkr ?', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('nepG05UBeZ04a3Z0mNlN4', 'WrDhErVP1Ef_KNtlUSzjb', 1677752055000.0, 'Yup. In 0a2cb5bea263de38480be8d8dbbd646a8d3cb246', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('k1HMUGd1TeWawintgnMgd', 'bFaOQIA4nFbMiKJdA2T8L', 1677696386000.0, 'Punting from milestone until someone has time to think about. We won''t get kicked out of bed for this deficiency.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('9J-UsoO1BmPfXRgh7vGLX', 'bFaOQIA4nFbMiKJdA2T8L', 1677700836000.0, 'Factored just the first item out into https://github.com/rocicorp/mono/issues/352.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('ICIHzuSHbhmLO0MhYWumv', 'bFaOQIA4nFbMiKJdA2T8L', 1684746343000.0, 'See more details of idea here: https://github.com/rocicorp/mono/issues/352#issuecomment-1461092522', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('7eYiGEClNKwX8T__10uI4', 'Gh2cCtydyJTG7-mm6lA2y', 1677696434000.0, 'Nice to have but not required for beta.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('QoL8gj8579C91gYZKeQDa', 'Gh2cCtydyJTG7-mm6lA2y', 1683855049000.0, 'I think we need to redesign the server-side API more broadly, but for now, for consistency with other events, I suppose we should call this `roomStartHandler` ðŸ˜•', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('3grjrGdtWIW1BIToNrp3i', 'Gh2cCtydyJTG7-mm6lA2y', 1684355253000.0, 'Woo, this will be great for next release.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('d_Hz2_ThqC-o_c6p1Sk-H', 'gEXJOAFBf35Kt0q7RN5Lh', 1677696442000.0, 'Nice to have but not required for beta.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('pLxoJb3v-fpBdpcozQyaD', 'gEXJOAFBf35Kt0q7RN5Lh', 1683485986000.0, 'I''m told that @grgbkr has a draft of this somewhere.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('h9B8ovPoCKPqfnxUPiuZB', 'gEXJOAFBf35Kt0q7RN5Lh', 1683679684000.0, 'Here is an example use case that came up in the ALIVE demo:

We want to shuffle the demo on load so the user gets a nice initial state.

We could wait until we get the initial state, delay displaying anything, and if there are no non-bot users shuffle. But there is a small chance that there is some other user also joining at that moment, and that user will see the state suddenly shuffle.

To fix that issue we could do the shuffle server-side. So: we wait to display anything, do a mutation that shuffles the demo, but only if nobody is present, and wait for that mutation to round-trip through the server. But in that case, we have to wait for the mutation to round trip, delaying startup.

The best place to do this shuffle would be on the server, right as a user is connecting. We can see at that moment if they are the only one present and if they they are we shuffle the demo just before they sync their initial state.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('FauyzfWBC64BqxfcC-DNn', 'gEXJOAFBf35Kt0q7RN5Lh', 1683764770000.0, '> Exceptions from the connect handler should prevent the connection from being accepted (and result in an error back to the client).

This last requirement differs from the existing behavior and can be used in interesting ways; I want to flesh out my thoughts and confirm that we''re all on the same page.

Currently, a connection can be closed early (in `handleConnection()`) due to errors originating from the contents of the connect request: bad request format, invalid connection group id, invalid lmid, invalid base cookie. Importantly, a client was always able to fix its error and retry the connect.

Closing a connection due to an error from the `connectHandler` introduces a new scenario in which the connection can be closed due to situations outside of the control of the client. I can see this as a desired feature (e.g. disallowing connections for rooms that are too full), but it is, I believe, a new class of behavior and I wanted to confirm is intended.

One of the reasons I ask is that @grgbkr''s initial implementation (granted, done before @aboodman listed these more detailed requirements) plumbs the `connectHandler` down into a callback where it''s not straightforward to close the connection (i.e. [`processFrame()`](https://github.com/rocicorp/mono/pull/494/files#diff-6821aedf9ef09ff065dbf002dc72a7927e474df1876b02677ac20b81aba73480R47)). I presume he did this to be symmetric with the `disconnectHandler`, but I also imagine that he didn''t have this close-connection-on-error behavior in mind.

In summary, two questions:
* @aboodman: Can you confirm that we want the `connectHandler` to be able reject incoming connections due to non-connection-related scenarios? (fwiw, it makes for an interesting capability but does add a bit more complexity to the code when compared to simply logging/ignoring the errors)
* @grgbkr: If so, can you advise as to the best place to invoke the `connectHandler` transaction? Doing it synchronously in [`handleConnection()`](https://github.com/rocicorp/mono/blob/7f6331652b2177acd4abac792f3f9e8aba70e20f/packages/reflect-server/src/server/connect.ts#L136), before the call to `putClientId()`, makes it easiest to close the connection on errors, but it also circumvents all of the poke-sending logic that in the `processPending()` -> `processRoom()` -> `processFrame()` callpath.
', 'darkgnotic');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('krahwxeKEGj870OkOr2Fq', 'gEXJOAFBf35Kt0q7RN5Lh', 1683768465000.0, 'The reason I put this requirement in is that I feel it would be very difficult to reason about the system if `connectHandler` was not guaranteed to run (and succeed) before a connection was accepted.

The main purpose of `connectHandler` that I envision is ensuring certain state exists for a connection before it proceeds. If we ignore errors and proceed then this invariant is destroyed.

You''re right to point out that it''s not symmetrical with `disconnectHandler`. We cannot offer the same level of guarantee that `disconnectHandler` completes, unless we are willing to kill the entire rooms forever if `disconnectHandler` fails. Honestly part of me is tempted to do that. I really like invariants :). But it feels like something users would often hit and hate.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('bEcsvmZMPkrbP0iwRVeOx', 'gEXJOAFBf35Kt0q7RN5Lh', 1683768652000.0, 'This is often the point where @grgbkr will point out some consequence of my design choices that I didn''t anticipate which will cause me to rethink. @grgbkr wdyt?', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('dxeD3d3xQcGjBJTawjOD6', 'gEXJOAFBf35Kt0q7RN5Lh', 1683768817000.0, 'Agreed, I like the behavior from an API / invariance perspective. I''ll figure out how to best rework the code to make this possible (and hopefully clean), given that currently the mutate/send-poke logic and the connect/reject logic are in different layers.  ', 'darkgnotic');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('It_NSCrLP-k8q8SuCaUbu', 'gEXJOAFBf35Kt0q7RN5Lh', 1683776315000.0, 'After covering more of the code, I see that there is precedent for closing connections after `handleConnect()` has succeeded, so I think this can in fact be done fairly cleanly.', 'darkgnotic');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('1bixXj280jSc1cAL9HmNr', 'gEXJOAFBf35Kt0q7RN5Lh', 1683777507000.0, 'Hmmm ... I guess it depends. If we run the `connectHandler` in the `processPending` loop after the connection has been accepted, I think it''s technically possible for a client to connect, push mutations, and then get disconnected due to a `connectHandler` error, but those pushed mutations would still have been accepted, thereby violating our desired invariant.

So it may be that the best way to prevent any client-mutations in the face of an error-returning `connectHandler` is to synchronously run the connectHandler in `handleConnect()` before accepting the connection and registering the websocket. Will pow-wow with @grgbkr when he''s back.', 'darkgnotic');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('Mrpcmbyd5mNCOLu2mS--F', 'gEXJOAFBf35Kt0q7RN5Lh', 1683837359000.0, 'Closing the connection (before we send any pokes to it) is straightforward.

However, not processing a client''s mutations until its connectHandler succeeds is complicated.   

A client''s mutations can be pushed by other clients (either via them sharing a client group, or via mutation recovery).  ', 'grgbkr');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('qxqQlYd9yKG-w4VzzjNpb', 'gEXJOAFBf35Kt0q7RN5Lh', 1683854047000.0, 'Summary of our huddle:
* Connection-rejecting connectHandler may result in unintuitive semantics given that clients can send each other''s mutations with DD31
* We can accomplish @aboodman''s intended use case better with [onRoomStart](https://github.com/rocicorp/mono/issues/174)
* This `connectHandler` may be reincarnated in some different form for handling Presence. Aaron and I will write a design doc.  ', 'darkgnotic');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('-TyCnopT48htyh9ixXyjx', 'Nq0htvSPDitB4OtUjrm_s', 1677696460000.0, 'I think this may have been fixed @grgbkr ?', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('iPOSTdAoMgxYGK5seloa9', 'Nq0htvSPDitB4OtUjrm_s', 1677697213000.0, 'Not fixed yet.  

My current thinking is to keep fastforward in turn processing (its efficient to do it as a batch when a DO restarts, and has a bunch of clients reconnecting).  I think new connections should cause a turn to run (just as mutations and disconnects do).   This structure can all onConnect (https://github.com/rocicorp/mono/issues/175 ) to be implemented in basically the same way onDisconnect is today ', 'grgbkr');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('qrF9oCeR2n0E1Ib4TDTgk', 'Nq0htvSPDitB4OtUjrm_s', 1677698737000.0, 'Duplicate of #293', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('9CsfwNy9zdeCJuUvEM3p9', 'OjzdiqxGAf2EdgbkenwD7', 1683332950000.0, 'I think this has been done, right @grgbkr ?', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('KHpRxUutcOOCc_5vEnkUk', 'B82TlBRphU8lxQeUwf5dO', 1675934779000.0, 'Erik says: "We should also probably wait 10s (or 5s, or whatever) before doing this to allow cycling through tabs"', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('RPTrB0PAFm0c_lchkjp0k', 'vhsfVoQ6fiiFCxLtFZcNB', 1675762685000.0, 'This one I feel it is sufficient to just rename the mutators when you want a breaking change, like in Replicache?', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('r6wJkOJ7wyOK8OTF-1_uZ', 'vhsfVoQ6fiiFCxLtFZcNB', 1675785018000.0, 'SGTM', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('vCNsYRaHxaCmZ2JXMwMm9', 'EvajFts82v8s3vXjLvwmb', 1675761725000.0, 'For Reflect we could probably collapse **PullVersion** + **PushVersion** into a single **ProtocolVersion**.', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('GfC-0NQme4K9KYzxR_0tt', 'EvajFts82v8s3vXjLvwmb', 1675762777000.0, 'Agreed - only the connection should be versioned in Reflect, not individual messages flowing over it.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('Jjl3Kkh9L9JvjLUt2fozQ', 'EvajFts82v8s3vXjLvwmb', 1675934022000.0, '> How does this overlap with PullVersion and PushVersion used in Replicache?

PushVersion should be removed from the protocol. It is just taking up useless space in the messages.

> SchemaVersion

SchemaVersion should also be moved to the connect message. It doesn''t make sense in push. But we should have a separate bug for schema management.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('HikfMeaEgR7uq2AXH2sL3', 'EvajFts82v8s3vXjLvwmb', 1675934490000.0, '> The client sends its protocol version when it tries to connect.

As a tiny note, I think it would be nice and consistent if the protocol version was sent by way of the API path -- /api/1/connect, or whatever.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('KqVVQKNNoQ_mdIObLqLvL', 'EvajFts82v8s3vXjLvwmb', 1676315183000.0, 'We will need this for DD31 as it involves breaking protocol changes.  ', 'grgbkr');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('yMIsr9emnkJadhrkyNJ5z', '37ERG8iH3VUeHDBRRZ0NZ', 1675136072000.0, 'To add one addendum, I''m pretty happy with how this low tech solution turned out -- it was super easy to add a new metric type (State) and to build a dashboard for it once I understood how datadog handles the concepts. I think it''s a decent (though maybe not ideal) foundation on which to build understandability for reps and reflect server. I think the next steps would be to address the scale issue described above by adding aggregation and then to develop the observability/monitoring/alerting plan for reflect (and later, reps) which is some combination of the poorly described and factored https://github.com/rocicorp/reflect-server/issues/193 and https://github.com/rocicorp/reflect-server/issues/60. I think that with a couple of engineer-weeks one could:
- define the minimal set of metrics one would like to keep for reflect-server (eg request error rate or ws message error rate, ws msg latency, request count, storage write latency distribution, auth failures rate, game loop lag, etc/whatever)
- add whatever new metric types one needs for these (likely: rate, count, maybe set for clientids) and integrate with reflect-server
- add basic dashboards
- prototype alerting by adding some alerts on the above to whatever the most trafficked demo app that there is. 

This would provide a pretty good guide for monitoring reps when the time comes. 

', 'phritz');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('zVC6uBuDnIBq1B05RUjO6', '37ERG8iH3VUeHDBRRZ0NZ', 1679818208000.0, 'Fixed by #437 ', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('AKAtUF9ovWV3a7Yoszi4t', 'ksNwVR3DysT5WFxCK3WIa', 1674548850000.0, 'omg yes, please.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('UCwcWCoeIZpZsRTru2TNY', 'ksNwVR3DysT5WFxCK3WIa', 1674548927000.0, 'This would simplify tons of code, all over the place. As far as waiting for persistent storage, we could have a testing-only `ready()` api or something, but why do the test need to know when storage is available?', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('iTdo2Bo5vGnAYcVtimsjg', 'ksNwVR3DysT5WFxCK3WIa', 1674548970000.0, 'Before SDD we used to reuse the `clientID` but with both SDD and DD31 we always create a new `clientID`.', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('X-syaHy9F0YsJi2AuIRfi', 'ksNwVR3DysT5WFxCK3WIa', 1674549789000.0, '> why do the test need to know why storage is available?

I think it is mostly useful for unit tests to reduce nondeterministic behavior.', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('CZRpg6mol6KWQEOFOZICR', 'hjO4PWlvoCCXj_AlS4CNk', 1674493837000.0, 'This part of Replicache was always a bit wonky. In retrospect, I''m not sure why we need the retry mechanism. Would something like:

```ts
class Reflect {
  constructor(options:{auth: string});
  // I think you guys were already working on a mechanism for "structured errors", this is just that mechanism
  // with a code for auth.
  onClose: (reason: "auth"|..., details: string);
}
```

work? User could hook `onClose` + `auth` to just recreate Reflect and try again. True there''s no backoff here, but  I''m not sure that really matters and every one of these indirections in common case makes Reflect a little harder to use.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('8cql4cLup4Wr8Wfyf-iWa', 'hjO4PWlvoCCXj_AlS4CNk', 1674558627000.0, 'I like the idea of closing the Reflect instance on auth error. Let''s try it!

~~I don''t like bundling this into `onClose` because it is not clear if `onClose` should happen if `close()` is called. I''d rather be more explicit and use `onAuthError`.~~

I realized that there are a few non auth close with error so using `onClose(ok: boolean, kind: string, details: string)` seems reasonable.', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('k4HUjUUDqb5rCm5Hb2fNa', 'hjO4PWlvoCCXj_AlS4CNk', 1674588975000.0, 'Picking up from https://github.com/rocicorp/reflect/pull/83#issuecomment-1402468754...

I see that it is awkward to special case this one class of error behavior. Sorry for not thinking this through. I change my mind: all classes of server errors should leave Reflect "open" so the app can continue working, and it should try and retry in the background.

I still feel a little sad about this though:

```ts
const rep = new Reflect({
  apiKey,
  roomID,
  mutators,
  authToken: () => myConstantAuthToken
})
```

... for the very common case where user doesn''t handle reauth. It''s just a tiny bit of friction that makes the dx feel less inviting when people are first trying the product.

A few simple fixes I can imagine:

1. authToken can be a string or a function. In the string case a function returning the string is implied.
2. There''s a separate reauth API
3. Aaron is being unnecessarily prissy

I guess of these I like 1 the best.

For the question of retrying, I think we can integrate retrying auth into the normal exponential backoff?

If the client fails auth on the server-side, we send an error and close the connection. The client sees that the error is auth related and internally clears the auth token. Exponential backoff happens normally. On next connection, client sees auth token is null and calls `authToken()` function.

WDYT? (also @phritz)', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('y0-DjJt5-gr2dCCRRhx4D', 'hjO4PWlvoCCXj_AlS4CNk', 1674600291000.0, 'I''m not sure about the exponential backoff for reauth. I will have to think a bit more about it tomorrow...', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('KqP7LBlae3USYI00TdQE8', 'hjO4PWlvoCCXj_AlS4CNk', 1675888734000.0, '> I''m not sure about the exponential backoff for reauth. I will have to think a bit more about it tomorrow...

I ended up using a backoff but the first auth error is tried immediately.
', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('BdnIRfm6VMqaTRuK_calZ', 'uKLntOMhEHkSnEv0g33OI', 1683340809000.0, 'Update: I forgot about this concerned and increased the rate significantly to 1 ping every 5s :). So maybe this is more of an issue now.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('aNGmx3VaxjBNYu42fiUaF', 'V6GY_WByY1kDHoeefuCtK', 1674056958000.0, 'https://discord.com/channels/830183651022471199/1063387649462775828', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('KR7sntQSBn5MeaFBm9V4w', 'V6GY_WByY1kDHoeefuCtK', 1674061546000.0, 'I think this is covered by rocicorp/mono#43 , no?', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('iOqNqX1gfNZTNqQAus3ap', 'V6GY_WByY1kDHoeefuCtK', 1674119543000.0, 'Yup. Closing in favor of rocicorp/mono#43 ', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('NfrmszEdpAbm31mbcEx_c', 'eYxcTP-MuuJ36qlHHEXbC', 1677696997000.0, 'I no longer think we should do this.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('QaUQ5HinPQl-oReH6akpF', 'xut4J7fPqhioTXsCGnVc8', 1673630311000.0, 'For custom attributes, say doID or something, do we have to do something to get datadog to recognize them (eg, add a pipline that maps them to a tag or something), or does that happen automatically if they are passed in the context? 

Generally, whatever we need to do to get datadog to recognize and search or join on the additional context in the log we should do. ', 'phritz');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('h-s4JOGX9fAVD7bf7LwlO', 'xut4J7fPqhioTXsCGnVc8', 1673948299000.0, 'We do not need to do anything to get Datadog to recognize these. We can filter and add columns etc

Filter:
<img width="931" alt="Screenshot 2023-01-17 at 10 33 48" src="https://user-images.githubusercontent.com/45845/212861996-335079bf-bb33-47f7-8229-133b48a79039.png">

Columns:

<img width="595" alt="Screenshot 2023-01-17 at 10 37 07" src="https://user-images.githubusercontent.com/45845/212862647-ebc14247-1f23-49fb-ba29-117e4ba995c6.png">

Datadog does have a feature that allows you to map names to other names so you can unify things like `clientID` and `client_id` etc.

', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('irdNAP2QWUsU3Upc3ptvj', 'xut4J7fPqhioTXsCGnVc8', 1677696950000.0, 'Still think we should do this. @arv it''s not clear to me if this requires a change in `LogContext` or just `DataDogLogSink`. ', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('9q7t3XL0GKUFxu3GnWArG', 'xut4J7fPqhioTXsCGnVc8', 1677752344000.0, 'My thinking was that the `LogSink` interface would get a new optional method:

```ts
logWithContext?(level: LogLevel, context: Context, ...args: unknown[]): void;
```

and the `LogContext` impl would use that if present instead of adding the context as string args.', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('cG4xNzhi39Ksc7azpB22m', 'QJZsGPRqM-9XZYPkJMrkJ', 1673470151000.0, 'aaaah.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('VRMlRAlHdfNY-i9VK01CK', 'QJZsGPRqM-9XZYPkJMrkJ', 1673470239000.0, 'But... this all started due to @jesseditson putting a large `UInt8Array` into DO. `UInt8Array`s, like almost anything else are type safe from a JSONValue perspective. It is just that they get serialized as `{"0":0,"1":1, ... }` which is very inefficient and also does not round trip.

In debug mode we could warn about usage of typed array (`ArrayBuffer.isView(value)`) but it seems strange to have a deny list... thinking if it is possible to have an allow list instead...', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('Us1Qa-zj6LErgDhjw0pA0', 'QJZsGPRqM-9XZYPkJMrkJ', 1673470820000.0, 'Realize this may be a non-desirable can of worms, but perhaps for an allowlist, rather than asserting, replicache could just perform a standardized encoding/decoding?', 'jesseditson');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('mQHJVx-iSanGfEYrm2sry', 'QJZsGPRqM-9XZYPkJMrkJ', 1673470937000.0, '> Realize this may be a non-desirable can of worms, but perhaps for an allowlist, rather than asserting, replicache could just perform a standardized encoding/decoding?

We had this but it is too expensive. Instead we try to assert in debug mode and do nothing in release mode.
', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('QHaUqLoCKnSsa7ZTkVn_T', 'uKGqplhXFnm_N7UWmi_mL', 1673447801000.0, 'Looking at Figma WS network request. They do not use `Sec-WebSocket-Protocol`', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('DxtQou1RcpTeA48u8asxC', 'uKGqplhXFnm_N7UWmi_mL', 1673453586000.0, 'yes for sure, auth via the header like we do seems potentially a contributor: https://github.com/rocicorp/mono/issues/198. since you have more info here i will close that one. ', 'phritz');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('ATDmE5af_2pnLMhIpm9jM', 'uKGqplhXFnm_N7UWmi_mL', 1673453707000.0, 'hehe, like minds think alike', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('kn89R9vjNCrYe-_dwGZ5U', 'uKGqplhXFnm_N7UWmi_mL', 1673453709000.0, 'Note the other issue suggests we should wait until we can measure the impact of any change here before making it (ie, after rocicorp/reflect-server#254)', 'phritz');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('rdbxDmAIKGEzJrBhP-xF2', 'uKGqplhXFnm_N7UWmi_mL', 1675861935000.0, 'Some background reading related to how auth works

https://www.notion.so/replicache/Invalidating-Auth-732e9f9abb6a4806b5461c87dfde580f
https://www.notion.so/replicache/Reflect-Authentication-Authorization-and-Security-bf2b1a2e31844efca73c4aa453fdecdb', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('7rTDB82dow3OgTOIFbsYf', 'uKGqplhXFnm_N7UWmi_mL', 1677697408000.0, 'I don''t think this project itself should be part of the beta, rather "fix connectivity" should be and if this is the solution so be it.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('mUpkXGycjX_KWFYeTPiya', 'Rohqczqcb8_uiuhA568rn', 1677698594000.0, 'We will not get kicked out of bed for this. Feel free to fix if it bothers you, but definitely not beta blocker.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('SXFYrKJhub_lzwpGsFFHX', 'pWlTHTkynqjd5gmljVESD', 1673398046000.0, 'I think we should look at this this week and triage it.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('Fk4j6pgTA1OZu8MIP_tj3', 'pWlTHTkynqjd5gmljVESD', 1684746081000.0, 'This is probably `sizeOfValue` that you''re working on current right @arv ?', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('kgkVRzWDnInjd_VVGIKbk', 'pWlTHTkynqjd5gmljVESD', 1684746615000.0, 'Most likely due to the fact that we were computing the size of large Uint8Array (as a Record) in every getNode.', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('j7rawmC6aZOyJJnI9TLKl', 'GpU_EDkk_C4jBuucWoRYZ', 1673354131000.0, 'We need to ask ourselves: Who are these errors for?

In the sample above it looks like our code is not handling the closed state correctly during ClientGC.', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('VReFfm_eCG-5NBzvKVFlj', 'GpU_EDkk_C4jBuucWoRYZ', 1675759371000.0, 'The thing fritz referenced above looks like an actual error an engineer should look like to me.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('6nEriraiplE8cDiKOaVZx', 'TApJHYop1jupTX3YsgT10', 1673055023000.0, 'Although I can trivially do myself too, mainly just writing down to remember.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('FcUbGDvjPT9dae-8QxK8V', '2o486QHPAKSkPKEp09j2c', 1684746121000.0, 'Lol @grgbkr ', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('g5kBIp9genz03i4FVxnZO', 'xQmT4UvY5YlFGXX5XfKiW', 1672952034000.0, 'We may want to add a "chunk event" logging mode to debug this.  This involves events across tabs (i.e. the tab that gc''d the chunk may not be the tab that tries to read the missing chunk). We could keep a separate db that we use in this mode that is a map from chunk id to tuples of (time, tab, action={WRITTEN, GCD, etc}) or similar. ', 'grgbkr');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('C99eVy_Gzm04C7rxIgxtX', 'xQmT4UvY5YlFGXX5XfKiW', 1672952842000.0, 'note this error co-occurred with another error "Error during refresh from storage Error: invalid value" which makes it sound like we wrote a chunk that we could not read out. 

![image (2)](https://user-images.githubusercontent.com/157153/210879945-7a257e6e-fc59-4eee-b4f8-5f3d50f998c7.png)

one more bit is that it remained broken across refreshes
', 'phritz');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('qVve5PDBvrStaoAw7OH70', 'xQmT4UvY5YlFGXX5XfKiW', 1681242528000.0, 'Closing this in favor of #434', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('MizJnGHySviX96a7k9JhL', 'wRqEqQYUCjqI022sBVO8i', 1673453612000.0, 'closing as dup of https://github.com/rocicorp/mono/issues/193 ', 'phritz');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('xvxr4teIy6tvhpFvPpPI3', 'hHwAQIHg-U3VXQAU7DJIF', 1672900094000.0, 'Yeah I suppose the version in `connect` should mean the version of the entire protocol that flows over that socket, right? That''s nice.

I think there are two potential behaviors for a version mismatch for connect:

1. Server rejects old protocols (page must reload with new client). This is nice because server doesn''t have to support old protocols.
2. Server supports old protocols

(1) is only really an option for on-prem, though.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('1CkC07YKDKJ0NNAT2InMR', 'hHwAQIHg-U3VXQAU7DJIF', 1675934424000.0, 'This is a duplicate of rocicorp/mono#183 ', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('UYh6cWe97FS-D8be43pZz', 'hHwAQIHg-U3VXQAU7DJIF', 1675934534000.0, 'Oh wait, `createRoom` still needs to get done.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('M_G-lsyDX5MmVER0yJvxd', 'dVgRDv1EGyv1GI5KJSXhe', 1674101793000.0, 'See also comments in https://github.com/rocicorp/reflect/pull/74.', 'phritz');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('uRTPrF1LPKdBSgJ41MS17', 'dVgRDv1EGyv1GI5KJSXhe', 1674162530000.0, 'I wonder if as part of this we should add an auth-over-ws path to client and server and have a switch in the client (eg in options) that can be set to toggle back and forth. That way we could get monday to try it out without having to get them a new binary...', 'phritz');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('IaBh-t2WOT75wkCO-aAFa', 'dVgRDv1EGyv1GI5KJSXhe', 1677698094000.0, 'Almost there. In order to call this done, let''s just do these last two:

<img width="883" alt="Screen Shot 2023-03-01 at 9 14 25 AM" src="https://user-images.githubusercontent.com/80388/222241239-24fa111e-bdb4-437a-8aa4-026a753833de.png">

The other unchecked items are represented by other bugs which are tracked and prioritized separately.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('sWhmSwqI1L1pmUBl6lUwa', 'dVgRDv1EGyv1GI5KJSXhe', 1677698135000.0, 'Removing P1 as remaining things not represented by other bugs are lower priority but we should still do for beta.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('yGKulzjBzGcZvaUCUo-lx', 'dVgRDv1EGyv1GI5KJSXhe', 1679343338000.0, '@cesara is this done now?', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('4cbMK0U63PtyQlFDQeUvu', 'dVgRDv1EGyv1GI5KJSXhe', 1679343887000.0, '> Almost there. In order to call this done, let''s just do these last two:
> 
> <img width="883" alt="Screen Shot 2023-03-01 at 9 14 25 AM" src="https://user-images.githubusercontent.com/80388/222241239-24fa111e-bdb4-437a-8aa4-026a753833de.png">
> 
> The other unchecked items are represented by other bugs which are tracked and prioritized separately.

This task is finished according to the above. Maybe we open a separate issue for any remaining items.', 'cesara');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('E2lb2hBTokz8hLOWOBUc7', 'KzIYXTz64Zx0_fyMDywos', 1675146857000.0, 'redundant ', 'phritz');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('FQ0aDlTWzeJYGC-PGC96U', 'nlwrOyWJN0vyNMRf86PQ8', 1675129249000.0, '@arv i think you fixed this yes?', 'phritz');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('G9RmXNl_29DRqYNME2I7a', 'nlwrOyWJN0vyNMRf86PQ8', 1675153969000.0, 'Sure. I have a plan to make clientID sync but it will have to wait for DD31
', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('cFm5nKCzNr_UqF73auB9D', '-7BhUMh2YaWRHglsa2qVX', 1673450581000.0, 'DataDogBrowserLogSink is clearly not doing what I want:

```js
  const sink = new DataDogBrowserLogSink();
  sink.log("info", "test info", { a: 42 });
```

<img width="618" alt="Screenshot 2023-01-11 at 16 21 22" src="https://user-images.githubusercontent.com/45845/211844867-215771e9-14bb-43df-b945-ac66e821c83a.png">
', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('XGDem8SRBshpsVUBd8NW6', '-7BhUMh2YaWRHglsa2qVX', 1673521974000.0, 'We should figure out where these DataDog loggers should live. Is it a new npm package?', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('ExDj52mSzO2beuFn7_6P8', '-7BhUMh2YaWRHglsa2qVX', 1673537935000.0, '> We should figure out where these DataDog loggers should live. Is it a new npm package?

I will have some datadog metric tools that I want to be shared across reflect or customer app and reflect-server. They don''t have to go in the same place, but certainly would naturally fit into a datadog-tools or similar repo. 

', 'phritz');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('c6fNZVqpv7H8kaZSZE4ab', '-7BhUMh2YaWRHglsa2qVX', 1673550677000.0, 'Thank you for doing this, all these little quality of life things really
help when debugging these production issues. Here is another one: in this screen shot, `doID` and `req` are our contextual attributes. But aren''t those supposed to show up in DD as "event attributes" so that we can filter by them using the first-class UI? It''s weird they show up as part of the log message string, I don''t think it''s intentional.

<img width="774" alt="Screen Shot 2023-01-12 at 9 07 49 AM" src="https://user-images.githubusercontent.com/80388/212160242-75b314e1-d759-4c95-8544-62b2d9d855c7.png">

https://docs.datadoghq.com/logs/log_configuration/parsing/?tab=matchers says:

<img width="828" alt="Screen Shot 2023-01-12 at 9 10 53 AM" src="https://user-images.githubusercontent.com/80388/212159217-5c5150c6-b8b0-4c91-9122-023efb928150.png">

They don''t have a picture of what the parsing is supposed to result in, but I don''t think the current behavior is right.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('TvhdKh6VVlR2PzJUmFJ7_', '-7BhUMh2YaWRHglsa2qVX', 1673552877000.0, 'There is also this old issue: https://github.com/rocicorp/replicache/issues/991', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('8JJas7RoiR38OCGFim6Fv', '-7BhUMh2YaWRHglsa2qVX', 1673557588000.0, '> doID and req are our contextual attributes. But aren''t those supposed to show up in DD as "event attributes" 

I don''t recall if we taught datadog to grok doID, requestID and similar? If we did, maybe the pipeline broke or needs to be updated. In any case, there is a lot of related work we have to make reflect client and server easier to understand, including teaching DD about these and potentially other fields. I added a new comment to this issue and included this task there so we don''t forget: https://github.com/rocicorp/reflect-server/issues/60#issuecomment-1380991408', 'phritz');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('Si_S2ejO8_Qajc8iSxzwG', '-7BhUMh2YaWRHglsa2qVX', 1673602817000.0, 'We never connected the dots between LogContext contexts and DD attributes', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('TeUXCZxJpWZhTBQbxzY9o', '-7BhUMh2YaWRHglsa2qVX', 1673948373000.0, 'Closing this in favor of https://github.com/rocicorp/mono/issues/191', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('o21r6J8JsajUwrshlZmvd', 'yi2sfNm8hnpVw5QzGLhhR', 1674058631000.0, 'Closing this.

We should move the LogContext addContext handlers to the router middleware but that can be done in the future.', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('C7DsTHiqVsKLCwLQx_6sp', 'VWXO0N4fdf-KhdXcsXD4T', 1672894848000.0, 'Should probably also address https://github.com/rocicorp/mono/issues/199 as part of this.', 'phritz');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('X7RECQvumi7990u6XdY_C', 'VWXO0N4fdf-KhdXcsXD4T', 1675934579000.0, 'Is this done @cesara ?', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('K03_R-JxzEyHP-uiTZzOD', 'VWXO0N4fdf-KhdXcsXD4T', 1675952330000.0, 'yes,  the top two are done. I believe fritz did the cors'' portion. I think Erik addressed rocicorp/mono#199. ', 'cesara');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('OLvYVQluTeSY69RNYQsaj', 'VWXO0N4fdf-KhdXcsXD4T', 1675958545000.0, 'I didn''t do rocicorp/mono#199. I was looking into it as part of rocicorp/mono#193 ', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('gA59jnCXkzRr0DhCaCD7X', 'VWXO0N4fdf-KhdXcsXD4T', 1675974632000.0, 'OK well we already have a bug for rocicorp/mono#199 so closing this.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('AsnZoOvcnivVq2NndkWmB', 'f32CMpps9G8iPf1V5jL9e', 1672869220000.0, 'See rocicorp/mono#210. From latest spreadsheet, I think that disconnect on blur and reconnect on focus should be sufficient. That will be a better ux too.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('2kWl0gYfh6aTrVAw1pBQi', 'f32CMpps9G8iPf1V5jL9e', 1672935359000.0, 'Disconnecting on `blur` seems like the wrong signal. Disconnection on [visibilitychange](https://developer.mozilla.org/en-US/docs/Web/API/Document/visibilitychange_event) to hidden seems like a more reasonable signal. `blur` seems wrong because having two tabs side by side would then disconnect one of those tabs.

We should also probably wait 10s (or 5s, or whatever) before doing this to allow cycling through tabs', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('EE9VtjUUeN4cDL3q8Yi1I', 'f32CMpps9G8iPf1V5jL9e', 1675934835000.0, 'This bug is a duplicate, mostly of rocicorp/mono#180 ', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('d9BCKLTbXnhdCOwHdcIN6', 'ABtgmV-PsHQxnlSahn33p', 1677705372000.0, 'I think due to typescript / intellisense we could actually get away with *not* having reference docs ðŸ¤¯ for playable beta.

But we do need a one-pager getting started doc.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('IAbIGA4RZqvEp6UhHkWd0', 'Us6h6NpZMZ5xUJtKVOIaY', 1672869361000.0, '> I think we should at the very least set allowConcurrency to false in the auth DO, and get rid of the manual locking there.

I don''t think it''s that simple. There are places that an input gate would not help us because there is a race with incoming requests while no storage operation is in flight, eg when updating a room. If someone wants to rip the lock out for reasons of perceived complexity then they need to do a careful analysis of what the locks are doing and determine which can be replaced.

However, personally, I''m *far* more comfortable reasoning about traditional locks than I am about the input gate. It is totally not clear to me that we will have fewer bugs and spend less time on the code if we try to eliminate locking in favor of gates. 

Greg and I had a discussion about the locking in the DOs and while neither of us was thrilled with having it in there we were convinced we needed it for correctness and that if it became an actual problem for throughput or complexity then we would revisit. So the current state is intentional, not accidental. ', 'phritz');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('24S90qD6r3bHaCilmOoFY', 'Us6h6NpZMZ5xUJtKVOIaY', 1672872718000.0, 'OK this makes sense. I also have an easier time reasoning about the locking. Nevermind this.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('vzWBs_vgi9ouz8vn0NTrq', 'gv4JD5Tag5mehVMUhGQ09', 1671673820000.0, 'We should definitely:

- Add an "idle room" metric -- length of time a room is running, but not doing anything (no mutations coming in)

Ideas for making metric go down:

1. Clients disconnect themselves on tab hide seems pretty simple, but not completely bulletproof (clients could miss the event but stay connected somehow)
2. Clients disconnect themselves if they''ve not sent any mutations (or received pokes?) for awhile
3. Server disconnect all clients when it hasn''t done anything except process pings for 5m is interesting, but we''d have to make the clients not to immediately try to reconnect themselves!', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('GAebaq4QYgSiNAyBT1-cP', 'gv4JD5Tag5mehVMUhGQ09', 1675935364000.0, 'So many stupid duplicate bugs. rocicorp/mono#180 ', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('E5jZYN0KvtUgFgCrPhPyd', 'gv4JD5Tag5mehVMUhGQ09', 1676297289000.0, 'Reopeneing due to:

- Clients disconnect themselves if they''ve not sent any mutations (or received pokes?) for awhile
- Server disconnect all clients when it hasn''t done anything except process pings for 5m is interesting, but we''d have to make the clients not to immediately try to reconnect themselves!', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('L99iUz07siNeuOA_qZDXx', 'PHPgfv6OYAOSsNEjhl64x', 1671672120000.0, 'This seems related to rocicorp/mono#276 and rocicorp/mono#225 but not exactly the same. I feel like the immediate tasks are:

- confirm this metric by looking at datadog data -- can we find examples of clients that took > 1min to connect in that data?
- debug those cases and see if we can determine something that went wrong', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('y50yWKCO2cUx8VQ9KyMX4', 'PHPgfv6OYAOSsNEjhl64x', 1671691523000.0, 'I do find a few examples of this. From a dataset in the neighborhood of 2022-12-15.

This client took 9m to startup:

<img width="1595" alt="Screen Shot 2022-12-21 at 8 22 52 PM" src="https://user-images.githubusercontent.com/80388/209070629-51ea203b-38a2-4e3d-a3f7-d432824a4ac1.png">

However we cannot rule out that this client was legitimately offline because at this time we didn''t have the `isOnline` log line, and the browser was open for awhile before. Perhaps network blip or something:

<img width="1482" alt="Screen Shot 2022-12-21 at 8 25 23 PM" src="https://user-images.githubusercontent.com/80388/209071022-5e4cfb6b-7e57-4943-b3ab-7a361714dd5e.png">

Here is a different one that was offline for 2m:

<img width="1593" alt="Screen Shot 2022-12-21 at 8 31 29 PM" src="https://user-images.githubusercontent.com/80388/209071772-d9ca71e0-ed4f-43f1-82f1-4e7f5148f9aa.png">

I don''t see anything wrong with these logs. It just seems to take awhile for the request to make it to the server sometimes. These are the two most egregious examples from this log set, but that same roomID `G1LcejpAqDj2vE4voT2Cgzd-2Xi4lX_x` from last example has two other cases where client took 40s to connect:

<img width="661" alt="Screen Shot 2022-12-21 at 8 35 08 PM" src="https://user-images.githubusercontent.com/80388/209072274-66158c7f-76cc-43d9-bbc8-194bad5770ac.png">

Interestingly we also see that same HK user from rocicorp/mono#276 (room `ka8iwY3wtfbQ4M6t8KYPWA8bQ4f9xBWd`) show up here with some connections that take ~14s:

<img width="693" alt="Screen Shot 2022-12-21 at 8 40 05 PM" src="https://user-images.githubusercontent.com/80388/209073077-51c3adb3-33ec-42ec-8de5-26cba782f131.png">

Another really interesting pattern is that these logs span: `2022-12-15T21:29:33.124Z` to `2022-12-16T02:29:57.425Z`, but the longest example of connections seem to cluster around `~2022-12-15T22:00:00Z`:

<img width="609" alt="Screen Shot 2022-12-21 at 8 42 11 PM" src="https://user-images.githubusercontent.com/80388/209073398-7a2c42e4-f262-43f6-9329-90e6f6ed5829.png">

Perhaps something was pushed around that time and all the DOs restarted?

There were a lot of server starts then, but also other times, but we don''t see the long connection times at other places where server starts were higher:

<img width="1604" alt="Screen Shot 2022-12-21 at 8 44 09 PM" src="https://user-images.githubusercontent.com/80388/209073674-0497e674-c261-4ac9-a051-fdcc549c141c.png">

Unclear. Perhaps something was happening at Cloudflare at that time.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('vVtQ0eDory2DCzRxuMCOQ', 'PHPgfv6OYAOSsNEjhl64x', 1671692279000.0, 'However, from this sample, the percentages are far lower than Noam observed:

- 8/934 (0.8%) > 10s
- 2/934 (0.2%) > 60s
- 2/934 (0.2%) > 120s
- 1/934 (0.1%) > 360s

I think the next step on this bug is:

0. We need to include the clientID in every message up to the server. The fact that the "connecting..." message sometimes doesn''t have the clientID makes this analysis difficult.
1. @noamackerman - can you check the graph on Cloudflare under workers > metrics > summary and see if you can see any increase in errors around 2022-12-15T22:00:00Z ?
2. I should do the same log analysis on the data that overlaps where @noamackerman saw lots of long connections and see if I see the same thing.
3. We need a metric on datadog to track how long connections take', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('ZJrBe6JoA65TuafH7k-ZW', 'PHPgfv6OYAOSsNEjhl64x', 1673054090000.0, 'I cannot run the analysis on the log data ourselves because we don''t have a join key between the `Connecting...` and `Connected` log line. I tried just finding the "Connecting..." log line for the same room before each "Connected" but sometimes it appears that the "Connected" line gets logged with a slightly lower timestamp than the Connecting" line. I''m not sure why.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('BwV_4cL6L4FPrLwKze0U0', 'PHPgfv6OYAOSsNEjhl64x', 1673055039000.0, 'Need to fix logging to make this analysis possible: https://github.com/rocicorp/mono/issues/196', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('GWn6hFqhF_M1trAdX5z1D', 'PHPgfv6OYAOSsNEjhl64x', 1683763876000.0, 'Closing this now as our metrics don''t support it.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('Og7WQr9wlRc8tVtAJQCm2', 'kyIsazaqxZ0MaMc2BPUNb', 1672874329000.0, 'We should try it out and make sure that it catches:

(1) unhandled exceptions
(2) ooms (just make a bad worker that allocates forever)
(3) kind of curious what happens if you make a worker that enters a busy loop. I assume they eventually kill it. Does that show up in the logpush log?', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('6wtnBBhHGrMhU3PQL_n0M', 'kyIsazaqxZ0MaMc2BPUNb', 1672887885000.0, 'the process of adding log collection to our reflect worker and DOs is kind of lost in the mists of time for me. @aboodman @arv i think this means we could replace https://github.com/rocicorp/reflect-server/blob/578c3ab3f83dde7fc96638721cd4eb99f70d7074/src/util/datadog-log-sink.ts with the built-in CF logpush to datadog? 

also: there must''ve been some reason that we didn''t use the node datadog library (https://github.com/DataDog/datadog-api-client-typescript) on the server? 

asking because we want to start collecting custom metrics on the client and server and https://www.npmjs.com/package/datadog-metrics on the server would be less work than rolling our own API client. ', 'phritz');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('V_BAuQqRe30WOUasLK3BS', 'kyIsazaqxZ0MaMc2BPUNb', 1672888710000.0, 'Yeah they didn''t use to have logpush for workers so we had to do the in-process thing.

I believe that the datadog client library didn''t work inside the worker env for some reason.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('3wR8zHAoj31sGczMEmDD_', 'kyIsazaqxZ0MaMc2BPUNb', 1672890144000.0, 'And for browser-side logging, we didn''t use https://github.com/DataDog/browser-sdk maybe because... too heavyweight? 

And didn''t use https://github.com/DataDog/datadog-api-client-typescript because maybe it assumes node and is incompatible with running in an actual browser? ', 'phritz');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('h86hyGWHlMBeznbz--ces', 'kyIsazaqxZ0MaMc2BPUNb', 1672897858000.0, 'For the browser we *do* use their library:
https://github.com/rocicorp/replidraw-do/blob/main/src/frontend/data-dog-browser-log-sink.tsx#L2

On Wed, Jan 4, 2023 at 5:42 PM Phritz ***@***.***> wrote:

> And for browser-side logging, we didn''t use
> https://github.com/DataDog/browser-sdk maybe because... too heavyweight?
>
> And didn''t use https://github.com/DataDog/datadog-api-client-typescript
> because maybe it assumes node and is incompatible with running in an actual
> browser?
>
> â€”
> Reply to this email directly, view it on GitHub
> <https://github.com/rocicorp/mono/issues/212>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AAATUBHN3YTBBF7OK7JJKPDWQY7KXANCNFSM6AAAAAATGHV3WU>
> .
> You are receiving this because you were mentioned.Message ID:
> ***@***.***>
>
', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('hmL8y58JaR4EyKfYSKQ2x', 'kyIsazaqxZ0MaMc2BPUNb', 1672914505000.0, '@phritz: IIRC, CF workers could not use the node library due to dependencies on node specific modules.', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('8XiujoW8Sp5LHL8-bcAU6', 'kyIsazaqxZ0MaMc2BPUNb', 1673132434000.0, 'FYI @aboodman logpush is [only available for enterprise customers](https://developers.cloudflare.com/logs/about). While that''s fine for us, doubt it''s going to be something that a tire kicker or small scale customer is going to be excited about needing.', 'phritz');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('a3HI53Ev_AgKoLAwRdeiO', 'kyIsazaqxZ0MaMc2BPUNb', 1673133328000.0, 'Darn. I''m very concerned about not having visibility into crashes in
datadog or metrics. This seems like something we need and wrapping all the
top level entrypoints doesn''t seem like a substitute to me because I worry
about missing events like oom restarts.

Thankfully we *will* see such events from client pov (in some ways even
better) once we have client-side metrics. Hm.

On Sat, Jan 7, 2023 at 1:00 PM Phritz ***@***.***> wrote:

> FYI @aboodman <https://github.com/aboodman> logpush is only available for
> enterprise customers <https://developers.cloudflare.com/logs/about>.
> While that''s fine for us, doubt it''s going to be something that a tire
> kicker or small scale customer is going to be excited about needing.
>
> â€”
> Reply to this email directly, view it on GitHub
> <https://github.com/rocicorp/mono/issues/212>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AAATUBE37Z3AJGLILQLQ64DWRHYR3ANCNFSM6AAAAAATGHV3WU>
> .
> You are receiving this because you were mentioned.Message ID:
> ***@***.***>
>
', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('CHu6ysBpbgXhIwOGQJT6C', 'kyIsazaqxZ0MaMc2BPUNb', 1673134266000.0, 'Just kidding! I asked for clarification in their discord. Logpush for *workers* is generally available, it''s logpush for other stuff (pages etc I guess) that is enterprise. 

But again it''s not a panacea -- there are a zillion reasons why a DO could be shut down with no exception delivered. We don''t even know if the exceptions that you are worried about (OOM, etc) are reliably delivered. I share the top level concern tho, and yes we should use logpush, but with logpush we still have scope for the exactly same problem, namely missing events like restarts. We need some metrics to actually understand if we are.', 'phritz');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('A_hVh7MT3yTqSp2uiu7V3', 'ZVMaUpEXC2DMc58HGmCmN', 1672991478000.0, 'OK this other activity at same time was different user. Noam gave the user who could not connect''s account ID and it''s different than the concurrent activity. So my suspicion isn''t supported.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('jDz7VK3J98BCFe1rzBF_H', 'D0bqjZ7ARO4z7oKD-XOav', 1672875280000.0, 'Note: might want to do https://github.com/rocicorp/reflect-server/issues/153 as it will have impact on the error messages. Also should probably wait for https://github.com/rocicorp/mono/issues/205 to land before doing this one because it''ll be easier to work with a single routing implementation.', 'phritz');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('Tc3YmCxs6PFC2tblZb8Zh', 'D0bqjZ7ARO4z7oKD-XOav', 1673468175000.0, 'Somehow we allowed @jesseditson to try and tx.put a `UInt8Array`: https://rocicorp.slack.com/archives/C013XFG80JC/p1673465473373249?thread_ts=1673055269.485049&cid=C013XFG80JC. This should not be allowed. Runtime validation should have caught this at least in dev mode.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('SePJDmQe3qzWZSH1QW3li', 'D0bqjZ7ARO4z7oKD-XOav', 1673470847000.0, 'Depends on rocicorp/mono#75', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('paBMakIZoGhLC1SNqZDiE', 'D0bqjZ7ARO4z7oKD-XOav', 1673988965000.0, 'After DD31', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('UPWgGbPY3ZGAbL1P6x3_6', 'D0bqjZ7ARO4z7oKD-XOav', 1678376872000.0, 'Blocked by #307', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('5iIoXb_ynbz4oYPrTUAHZ', 'nAUaIawWukZ0ILh81OI5o', 1684746367000.0, 'This bug is a subset of #173 ', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('86ojZ_-FRf0GrThTEiOWr', 'QJX3WHGHFxj4Jd5lMCV_K', 1677009186000.0, 'You should be able to just say:

```ts
new Reflect({
  userID,
  roomID,
  auth,
  jurisdiction,
});
```

... and away you go, without needing to first call the `createRoom` endpoint.

Right now, `/connect` returns an error if room hasn''t been created already. This implies either client should call `createRoom` before every `connect` or else `connect` should carry enough information so that it can internally `/createRoom` first.

I favor the latter for latency reasons -- one roundtrip to connect vs two.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('VFFsCJho3ZqJv5EkM0c-3', 'QJX3WHGHFxj4Jd5lMCV_K', 1677668244000.0, 'There is a subtle and unlikely edge case if we bring this back, which I''m treating as a separate bug. See rocicorp/mono#232 .', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('H461T9RRi78xNlb7vT_z6', 'DoPst6sihTnuPFQJyoOQs', 1677704576000.0, 'Closing in favor of #76 now that we are in monorepo! yay!', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('vqVXrw6zqopbbV4zObT4J', '4YoRIX20muvgX9UTonFNY', 1672874902000.0, 'This needs API design. The obvious thing is a field like:

```
environment: "client"|"server"
```

But we may also want to distinguish rebases and I''m not sure how to do that without making it a lot more obscure.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('UrWibSlLgnwnyADGY3zAe', '4YoRIX20muvgX9UTonFNY', 1677784113000.0, 'Thinking about this more , let''s not overload the noun ''environment''. Let''s use:

```ts
location: "client"|"server",
```

In the future we can also add:

```ts
reason: "initial"|"authoritative"|"rebase"
```

That way if you only care about client vs server (common) you just use `location`. If for some reason you want to know the difference btwn initial and rebase, you can do that too.

@jesseditson - you have been using this part of the API, opinion on these two?', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('hSxY4VjkaPlgY2_W9sSPQ', '4YoRIX20muvgX9UTonFNY', 1677784295000.0, 'I think we should make this change in Replicache. In Replicache we will only set `client` (and `initial`|`rebase`) but in Reflect we will also set `server` and `authoritative`.

Later, we could aditionally modify https://github.com/rocicorp/replicache-transaction to specify `server` and `authoritative` as a bonus.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('XutqBO38Ib1v--ptOfdUZ', '4YoRIX20muvgX9UTonFNY', 1677784340000.0, '@cesara I think you can take this one. Please do not prioritize the work for `replicache-transaction`. Let''s just do that if there''s time.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('KapxylChryKDYewBo-hS_', '4YoRIX20muvgX9UTonFNY', 1677790101000.0, '`location` would work for me, I''d be a bit concerned about people using this to detect, for instance, browser features - which wouldn''t be a good long-term design. Perhaps could imply a more specific meaning using:

```
context: "client" | "worker"
```

to differentiate between this and a physical location and ambiguity between reflect workers and customer servers (AFAIK there''s no reason not to think someone will run a reflect client on a server in the future, or that we could provide information related to a physical location to help with latency compensation or something).

That''s a nit. I''m good with it landing as described above, and it would make sense for my use case.', 'jesseditson');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('ZA_MShf7gUNhoxSxUq4-u', '4YoRIX20muvgX9UTonFNY', 1677829991000.0, 'I was trying to avoid the generic words _environment_ or _context_, but I give up. Let''s go back to:

```ts
environment: "client"|"server"
reason: "initial"|"authoritative"|"rebase"
```', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('Cj5tyLvU3yiN8eoPMjrQQ', 't8JkuDLqepHnD-M9px6pc', 1677781526000.0, 'Uh, actually this is already done I think. We wrap Replicache, not inherit it. So we didn''t automatically inherit indexes. My bad!', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('mkjZoWUErh3GUItjSEt1z', 'jKyY9OC7B6RkQ2sJG9Dah', 1672737797000.0, 'Fixed with rocicorp/reflect-server#241', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('7xqbQnXYkhm3YWXffDsYE', 'qjL0v-_SW6cpPNEduJnCi', 1670962793000.0, 'Probably need to accommodate not just backoff but also:
- does the reconnect logic make sense? need something similar to what arv added to replicache. 
- does the logging make sense?
- what about if DD31 is in and the user is expected to be offline for a long time?', 'phritz');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('-9cTTlVhw27dDDXRK_hRD', 'qjL0v-_SW6cpPNEduJnCi', 1670982311000.0, '`onOnlineChange` should only fire when *reconnect* fails, not just because of passing socket drops.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('GTSjwVtybuAf73AFhIcKZ', 'qjL0v-_SW6cpPNEduJnCi', 1671059598000.0, 'ensure that anything that throws (eg, connect) logs an error (eg top-ish level error handling)', 'phritz');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('gz9Xh-YRG36D_JZtoNmgw', 'qjL0v-_SW6cpPNEduJnCi', 1671609712000.0, '* disconnect on tab blur
* reconnect on tab focus', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('xZJ3ExpSyLBbTqknoBss0', 'qjL0v-_SW6cpPNEduJnCi', 1671644819000.0, '> disconnect on tab blur
> reconnect on tab focus

Guess it makes sense for this issue to be the holistic set of things we need to do around dis/connect. So also for consideration is disconnecting from an inactive room and having a mechanism to reconnect when there is activity.', 'phritz');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('4EHu8BqeGFrKAjS5iIjsh', 'qjL0v-_SW6cpPNEduJnCi', 1672882887000.0, 'Closing in favor of https://github.com/rocicorp/mono/issues/200 which has more detail', 'phritz');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('W5vESqsKEqKSSPUonFG79', 'CDe4S8jYbbE_YjPDjCJJg', 1677670788000.0, 'Code is gone', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('RiMSQfy9AihvuE7knw2eQ', 'EfDktb1UjMH7V8A-uVj9n', 1676316403000.0, 'I can imagine doing this at least two ways:

* the easy hacky way
* the good way :)

The easy way is to provide some kind of language (jq?) to select keys to sync at any moment in time. This filter can be changed at runtime.

The good way is to provide a predicate function that does the same.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('BvDbXtXmplqu12_yaDuWu', 'RynRjG25I_BAM3DJOqDmI', 1670644492000.0, 'I think this will involve cleaning up the storage abstractions. Right now, `Replicache` directly uses IDB for the databases db. We need it to use supplied kv implementation instead, in the case that e.g., the environment doesn''t support idb (react native has this problem).

I think we need to introduce `kv.Factory` and `experimentalKVStore` => `storeFactory` or something. Then we would create an instance of the kv.store for the databases database, and also one for the storage backing the perdag.

Memory ( rocicorp/mono#43 ) would also implement the same interface.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('X056rpYl6tndH9wYyPjlN', 'RynRjG25I_BAM3DJOqDmI', 1675958063000.0, '`experimentalKVStore` is flawed. We need to use a factory to support the database of databases.', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('tPbWpYITgCYSWp83iOFo2', 'RynRjG25I_BAM3DJOqDmI', 1675958215000.0, 'With `experimentalCreateKVStore` we might want to export the MemoryStore or provide a separate open source repo for it since it is non trivial with the locking.', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('UcyWtz8w_R5tqruzO164n', 'RynRjG25I_BAM3DJOqDmI', 1676200176000.0, '@arv assuming you are good with my last two PRs, I believe this can be de-experimentalified now, at least on trunk.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('qGyQx2mld5WMaNjp9dsAp', 'RynRjG25I_BAM3DJOqDmI', 1676281127000.0, 'Proposed API:

```ts
store?: (name: string) => Store | undefined;
``` 

@aboodman suggested also allowing `''memory''` as a short for `name => new MemStore(name)`. I do feel like maybe that is better covered by `persistence?: boolean` because if we know we are not using persistence the strategy changes a bit. No need for  LazyStore and no need for a lot of the background processes.

Also rocicorp/mono#43', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('tz_tWh1DTMc4k3z1GYhFO', 'RynRjG25I_BAM3DJOqDmI', 1676282213000.0, 'Let''s wait and see how reflect develops then. No hurry here.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('aFc86DeFFZ9WEqOKAyFfJ', 'VfuNdbCGYyHDLzfNsIqxU', 1670573422000.0, 'I don''t think we need to use workspaces. All that is really needed is to:
-  change the name in package.json 
- `npm install` to update package-lock.json
- Change the build rule. Can be done in code by checking the name in package.json
- `npm publish`
- Change the name back

I guess this can be done on a branch or by using a script?', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('rEgyJ53mJbQ3N0d5nT5oS', 'VfuNdbCGYyHDLzfNsIqxU', 1670573715000.0, 'Oh yeah good idea, that''s easy enough. Thanks.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('Q7rRXl2nhK8igTdXf_Ilj', 'VfuNdbCGYyHDLzfNsIqxU', 1670574190000.0, 'I think a branch is the simplest solution. The only thing that changes are the name field in package.json and package-lock.json which should be easy to maintain.', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('_cWDkGpazwTnGb5tVCMAr', 'g-ved13slCh2SylOTH-Eh', 1675935496000.0, 'Duplicate of rocicorp/mono#208 ', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('DjsE-uWvWigadURT5ORD6', '89oMKPoYWax07t24iwPP9', 1673991910000.0, 'This should happen as part of rocicorp/mono#200 ', 'phritz');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('naOTaO-kyfYysAtK8fhTm', '89oMKPoYWax07t24iwPP9', 1675933426000.0, 'This is done.', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('XNaFLkbxxy-p1TwSwzEQA', 'deF6IymyIRJjUt_G0bKPa', 1670960823000.0, 'also we should clean up and upintegrate these log line changes: https://github.com/rocicorp/reflect-server/pull/213', 'phritz');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('RDmBCrro1jmGJjAd5_67U', 'deF6IymyIRJjUt_G0bKPa', 1672875147000.0, 'This issue should also include looking at all commits on early christmas and if they are logging improvements integrate them into main.', 'phritz');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('pDPc9aVs31QgQ9CehPsFk', 'deF6IymyIRJjUt_G0bKPa', 1673067889000.0, 'We should have a structured error sent down over the ws for each of these errors that originate on the server: https://www.notion.so/replicache/metrics-0b848461ffce4312bdb645d65adb7da2#4a97339d439a414eb0090270d53c4706

We''ll need them to add the metrics described in that doc. ', 'phritz');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('Ht4Y35ClCQt1x_RcvCHtF', 'deF6IymyIRJjUt_G0bKPa', 1673715650000.0, 'The ''make errors structured'' part of this should also include https://github.com/rocicorp/reflect-server/blob/520f1a2f5ed35a5e6b5dc5eb018995b20d9ffeb4/src/server/auth-invalidate.ts#L12 which should be differentiated from the initial auth failure. ', 'phritz');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('ShaxZ_SPzwJqU_XKxf6AK', 'deF6IymyIRJjUt_G0bKPa', 1675129229000.0, '@arv i think you can close yes?', 'phritz');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('twNhbtyVNd9rTel_2X23Y', 'sCZFFkpWjtQRxsQiy9jWN', 1677008998000.0, 'On second thought not really a duplicate:

This bug is about changing existing protocol to detect when rooms are reused across instances of the server. Right now this works 99.99% of the time, but it should work all the time. This is a protocol correctness change that is orthogonal from API.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('V8CZwGRoJ24tYTV0Zq6k_', 'sCZFFkpWjtQRxsQiy9jWN', 1678209020000.0, 'BTW I think fritz''s [proposed solution](https://github.com/rocicorp/mono/issues/232#issue-1595660179) 

> having the room choose and persist a random value when it is created and return it to a client the first time the client connects. the client passes this in future connections and if it ever does not match what the server has then the server errors the client out.

only works when rooms are completely lost.

It is also common to just lose state changes for some recent versions.  This is exactly what happens if you start wrangler dev, access an existing room, make some changes, and then restart wrangler dev. 

To address both think we could change cookies to be a {id: uuid, version: number}, and keep a history of cookies on the server. Then when a client with existing state for a room connects, the room-do can validate if the cookie it is connecting with is in its cookie history.
', 'grgbkr');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('3wwAtmI-VdHlAW_0xV6i5', 'sCZFFkpWjtQRxsQiy9jWN', 1678216730000.0, 'Let''s call one running instance of the RoomDO for a given room ID, a _room instance_.

Room versions are monotonically increasing so it''s sufficient to store only the highest version reached for each room instance. If the version in a received cookie is greater than this highest version reached for the corresponding room instance, then the server has lost state and any pending mutations from that client should be dropped (or perhaps replayed, breaking causal consistency).', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('SentcGfDnFT-Ni9wEzJBY', 'sCZFFkpWjtQRxsQiy9jWN', 1678216805000.0, 'Note this bug now relates closely to #330. When #330 is implemented, the occurrences of lost server writes will increase, causing same issue described by https://github.com/rocicorp/mono/issues/232#issuecomment-1458532241.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('_dEnC6vMZbx4XtFWCCkRw', 'sCZFFkpWjtQRxsQiy9jWN', 1678218615000.0, '> Let''s call one running instance of the RoomDO for a given room ID, a _room instance_.
> 
> Room versions are monotonically increasing so it''s sufficient to store only the highest version reached for each room instance. If the version in a received cookie is greater than this highest version reached for the corresponding room instance, then the server has lost state and any pending mutations from that client should be dropped (or perhaps replayed, breaking causal consistency).

But since the version is just a counter, it is possible that state is lost, and then other clients bump the version back up to a higher value.  ', 'grgbkr');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('L42Pwb10-RIwwM9ons5aW', 'sCZFFkpWjtQRxsQiy9jWN', 1678218811000.0, 'Is it possible for that to happen in any other way than the room restarting? It seems like if that can happen it''s a CF bug.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('AVpGzgRls7yb4D8GFh6ii', 'sCZFFkpWjtQRxsQiy9jWN', 1678224497000.0, 'It can happen with
1. dev mode (which loses state when you restart it)
2. the room restarting if the output gate is off, or if we are not flushing writes before sending pokes to clients', 'grgbkr');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('9AzSZbXH79Kx_nO2qsnEN', 'sCZFFkpWjtQRxsQiy9jWN', 1678226629000.0, 'What I''m saying is that each time `RoomDO` starts up it chooses for itself a new unique ID. This is its "room instance ID". So in 1, when you restart, you''d get a new instance ID. Same with 2. I''m probably missing something.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('bEbN0un8bwkOZ8W3SyYdp', 'sCZFFkpWjtQRxsQiy9jWN', 1678227406000.0, 'I see. I was missing that the id changes on restart.  Yes, that would allow
for storing a much ''smaller'' history, so we wouldn''t need to worry about
gcing history in some way.  I like it.

On Tue, Mar 7, 2023 at 3:04â€¯PM Aaron Boodman ***@***.***>
wrote:

> What I''m saying is that each time RoomDO starts up it chooses for itself
> a new unique ID. This is its "room instance ID". So in 1, when you restart,
> you''d get a new instance ID. Same with 2. I''m probably missing something.
>
> â€”
> Reply to this email directly, view it on GitHub
> <https://github.com/rocicorp/mono/issues/232#issuecomment-1458935630>, or
> unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AESFPBC73ZEJI2ZROBI4XNLW26WFBANCNFSM6AAAAAAVEWVVDM>
> .
> You are receiving this because you commented.Message ID:
> ***@***.***>
>
', 'grgbkr');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('_ZxWtmrFsauqIiZ2EjsfP', 'ziAKPSegyw9lpjiFq3z3k', 1670496475000.0, 'WIP Notes: https://www.notion.so/replicache/Monday-Incident-Out-of-Order-Poke-a889ea29bcd7469d8feec679804ce2bd', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('FRG3vSyYIxtl9ZKwucjMc', 'ziAKPSegyw9lpjiFq3z3k', 1671037168000.0, 'After we fixed rocicorp/shared-monday#3, this mostly went away, but it still occurs at a much lower rate:

<img width="1384" alt="Screen Shot 2022-12-14 at 6 35 22 AM" src="https://user-images.githubusercontent.com/80388/207653949-7f4548c9-a2b3-42d1-b810-94f513cd7de3.png">

These aren''t just one-shot out-of-order pokes (ooop) either. It appears that clients still do loop on them. Here''s one:

<img width="869" alt="Screen Shot 2022-12-14 at 6 36 58 AM" src="https://user-images.githubusercontent.com/80388/207654341-5a683d2f-a3c3-471d-b56b-c94618225192.png">

This particular room has an interesting history. It was first created dec 5:

<img width="1394" alt="Screen Shot 2022-12-14 at 6 50 18 AM" src="https://user-images.githubusercontent.com/80388/207657353-8c8df178-7a69-40bf-9d99-c43caec414de.png">

Only two client IPs have ever accessed, but both from frankfurt, same UA, so I''m guessing same user:

<img width="198" alt="Screen Shot 2022-12-14 at 6 51 56 AM" src="https://user-images.githubusercontent.com/80388/207657661-f83964f5-1c27-4975-92c7-e6bef7eddf9b.png">

No server messages have ever been generated for this room. However there hasn''t been any activity since the recent change that added logging.

On Dec 13, there was a rash of "web socket error: no userData" messages:

<img width="1398" alt="Screen Shot 2022-12-14 at 6 58 09 AM" src="https://user-images.githubusercontent.com/80388/207659110-a4e49d49-99a6-4914-83c0-db74782ca631.png">

This happens in other rooms too so unsure if related:

<img width="1407" alt="Screen Shot 2022-12-14 at 6 58 51 AM" src="https://user-images.githubusercontent.com/80388/207659228-2882ad2a-7f8e-48ba-8299-c5de2ddb7eee.png">

', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('IFgCPzMQBAOn7OtPubiJF', 'ziAKPSegyw9lpjiFq3z3k', 1671681520000.0, 'This still happens but much much lower frequency, and it doesn''t seem to repeat. It shouldn''t be possible with the current protocol to ever see this, so it''s still a bug.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('0CgHV_MFGh13PuckuuPyp', 'ziAKPSegyw9lpjiFq3z3k', 1672880975000.0, 'I don''t have any information about OOP but the "401: no userData" happens when their customer''s auth handler returns a falsey userData or a userData with no userID. This could indicate an auth failure or transient auth problem on their end. Note code here is in early christmas branch, not yet integrated into main: https://github.com/rocicorp/reflect-server/blob/af65727174e9030746dcb3ac1bcacfa813e8fca5/src/server/auth-do.ts#L179. (It should be upintegrated as part of https://github.com/rocicorp/reflect-server/issues/206.)', 'phritz');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('D7FzvMwwetL60L2ejq1SZ', 'cq_yNS_2PFGbgkMVfUIGj', 1670532688000.0, '<deleted previous messages, I was half-asleep and they didn''t make much sense>', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('_7SEynbqrkUb7XClQ6TPY', 'cq_yNS_2PFGbgkMVfUIGj', 1670536422000.0, 'I have updated https://www.notion.so/replicache/Monday-Some-users-don-t-ever-connect-b89e9f770dbf4d518281d5ada8c47c69 with my notes on this. There are some clear next steps. Serializing state to switch to different Monday bug ðŸ˜….', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('Bqbf_fSDdA5V4YtFty6kI', 'cq_yNS_2PFGbgkMVfUIGj', 1670555661000.0, 'https://github.com/rocicorp/reflect-server/pull/205 has the logging improvements. ', 'phritz');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('LHzZZS4tbS6_uC9JCOvqI', 'cq_yNS_2PFGbgkMVfUIGj', 1670924898000.0, 'Hm, these log messages do not show up in data dog:

<img width="1512" alt="Screen Shot 2022-12-12 at 11 18 23 PM" src="https://user-images.githubusercontent.com/80388/207277327-c2ecfa69-6a6b-4243-8822-b010d54ce0bf.png">

I confirmed by manual inspection that the client includes the logging code:

<img width="800" alt="Screen Shot 2022-12-12 at 11 19 35 PM" src="https://user-images.githubusercontent.com/80388/207277586-376624ad-876a-48e3-8919-2b80fcd45cfb.png">

... and I did manually test that these logs showed up (in the console, not in datadog) when I added the patch.

Confirming the theory that these events just aren''t happening, there are some relevant server-side logs that also aren''t happening (I confirmed correct server version running too):

<img width="1512" alt="Screen Shot 2022-12-12 at 11 22 12 PM" src="https://user-images.githubusercontent.com/80388/207278194-b0e378d4-9535-4a88-819f-9a891d221a68.png">

<img width="1512" alt="Screen Shot 2022-12-12 at 11 23 02 PM" src="https://user-images.githubusercontent.com/80388/207278380-239f41a2-8100-4309-8b4e-0c846e583ecd.png">

However, it certainly seems the reconnect loops are still happening. If we search for "disconnecting" and count by client IP:

<img width="914" alt="Screen Shot 2022-12-12 at 11 26 40 PM" src="https://user-images.githubusercontent.com/80388/207279298-d6809deb-fb6a-4992-9874-3a21d615c30e.png">

`198.203.181.181` is having almost 10x as much trouble as anyone else over the past day.

This client had two different documents open for a total of 6 hours today and reconnected every 2s the entire time:

<img width="1512" alt="Screen Shot 2022-12-12 at 11 31 15 PM" src="https://user-images.githubusercontent.com/80388/207280368-09a7f238-a6f7-4393-8b08-4386f50ffea0.png">

Neither of these rooms has *any* server log entries at all:

<img width="1511" alt="Screen Shot 2022-12-12 at 11 32 56 PM" src="https://user-images.githubusercontent.com/80388/207280658-23a98e82-bce3-480a-813e-235e8d1088ae.png">

<img width="1512" alt="Screen Shot 2022-12-12 at 11 33 57 PM" src="https://user-images.githubusercontent.com/80388/207280846-248a4183-9cf8-4061-97dc-30035e9a6e31.png">

It looks to me like this client genuinely could not connect. Perhaps they were offline this entire time and the datadog client queues up the messages to send later?

But it''s suspicious to me that there is not a *single* message from either of these rooms. The user was online at one time enough to get the code for the app. But they could never connect to the server.

Nothing looks particularly odd about their UA:

<img width="778" alt="Screen Shot 2022-12-12 at 11 36 56 PM" src="https://user-images.githubusercontent.com/80388/207281635-ede0c18e-4a97-4bb5-a6f1-67b40a0bf379.png">

Let''s look at the next most common client sending "disconnecting" messages. The next most client is almost 1/10 the frequency:

<img width="1109" alt="Screen Shot 2022-12-12 at 11 39 38 PM" src="https://user-images.githubusercontent.com/80388/207282652-0aa81231-3fae-4ada-8b96-1d1243e8d2d6.png">

This pattern looks much healthier, the gaps between "connected" and "disconnecting" are much longer:

<img width="1507" alt="Screen Shot 2022-12-12 at 11 42 40 PM" src="https://user-images.githubusercontent.com/80388/207283256-c8b93b7e-6eb2-4628-bbae-d601227ae536.png">

Also in this case there are server logs!

<img width="1512" alt="Screen Shot 2022-12-12 at 11 45 58 PM" src="https://user-images.githubusercontent.com/80388/207284001-c6f9977e-f4e0-4941-a37d-e696daab9e29.png">

The third example is back to the bad pattern though. Reconnect loop:

<img width="1511" alt="Screen Shot 2022-12-12 at 11 46 44 PM" src="https://user-images.githubusercontent.com/80388/207284144-13bb4bfd-bb4b-425d-90f2-058a7fde4d71.png">

No server logs:

<img width="1512" alt="Screen Shot 2022-12-12 at 11 47 15 PM" src="https://user-images.githubusercontent.com/80388/207284269-484f04e3-f7be-4036-954b-9b31a85f775b.png">

The UA again seems like a big org:

<img width="492" alt="Screen Shot 2022-12-12 at 11 47 45 PM" src="https://user-images.githubusercontent.com/80388/207284376-65e8559a-cdb6-4bb3-a23b-dff00efb6205.png">

Could they be blocking sockets?', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('nkyZ3QCWpWi9g8CY7udJv', 'cq_yNS_2PFGbgkMVfUIGj', 1671045707000.0, 'Fritz added a bunch of logging to confirm whether these users are ever connecting to our server at all:

https://github.com/rocicorp/reflect-server/pull/213

These new logs *do* show up in datadog:

<img width="1402" alt="Screen Shot 2022-12-14 at 8 49 13 AM" src="https://user-images.githubusercontent.com/80388/207685923-afa6d89d-5d50-42f3-8360-e59bd5f15369.png">

However, the client with the most number of "disconnecting..." messages in the last 4 hours is still not generating any server logs:

<img width="1400" alt="Screen Shot 2022-12-14 at 8 51 46 AM" src="https://user-images.githubusercontent.com/80388/207686534-07ed1091-23f7-4770-9ac2-da205472c718.png">

<img width="1509" alt="Screen Shot 2022-12-14 at 8 53 14 AM" src="https://user-images.githubusercontent.com/80388/207686737-0822d920-2518-43e1-9694-54fe17160a75.png">

<img width="1390" alt="Screen Shot 2022-12-14 at 8 54 21 AM" src="https://user-images.githubusercontent.com/80388/207687223-2f9eedda-2c1a-4639-baa2-40a579a8879d.png">

Confirming, other rooms *do* show server logs:

<img width="1403" alt="Screen Shot 2022-12-14 at 8 56 32 AM" src="https://user-images.githubusercontent.com/80388/207687573-1c7b09ad-fd59-46ef-8594-37a2a036cbb9.png">

So it seems that we still have some clients who never generate a single log message from our server. It''s not perfectly clear to me how common this is because a client in this state will generate "disconnecting" messages continuously. But let''s try a few more.

Second most client in disconnecting messages:

<img width="1401" alt="Screen Shot 2022-12-14 at 8 59 51 AM" src="https://user-images.githubusercontent.com/80388/207688198-427f016e-f72e-4f39-a398-aa613eac7441.png">

Interesting thing here the server *does* log messages for this client. It''s a fairly consistent pattern:

<img width="1403" alt="Screen Shot 2022-12-14 at 9 03 04 AM" src="https://user-images.githubusercontent.com/80388/207689228-132e04f9-4592-4485-bfd2-d6e4451d2f74.png">

Here''s an example of the pattern. These are all log lines for room `9E4L_hoi1n9HQlfXq6kRSNxmaN-8xCsA` associated with the `ts` querystring field `10213662`.

The first entry is actually from the auth DO:

<img width="743" alt="Screen Shot 2022-12-14 at 9 14 29 AM" src="https://user-images.githubusercontent.com/80388/207692103-52714030-1037-415f-82f8-dd668af87827.png">

I think this is just because all the workers are separate concurrent processes, so the order across workers is not realtime. Anyway, next one is the worker:

<img width="734" alt="Screen Shot 2022-12-14 at 9 09 59 AM" src="https://user-images.githubusercontent.com/80388/207691248-de52ca37-4a97-43bb-ae9e-7f7f93205808.png">

The roomdo receives the request:

<img width="742" alt="Screen Shot 2022-12-14 at 9 10 32 AM" src="https://user-images.githubusercontent.com/80388/207691344-eddc922f-44e0-4734-bfa4-eddc3f1bae50.png">

The roomdo finds a prev socket for this client so closes the old one ðŸ¤”

<img width="754" alt="Screen Shot 2022-12-14 at 9 11 54 AM" src="https://user-images.githubusercontent.com/80388/207691673-40e7fe80-4a6a-4f25-8fa2-f3f7baee9db3.png">

Room do notices the close:

<img width="745" alt="Screen Shot 2022-12-14 at 9 12 33 AM" src="https://user-images.githubusercontent.com/80388/207691751-3e0799cf-7045-48e5-9f7b-a356543ca735.png">

Then the pattern repeats with the authdo receiving a new request with a different timestamp:

<img width="752" alt="Screen Shot 2022-12-14 at 9 17 17 AM" src="https://user-images.githubusercontent.com/80388/207692615-ac75adad-c28c-44fd-8f02-4ec1fe3a0ed2.png">

The client with the third most number of "disconnecting" messages is seeing the second pattern. Log messages on server, but connection doesn''t last long.

<img width="1401" alt="Screen Shot 2022-12-14 at 9 20 36 AM" src="https://user-images.githubusercontent.com/80388/207693185-de62727a-b5b4-4218-a0a9-24ce4b537fce.png">

Same with fourth:

<img width="1383" alt="Screen Shot 2022-12-14 at 9 21 21 AM" src="https://user-images.githubusercontent.com/80388/207693317-444bcb7a-4450-4626-b8c3-9eb80f5f5bf8.png">', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('gxXKn2IfTUWFKRAo04Ayp', 'cq_yNS_2PFGbgkMVfUIGj', 1671046836000.0, 'I feel like stepping back here, I really want to know how common these phenomena as a percent of entire client population. It feels common but I don''t really know. There has to be a way to ask datadog this question, just have to figure out how.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('BPzhCrvSqDf5ECg4FeUy8', 'cq_yNS_2PFGbgkMVfUIGj', 1671053901000.0, 'OK here''s part of the answer:

Out of 496 unique client IPs in last 24h, 473 have "Connected" once. So ~4.6% do not ever emit a "Connected" message.

<img width="1168" alt="Screen Shot 2022-12-14 at 11 36 42 AM" src="https://user-images.githubusercontent.com/80388/207720050-d3d072cc-df38-479d-b29d-6b76c770cdad.png">

<img width="1164" alt="Screen Shot 2022-12-14 at 11 37 14 AM" src="https://user-images.githubusercontent.com/80388/207720194-fbda407e-00d5-4c00-9a9a-8fa04a539459.png">

This 4.6% would encompass both patterns observed above -- no server messages and server messages, but client never connects.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('ZNlsvyzLQN0e_Pa_y4s7X', 'cq_yNS_2PFGbgkMVfUIGj', 1671092081000.0, 'I downloaded a bunch of logs from datadog in order to do offline-processing. The code is in https://github.com/rocicorp/monday-log-processing.

I processed logs from 2022-12-14T0500 to 2022-12-14T2100 HST (2022-12-14T1700 to 2022-12-15T0900 Israel).

Here''s the summary:

* 405 distinct client IPs
* 233 client IPs experienced at least one disconnect [0]
* 17 (4.2%) client IPs never connected ("non-connecting IPs")
* 9 distinct *rooms* never connect ("non-connecting rooms") [1]
* 4 non-connecting rooms have zero server logs [2]
* 2 non-connecting rooms are missing their web socket upgrade header [3]
* 3 non-connecting rooms are still a mystery

[0] Remember that disconnects in of themselves are not unexpected or problematic, it only matters when they are not reconnecting or thrashing.

[1] For this dataset the server doesn''t have client IPs and the client doesn''t always have client IDs, so there is no way to tie the client/server logs together other than room. Generally, I think, each user is in their own room. So the fact that there are fewer non-connecting rooms than IPs indicates to me that some users who could not connect moved IPs. So perhaps the real number of non-connecting users is more like 9/405 (~2%).

[2] This indicates that the web socket request never made it to the server at all. Perhaps these users were genuinely offline, or some other networking related situation is happening.

[3] This indicates that some system between the browser and the server (ie a corp proxy) is messing with the connection

Here are the relevant roomIDs in case anyone wants to dig further:

```
ips that never connect (17): [
  ''207.236.13.73'',
  ''93.93.216.236'',
  ''207.236.13.84'',
  ''180.208.59.157'',
  ''161.69.114.29'',
  ''103.143.8.126'',
  ''2a00:23c5:7e1d:d901:897b:9607:adf3:a829'',
  ''2a02:a212:c0:a500:e5d3:3234:c46d:3f37'',
  ''2a02:c7c:6e5a:2000:1879:4c16:79eb:5485'',
  ''198.154.191.158'',
  ''2a02:5080:2d03:7f00:d5b0:d013:ead5:1d0b'',
  ''2a00:23c5:cd9f:b601:2595:ce3e:cdb1:d85a'',
  ''68.129.143.233'',
  ''2607:fea8:be5f:900:38c2:ad27:6400:bc64'',
  ''108.166.141.122'',
  ''65.56.144.146'',
  ''73.73.24.212''
]
rooms that never connect (9): [
  ''96Dme_K9e7DUXVZ4uDbI4kJbFwRttvkz'',
  ''o7jpcRjKr6D9O9ImQ9WDEBAMr8UFuto-'',
  ''ka8iwY3wtfbQ4M6t8KYPWA8bQ4f9xBWd'',
  ''QrhVlciHlrspPIDBI1-ciskTgEAu-dQt'',
  ''NnLfKqhCZ44tlPLbGe-qJuN_5Fg-Oohs'',
  ''fZwWkZH4sPumVgSKhef903mQAB7H4IrC'',
  ''WZ14G6J7gBAk5GnRujDBEb_VwLWLJa4l'',
  ''il-ZtBddMLcM7rLi3Hi1uEnCdGpFcn04'',
  ''7W2qErt6IJhSWTIdHOxJ1Z8qN-xtGgZp''
]
non-connecting rooms without server logs (4): [
  ''QrhVlciHlrspPIDBI1-ciskTgEAu-dQt'',
  ''NnLfKqhCZ44tlPLbGe-qJuN_5Fg-Oohs'',
  ''WZ14G6J7gBAk5GnRujDBEb_VwLWLJa4l'',
  ''7W2qErt6IJhSWTIdHOxJ1Z8qN-xtGgZp''
]
non-connecting rooms with missing upgrade header (2): [
  ''96Dme_K9e7DUXVZ4uDbI4kJbFwRttvkz'',
  ''o7jpcRjKr6D9O9ImQ9WDEBAMr8UFuto-''
]
```

Next steps:

* Re-do this analysis but in terms of clientIDs
* Fix the logging of navigator.online to confirm user is online when these happen
* debug remaining three non-connecting rooms
* Add to analysis grouping by UA/corp proxy
* Research socket connection success rates of other similar products (ie figma, notion)
* ([separate bug](https://github.com/rocicorp/mono/issues/225)) Understand connection uptime', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('kKouuum4IyhklZroSnZ_V', 'cq_yNS_2PFGbgkMVfUIGj', 1671224917000.0, 'OK, armed with the new logging (thanks @noamackerman) I re-ran this analysis in terms of unique clientID and cient IP addresses.

These logs were from 20221215T1200 to 20221215T1630 HST.

* unique client IP addresses: 98
* unique client IDs: 225
* IPs addresses that never connect: 4 (~4%)
* client IDs that never connect: 12 (~5%)

Most of the clientIDs are spurious results. They are examples of a pattern where extra Reflect instances are created by the app and immediately destroyed. The same IP address does connect concurrently with a different Reflect instance.

See for example clientID: 13599658

There is only one occurrence of a log line (client or server for this client ID):

<img width="1376" alt="Screen Shot 2022-12-16 at 9 51 11 AM" src="https://user-images.githubusercontent.com/80388/208177963-0c164f0a-b4b4-405e-8559-dabdd7380ac9.png">

The reason is because the very first few log lines from a client unfortunately don''t have their client ID associated. However, if we filter on their IP address in this window, we can get a sense for what''s going on:

<img width="1377" alt="Screen Shot 2022-12-16 at 9 53 53 AM" src="https://user-images.githubusercontent.com/80388/208178405-88732d08-7f54-4348-a953-eb8b51808407.png">

There are two connections in quick succession in the same room from same IP for some reason. The second one gets through and connects normally. The first one never has its request hit the server at all. My guess is a React thing, this looks like the useEffect that instantiates Reflect happening twice, like what would happen in dev. Do ya''ll have a developer in NA using opera ðŸ˜†? If not maybe under some circumstances Monday instantiates Reflect twice in succession? I feel like I''ve seen this elsewhere in the logs form time to time.

Anyway long story short, this client is not problematic. Most of the clientIDs that don''t connect fit this pattern.

So it''s actually more useful to look at the IP addresses. There are four that don''t connect.

*180.208.59.157*

This is the same one we saw the other day in China. Fascinatingly this same user (by way of room ID) connected later from Hong Kong and succeeded!

So this IP legit can''t use web sockets.

*177.244.53.34*

This user is in Mexico. Their cable provider doesn''t seem weird to me. Their requests make it to the server but seem to be terminated moments later each time.

*131.125.11.1*

This user is in New Jersey using business internet. We only ever see three logs from their room, all client-side. It appears they cannot make socket requests.

<img width="1391" alt="Screen Shot 2022-12-16 at 11 01 28 AM" src="https://user-images.githubusercontent.com/80388/208188305-0a6ddae3-e325-4f17-9dc7-9ab32a8e4ec9.png">

*46.117.249.75* 

This user is in Tel Aviv. They appear to not be able to connect. The connection succeeds from server perspective but never from client. 30m later, the server gets a close event ðŸ¤”.

So I think the takeaways are:

* From this dataset, 4 IPs of 98 could not connect.
* We should do this analysis with ~1 week of data to see if this 4% figure holds.
* Roci should check if this 4% figure matches others using sockets.
* Roci should implement an HTTP fallback if these figures hold.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('J7KtfOGZrKbMdB9gcAx5n', 'cq_yNS_2PFGbgkMVfUIGj', 1683763941000.0, 'Closing this as we have our own metrics now.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('-CPkH0xzqOMLN2mKA__K7', 'wGxWtCO7rh5qTTOZIjV-Y', 1670499358000.0, 'Done', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('C_9oZjw-AK55KGMMMZYON', 'PT3jXRcu19PYHh-q7hV3_', 1676052071000.0, 'External tracking bug: https://github.com/rocicorp/replicache/issues/1029', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('BV5TH3dXppF0CmS3bDG2o', 'UORH6N6zcrn-pQ8nzqpaE', 1669918286000.0, 'See https://discord.com/channels/830183651022471199/1047647135774036041 for context.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('R2jQArvD9k6bBrZeNUc8f', 'UORH6N6zcrn-pQ8nzqpaE', 1669949112000.0, 'Update @grgbkr says this does work now and the issue that placemark is seeing is that v11-12 should have been a format change (since hash format changed).', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('mk9xVVtl6Rl7Dg103t0yj', '_s7mPxujoqBtybUn78cKJ', 1675129206000.0, 'we fixed this', 'phritz');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('casfh-ge4zLReZd55xu3j', '16bjNPfoHawGEaDZRHPwO', 1669152926000.0, '@grgbkr WDYT?', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('nF3sx-mc7MXzcYtO2vF6_', '16bjNPfoHawGEaDZRHPwO', 1669949218000.0, 'I think that making hashes less strict doesn''t help us because it won''t let users rollback to v11 unless they first have the newest v11 that relaxes the check.

I think instead we should increase the format version in 12.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('EKAqeUXimjXwhtfRWqXse', '16bjNPfoHawGEaDZRHPwO', 1669973514000.0, 'It does let them roll back to the latest 11', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('Enawbs5TMgLUzDeNmicfb', '16bjNPfoHawGEaDZRHPwO', 1669973566000.0, 'See rocicorp/replicache-internal#425 on v11 branch', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('A4N_VbqsVJKDG9UmpAUxO', '16bjNPfoHawGEaDZRHPwO', 1670499378000.0, 'Done', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('XW4-Q_gznfErcdOgffqtm', '8P8fGLJ2uldgKFT5ERK1R', 1671588866000.0, 'I am not certain that such a thing is needed once we have offline support. It''s easy to open two reflect instances and copy data?', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('l_YnihYhgOH6wFtMKfmkt', 'tgIGh7Lmke7WUoduo-KP8', 1709536564000.0, 'I think we''ve done this, please reopen if false @arv .', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('Ne7ehJoq4zB_Xq5EUABRK', '6Xj3jAEB5s2jWQnJWq6rK', 1669056272000.0, 'By entrypoints I mean:

- any place user JS calls into Replicache
- any place we parse an http response from a user endpoint
- any place we call user code and handle their result', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('0CnwUQ2xASBWB34HMbkNa', 'ZffAHdq6EUF36pwBdx7VK', 1668951314000.0, 'Meant issue to be public. Replaced with https://github.com/rocicorp/replicache/issues/1035.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('FAcp4klPoy59hToxcGxqs', 'CpwNCPCX7E2ZqQ3FI5ztD', 1675129452000.0, '> a strategy for managing auth and room storage schema, i don''t want anything complicated but we have to know how to make changes

Don''t overlook this one. Seems like there needs to be a way to undo pushing a new, incompatible reflect server version that interacts with storage. If rooms are small (<25M) maybe we can get away with just copying data to a new storage path for the new version when it needs to migrate and deleting version n-2 when we push n? If not then backing rooms up to R2 ugh. ', 'phritz');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('TH_ZQ6mUaDjrOKCQwQsdD', 'CpwNCPCX7E2ZqQ3FI5ztD', 1677704469000.0, 'I don''t think we need this stuff for playable beta. The playable beta is beta, it''s not necessary to be rock solid. I think these are things we probably need to do before GA though so adding a GA label.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('JNn2-CEhQA4rhYt8caMSx', 'msfBFWwz1BHa9a4P0wpxj', 1668424256000.0, 'Why is it likely developers will only test in release?

I like option b. Option b is a superset or a. A is what happens when you run replicsche in release mode without the flag and pass something with undefined.

put a different way, b is a but we try to help users detect/avoid this to the extent we can.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('C7NOLJqWIXF7jnsTi0q1b', 'msfBFWwz1BHa9a4P0wpxj', 1668548736000.0, 'Going with b for now.', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('tf2qSA02rrjPj3nprcKrX', '-lyB2jG8ruMO0XLDAcM1-', 1668314163000.0, 'Assigning @arv as master-of-replicache and because he wakes up first Monday :). Also cc @grgbkr since it appears it was your PR that regressed this.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('qjEBYDePrp27EWPCq0DBd', '-lyB2jG8ruMO0XLDAcM1-', 1668314242000.0, 'To reproduce:

```bash
cd replicache-internal
git checkout main
npm pull
npm pack
cd ../replicache-todo
git checkout main
npm pull
npm install
npm add ../replicache-internal/<tarball-you-just-packed>
npm run dev
```
', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('uNt-DO5jxuDpPM6s_El5Q', 'BNyyrMVDWQYoraAdNu2bn', 1668083240000.0, '```
  expect(deleteCount).to.equal(2);
```

we are getting 3

It seems like the extra one could be coming from a persist of a refresh.', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('hgoJZUWjAIMv27mmC2Bmd', 'nIg73t7LpImlEz-rwv_eW', 1668083119000.0, '```
  expect(store.write.callCount).to.equal(0);
```

We get 1 here.', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('eWqiYq2M4mEemk0k8syzD', 'r-yUB5GoR9btnEGqzNr30', 1667864246000.0, 'related: https://github.com/rocicorp/reflect-server/pull/20', 'phritz');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('rbT06MlDjt7VQOb6fIDGs', 'SHr0OH-dH5OzMQznNglDj', 1667677814000.0, '@arv can we do this for 12, super annoying for docs.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('vjdd_r4SXKmWitJ0C0Eqp', 'SHr0OH-dH5OzMQznNglDj', 1667812163000.0, 'Should be straight forward.

How do we explain `name`?', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('z-msmY0mqPabntv5a-TJ7', 'SHr0OH-dH5OzMQznNglDj', 1668548036000.0, 'I wanted to get this into v12.

How about only having a `user`? In other words just rename the option and the property?

@aboodman ^^^^', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('UnAo8xS2SIpLtxtvLi5lW', 'SHr0OH-dH5OzMQznNglDj', 1668564216000.0, 'I think we need `user` and either `name` or a new `space` option.  Otherwise the common space use case will require passing `${userID}:${spaceID}` as the value for `user` which is awkward.   ', 'grgbkr');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('RmEg41cGL56mk8U30z5tI', 'SHr0OH-dH5OzMQznNglDj', 1668656016000.0, 'I think really it should be `user` and `space` and we should embrace being opinionated, and embrace spaces. But I don''t have enough cycles to think through whether I''m missing something there, so `user` and `name` is more conservative.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('6OAQgFxlN6Sb5Yjel-F6Y', 'SHr0OH-dH5OzMQznNglDj', 1668671599000.0, 'Moving this to v13 due to the issues with `makeIDBName` not generating unique names and `IndexedDBDatabase` needing changes.', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('2zylBoUZYAXCyBvGfzU7j', 'paZ3kPMWit7ec3d077oQk', 1667677729000.0, 'Looks like we can maybe do this with a customer reporter:
https://jestjs.io/docs/configuration#reporters-arraymodulename--modulename-options
https://github.com/facebook/jest/blob/main/packages/jest-reporters/src/types.ts
', 'phritz');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('BrRTvHUQc6wk2UiQkDS9t', 'paZ3kPMWit7ec3d077oQk', 1668120645000.0, 'To run only one test we can use jest''s `.only`', 'phritz');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('emOK2vJNSTKTR02CVqcvB', 'paZ3kPMWit7ec3d077oQk', 1672740003000.0, 'You can change the log sink to `consoleLogSink` to emit errors. I agree it would be nice to automatically spew in the case of failing tests.

To run a single test from CLI, I use: ` npm run test -- -t ''roomStatusByRoomID''   `', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('PtHJqd_5F1shmbJgTQHgZ', 'QJS5_XjX-Q2gjWnz8zRW7', 1667502721000.0, '@phritz can you enqueue this after what you''re currently doing, before resuming RAAS.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('3eNoSvk0KlLUje_Ss1y0x', 'QJS5_XjX-Q2gjWnz8zRW7', 1667554006000.0, '> What does this stack correspond to? Can we use the trick that @arv just did in Replicache to demangle the stack and see where this is coming from?

We do not have Noam''s sourcemap :''(', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('oBF_56e8WhhlOAuYgH6Mq', 'QJS5_XjX-Q2gjWnz8zRW7', 1667679986000.0, 'Can we get the ts definition of his mutator, `changeElements`? 

Is it possible that the type of something the mutator reads from storage changed in the mutator in a way that is incompatible with the type that exists in storage? ', 'phritz');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('UgiCBjjeuGKocwxBMDJaf', 'QJS5_XjX-Q2gjWnz8zRW7', 1667731251000.0, 'OK @noamackerman says that it''s just storing data with a field that is undefined which causes, such as:

```ts
  value: {
    type: ''textBlock'',
    id: ''2nl5UfoVBhT3lIF8f6dF7'',
    x: 786,
    y: 588,
    fill: ''#000000'',
    fontSize: 36,
    width: 300,
    height: 43,
    cursorPosition: 1,
    attachedConnectors: {},
    textPosition: { x: 0, y: 0 },
    align: ''center'',
    zIndexLastChangeTime: 1667730876888,
    fontProps: 0,
    lastModifiedTimestamp: 1667730879924,
    text: undefined
  }
```

We should validate writes not just reads.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('j1sKdApUYc9MIz57JqEN-', 'QJS5_XjX-Q2gjWnz8zRW7', 1672874240000.0, 'Duplicate of rocicorp/mono#164 ', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('y-9CsY41Vlf_ymxxoXVCB', '45o4ZNvHvQOD-7lnw_2Ud', 1667810719000.0, 'Done', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('VGkWdpI7a2pYbSkFgOWju', '4CL6b_WYsSrA-CsdtAjFQ', 1666297567000.0, 'To make this easier we need a way to get the sourcemaps for releases. We could potentially do this as GH action that uploads a "release" when we add a git tag.', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('nu0lDSPXByZ-xKVeUDCtU', '4CL6b_WYsSrA-CsdtAjFQ', 1667400345000.0, 'Now we upload the `.map` files when we do a release (tag and git push --tags)

<img width="1310" alt="image" src="https://user-images.githubusercontent.com/45845/199520409-4b289058-df39-40ed-994b-2e9be8107b32.png">

Deleting the temporary tag now.
', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('ix1Poq5OOBpvRpqRGrgqq', '4CL6b_WYsSrA-CsdtAjFQ', 1667401673000.0, 'You can now download the sourcemap using `gh` (Github CLI)

```
gh release download v11.3.3 -p ''*.map''
```

and deobfuscate the stacktrace using:

```
npx stacktracify replicache.mjs.map --file stacktrace.txt
```', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('iMdxToWPZRRSHSTWb4k0u', '4CL6b_WYsSrA-CsdtAjFQ', 1667554870000.0, 'But will this work if the stack trace came from a build that compiled Replicache into some other single js file :-/. Seems like same thing would happen as with that stack from Noam in reflect-server.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('_8IJc5d5kvrEKUacOiTW9', '4CL6b_WYsSrA-CsdtAjFQ', 1667642449000.0, 'Yeah. That is a serious problem. The only way that can work is if we get their sourcemaps and the compiled their source with our sourcemaps which is close to 0%.

In tmcw''s case he did not bundle replicache into his own bundle so we were able to deobfuscate the stack trace.

In most cases the simplest solution would be to give them the original code :''(', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('vqMM0dcEk3mYb5tAGalJ3', '4CL6b_WYsSrA-CsdtAjFQ', 1668292546000.0, 'OK then if the sourcemap is not a real solution then let''s not add complexity for something that doesn''t work most of the time. This just seems like more build goop that''s not a real help to us.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('6MGMoltf-JWTdT-_133eN', '4CL6b_WYsSrA-CsdtAjFQ', 1668292680000.0, 'Or maybe the complete solution is to tell users to not bundle Replicache or something if they want debugability. Could we experiment with that in our sample apps, say Repliear? I''m not sure how difficult it is. If it is easy, we could have a doc telling people to do that and that would be a good solution to this bug.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('FidJPw0KnyeTcXQb7p99i', '4CL6b_WYsSrA-CsdtAjFQ', 1668294043000.0, 'I think probably what we should actually do here is make it easy for paying/selected customer to use the non-minified (source) builds. Our license already requires that they keep such code in confidence and I''m not really concerned about it as long as it''s with a controlled population.

Is the right way to do that with npm private packages?', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('xJ1qaLBRQ7y0qwzFiJT0G', '4CL6b_WYsSrA-CsdtAjFQ', 1672746969000.0, 'I just published https://www.npmjs.com/package/@rocicorp/replicache

To publish the private package:

```bash
git checkout rocicorp-replicache

git merge main

# Verify that the only diff is the name and the sourcemap
git diff main

git push

npm publish
```', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('H1qUfnO-Fct-A_3NLJB1c', '4CL6b_WYsSrA-CsdtAjFQ', 1677874911000.0, 'I wonder if it is possible to do this for Reflect for alpha. If not can be beta. Not critical for alpha but would really help our early serious tire kickers (like subset) to have a good experience.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('rFM2IdKn3a2NqsRdG0YIX', 'RT9OCN_PhnDZgZJKK-VSo', 1666118708000.0, 'This we have to wait for the defork', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('BchgGY5VJL1ZipeODA6gb', 'RT9OCN_PhnDZgZJKK-VSo', 1667899453000.0, 'I think we can remove these in v12 but I''m not sure. Mutation recovery uses v11 commits but we only replay Local mutation commits and IndexChange commits are ignored.', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('IS-doWoLzwtV0rzZ1R-Rm', 'RT9OCN_PhnDZgZJKK-VSo', 1670499417000.0, 'Cannot get rid of this until we stop recovering old mutations.', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('dnNUyvZfTebAMh3acBFJB', 'Ag5p9iWOp9-DEap2jg6jy', 1665587852000.0, 'cc @arv @grgbkr ', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('8Zam8Fmx1YlhtLAjfpswK', 'Ag5p9iWOp9-DEap2jg6jy', 1666297667000.0, 'Step one is to create a perf tests that does ~100 mutations', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('wurQ9-_7rYMAQ8gTWGM4b', '1Fv8DCZaPgZHVrHVTuK2o', 1686065391000.0, 'It''s interesting why this comes up. It''s not really that you actually ever want/need multiple params. I feel like in real code using a single object param will be common.

But when playing around/demoing, it''s faster to type/read:

```ts
async (tx, foo: string, bar: number) {

}
```

than:

```ts
async (tx, {foo: bar}: {foo: string, bar: number}) {

}
```', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('-I2yrE6d74kET7-T5ZcsR', 'F6tcpFPBlYqFuAAGAF_3C', 1664958452000.0, 'It doesn''t make much sense to me either ;-)

The type of invokeResult is `true | false | ''throw''`

```ts
    invokeResult?: boolean | ''throw'';
```

Let''s just change this the test to log ''true'', ''false'' and ''throw''. Not sure why I wanted things to be more concise. It is confusing.', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('MBJuv09lBrQ9h762tW5X2', 'zvV2RenfHuqN3CiHcz8_u', 1664839652000.0, 'We have narrowed this down to [a commit in the run up to Replicache 10](https://github.com/rocicorp/replicache-internal/commit/345df2b3594352dcd6cab64b58956711473892ee), which ended up in Reflect 0.4. We can reproduce the jank in Replidraw, but only when the console is open. Unknown why it''s so much more pronounced in Canvas.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('1coYf_0oLUTTlmq4uvK81', 'zvV2RenfHuqN3CiHcz8_u', 1664839748000.0, 'Also I don''t think we have a solution for doing what 345df2b3594352dcd6cab64b58956711473892ee was originally trying to do some other way yet.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('kVFo0-9J0sIMqfeFXAEhS', 'zvV2RenfHuqN3CiHcz8_u', 1664845610000.0, 'OK I''ve been working through the history here. Some notes:

- The `persistPullLock` was added at 345df2b3594352dcd6cab64b58956711473892ee.
- This was done because of https://app.slack.com/client/TMQQ9DWPQ/C013XFG80JC/thread/C013XFG80JC-1651685554.718029. Repliear was hitting a check in `maybeEndPull()` that was checking the sync head had not moved since pull began.
- This check dates all the way back to the first impl of pull! In Rust! https://github.com/rocicorp/repc/blob/273101caffa6fc389957c9fa24df828e9afe89a6/src/sync/pull.rs#L221
- The check makes sense in context: Back then, the sync (and main!) heads were shared across tabs. and the way rebase worked, it would gather a list of commits that needed to be rebased (from main head) that weren''t rebased yet on sync head, then return them to the JS to rebase. When returning the lock on IDB was released. This check prevents `maybe_end_try_pull` from continuing if some other pull in a different tab had begun in the meantime and changed the sync head.
- But in the context of SDD, I think the check stopped being a real necessary thing for correctness and became more of an internal assert/sanity check. Because in SDD the sync head was a part of memdag and so by cannot be accessed or modified by some other process.
- Except that then `persist()` was added and actually tripped the sanity check. Because the goal of persist is to transform temporary hashes to permanent hashes, and persist can happen in the gap between `beginPull()` and `maybeEndPull()`. This modifies the hash in the sync head, triggering the error.

So it seems to me we can and should remove both the `persistPullLock` and the sanity check here https://github.com/rocicorp/replicache-internal/blob/main/src/sync/pull.ts#L641. Once we remove both, we should test the case in Repliar that originally motivated this change and see whether it still works: https://rocicorp.slack.com/archives/C013XFG80JC/p1664829629149959?thread_ts=1664815353.385429&cid=C013XFG80JC

Alternately, we could remove temp hashes from SDD. Then we can remove the `persistPullLock` and the sanity check should keep working even when persist and pull interleave.

Separately I think we need to do a group code review of sync. Reading through this I feel like this has gotten a bit grotty through many refactorings and I worry that parts of it no longer make sense. Perhaps this should happen after DD31 lands and the SDD branches are removed.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('6RZ4UiJljuLK5L3_MueEx', 'zvV2RenfHuqN3CiHcz8_u', 1664847598000.0, 'There is also the question of why this mutex causes the behavior we see. The behavior we saw was that pokes never get processed during dragging because `await req.json()` on a DOM `Request` object doesn''t return for awhile.

I do not know how this mutex could affect the DOM `Request.json()` method. But if the json method didn''t return for awhile (for other reasons, such as my task prioritization theory) then that would hold the  `persistPullLock` open which would prevent persist from happening. Not sure what the affect of that would be.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('tyFFgCnjqvpfjdqMDWq0K', 'qNDHq-v9DJi9Q8Phkqi7p', 1666297831000.0, 'Haven''t seen this lately', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('WuKItinlFuuRqHTAZvz5Y', 'MEtP8zRWMlRBupye-z2cU', 1664533632000.0, 'Not clear why this autoclosed?', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('hcMWTgG4TYJY6bpxpLrru', 'MEtP8zRWMlRBupye-z2cU', 1665350932000.0, 'Fixed in rocicorp/replicache-internal#296', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('uDVeMX6iUNeG8jLjRZ1iy', '0OoBGWKQ-7QLSmC3AlQpk', 1672741721000.0, 'I''m happy with this: https://docs.google.com/spreadsheets/d/1d6xCMg6c9_oKso-124gFkfuKsY1aJXEdRqo_MX8yzk4/edit#gid=2131158829, but it would be good for @phritz to validate it.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('Ll6kI4Nvk_r3KLiGMTT4B', 'RIOpt4qqn1wHFAbbVGLfZ', 1666298022000.0, 'Is there anything left to do except to change the name?', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('eaFXpVr0r17gjqjj7yMV4', 'RIOpt4qqn1wHFAbbVGLfZ', 1669010849000.0, 'Yes there is. Here are the API changes I''d like to do:

- Add the rest of the features from scan(). The options argument to watch should be the same type as the argument to scan.
- Remove the `initialValuesInFirstDiff`. I can''t imagine a use case where you''d not want the initial values. And if there is one, users can just ignore the first callback. At the very least, we should flip the default of the flag since I think wanting the initial values is overwhelmingly more common.
- Add a convenience to get the list of values not just the diff (identity should be maintained).', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('BaxWU01k2CPtur0jVkHiX', 'UIjwj9vMRKJWAM6JxYKO5', 1663900581000.0, '@aboodman any chance we''d get this for free via https://github.com/rocicorp/reflect-server/issues/149?', 'phritz');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('J-Fz2rgpX69OgqBLMgsA7', 'UIjwj9vMRKJWAM6JxYKO5', 1663900883000.0, 'We would but I was in there anyway so I just exposed it for now. Subset was asking for it.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('fEZqI25lMZ36jMoujRpyd', 'UIjwj9vMRKJWAM6JxYKO5', 1663900932000.0, 'Fixed via https://github.com/rocicorp/reflect/commit/12418f91feb37257fa60432dc600660eaca2cba2.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('nsc_hihzj6aYDhTgFjayO', 'w-m-1MtivCOqjAD1DEVPY', 1663744504000.0, '> we have in the past been enamored of the idea that we could use mutation timestamps from the sending client to have perfect replay. the mechanism by which the sending client, server, and receiving client clocks are aligned is not clear and seems complicated, my guess is we could start with maybe server receive timestamp (or better, frame number) and see if it works

Agree with all except this one. My bet is it''s simpler and going to look a lot better to use the source timestamps.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('FElp6xJahePCLxL-45_yH', 'w-m-1MtivCOqjAD1DEVPY', 1663745033000.0, '> Agree with all except this one. My bet is it''s simpler and going to look a lot better to use the source timestamps.

Fair enough. 

Small digression: I wonder if it is easier to think about these things if we talked about _frame numbers_ instead of timestamps? Like the sending client, server, and receiving client all keep a sequential counter of what frame they are in, independently. For some reason aligning on that level of granularity makes it easier to reason about for me. ', 'phritz');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('MEY7YLwteal5N2HJPZA5T', 'w-m-1MtivCOqjAD1DEVPY', 1663745513000.0, 'I started sketching out the algorithm on the receiver. I was wrong, it''s harder than using the server timestamps :). I''m OK trying the server ones to start, but I''m worried it won''t look good without using the source.

I think the source is possible the only little tricky bit is that a badly behaved source client could hold up the show and there has to be some heuristic to prevent that.

>  I wonder if it is easier to think about these things if we talked about frame numbers instead of timestamps? Like the sending client, server, and receiving client all keep a sequential counter of what frame they are in, independently. For some reason aligning on that level of granularity makes it easier to reason about for me.

That''s a cool idea, but what resolution would you use? Some devices have 120 fps now. Is this just a different sort of clock whose ticks are arbitrary 16 2/3ms long? Also how would you even keep the count? Because there''s no inherent notion of a frame counter in browsers. You could just do the math I guess just for purposes of having an easier to reason about number.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('uy5KMTR-vdhzA8OC-Bipz', 'w-m-1MtivCOqjAD1DEVPY', 1663796023000.0, '> That''s a cool idea, but what resolution would you use? Some devices have 120 fps now. Is this just a different sort of clock whose ticks are arbitrary 16 2/3ms long? Also how would you even keep the count? Because there''s no inherent notion of a frame counter in browsers. You could just do the math I guess just for purposes of having an easier to reason about number.

I was thinking just increment a counter every 16.6 ms, doesn''t have to be super precise. Doing math on the timestamp could work too and would probably be better. The important thing for me was that it was easier to think about frame numbers instead of timestamps for some reason -- even if it happens to be a timestamp under the hood.', 'phritz');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('VlwLIdUBOiWYVkdEITrdh', 'w-m-1MtivCOqjAD1DEVPY', 1663807593000.0, '> I was thinking just increment a counter every 16.6 ms

I don''t think we have any timers that precise either on the client or server. ', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('wX9K21AVIMKi5y9D12BoG', 'w-m-1MtivCOqjAD1DEVPY', 1663807728000.0, '> > I was thinking just increment a counter every 16.6 ms
> I don''t think we have any timers that precise either on the client or server.

Then how could we use precise timing information from the sending client? Or from the server for that matter? Happy to switch to slack if easier to discuss.', 'phritz');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('aEx58UriD4byZCP-5i4U4', 'w-m-1MtivCOqjAD1DEVPY', 1663808101000.0, 'We have `performance.now()` (https://developer.mozilla.org/en-US/docs/Web/API/Performance/now) on the client which is super precise. On the server we only have `Date.now()` and CF hobbles it, so it''s way less precise.

But both are just a way to ask what time it is, they don''t schedule code to run. If you wanted to increment a counter, you need to run code on a timer. For that we have either `setTimeout()` which can be a little wobbly, or `requestAnimationFrame()` which is more precisely the next frame.

But we wouldn''t want to run either of those constantly to just count, because battery issues. Our users would hate us.

We can grab the time when an event happens and translate it to some other coordinate system, but we can''t run a timer loop just to count frames.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('GSRjQ8yqOrFjkrPpnBAkC', 'w-m-1MtivCOqjAD1DEVPY', 1663808307000.0, '> We can grab the time when an event happens and translate it to some other coordinate system,

This is what I was imagining. 

But also, we _currently_ try to run a setinterval every 16ms on the server. I would expect it to not be precise, but if we did it every 4 frames or whatever then that lack of precision matters less. ', 'phritz');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('R4a85l3jngvw-B_e33kvB', 'w-m-1MtivCOqjAD1DEVPY', 1663808882000.0, 'Right, but we stop when the events stop coming. I think we need to stop when there''s no new input. And especially we can''t loop on client. Sounds like we''re aligned!', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('eLqBe9edPcNXifniMwPMa', 'w-m-1MtivCOqjAD1DEVPY', 1672870059000.0, 'More context:

- from @aboodman: this Jan''23 thread in slack: https://rocicorp.slack.com/archives/C013XFG80JC/p1672866903559879?thread_ts=1672866826.489229&cid=C013XFG80JC
- docs ingar produced: 
  - https://www.notion.so/replicache/Reflect-Batched-Writes-c99c237c0e0e472d9999c5188bd5b34d
  - https://www.notion.so/replicache/Mutation-batching-and-passthrough-client-timestamps-cef2fed007004b029e2fe5e78d14ec1a
- please note: fixing this issue requires a design sketch
- please note: do not overlook the N^2 communication complexity of poke-per-mutation which adds up really really quick. Eg 30 users in a room with 10% sending a mutation per frame implies at the server an incoming mutation rate of 3 mutations per frame which results in 3*30=90 outgoing messages per frame. That''s 90 messages * 60 frames per second = 5400 outgoing messages per second which would consume a huge and undesirable amount of cpu (see above, but rate of <=2000 is more prudent)
', 'phritz');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('O1HSHp_Si7L1qZOqQGKlC', 'w-m-1MtivCOqjAD1DEVPY', 1672871724000.0, 'I just read through all this and these notes are surprisingly still current and useful!

A few follow-up notes. I think 4kb per poke is way too high of an estimate. The typical poke is a mouse movement update, it''s going to be tiny. Here''s a sample from replidraw:

```
["poke",{"baseCookie":147,"cookie":148,"lastMutationID":260,"patch":[{"op":"put","key":"client-state-5026c476-d373-4376-82d3-9f26875b78ab","value":{"overID":"","selectedID":"teBpwxAVPo-6_j0RmXBqB","userInfo":{"avatar":"ðŸ£","name":"Chick","color":"#f94144"},"cursor":{"x":1185,"y":274}}}],"timestamp":1672870745932}]
```

This is 314 bytes. If we say 512 bytes is more typical then maybe we can send 4kb/512*2000 = 8000 messages per second. In that case our 30 users in a room example above works!

And there''s obviously a lot of room to reduce the size of this message! Just adding snappy compression might get it to 200 bytes!

===

Also - even if we do one poke-per-server-frame, we might not end up sending that much less data. If we are assuming that multiple clients are moving in each frame then a change from two active clients would look like:

```
["poke",{"baseCookie":147,"cookie":148,"lastMutationID":260,"patch":[{"op":"put","key":"client-state-5026c476-d373-4376-82d3-9f26875b78ab","value":{"overID":"","selectedID":"teBpwxAVPo-6_j0RmXBqB","userInfo":{"avatar":"ðŸ£","name":"Chick","color":"#f94144"},"cursor":{"x":1185,"y":274}}}],"timestamp":1672870745932},{"op":"put","key":"client-state-5026c476-d373-4376-82d3-9f26875b78ab","value":{"overID":"","selectedID":"teBpwxAVPo-6_j0RmXBqB","userInfo":{"avatar":"ðŸ£","name":"Chick","color":"#f94144"},"cursor":{"x":1185,"y":274}}}],"timestamp":1672870745932}]
```

~560 bytes.

If the main cost of sending data over the socket is just total bytes sent (if there''s no overhead per-message) then we don''t win much by sending one poke per server-frame.

===

I continue to be open to the idea of starting out doing one-poke-per-server-frame and seeing how that goes. The server timer will flex a bit but hopefully not too badly. Also because of the way we are constrained to replay messages in the order they were processed by the server there is a natural limit to how much client-side timestamps can help.

If a source client glitches badly and delivers a message to the server very late, then other mutations will run before it, and the receive clients will *have* to play those pokes in that order no matter how much buffering they do.

===

I could see either of these approaches working. I think slight bias for poke-per-mutation because (a) no reliance on server clock, (b) don''t see data size being significantly less + (c) assuming there''s no difference between sending more data in less messages or less data in more messages.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('mZkhFDXDeQfjZCUBIVqRs', 'w-m-1MtivCOqjAD1DEVPY', 1672872251000.0, '@aboodman as a reminder the cpu consumed to send the message is primarily a function of _number of messages sent_, not their size. It''s the i/o system calls that are costly, not copying the bytes around. ', 'phritz');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('-MVGLyIhtsKF-H96ouN3v', 'w-m-1MtivCOqjAD1DEVPY', 1672872582000.0, 'Ah ok, I forgot about that. Then your argument makes a lot of sense.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('TAgtqhwOyEAuNWmT-vT1w', 'w-m-1MtivCOqjAD1DEVPY', 1672872848000.0, 'Oh one more thing: have to factor in offline or recovered mutations, how do they play with the new loop.', 'phritz');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('uGi1seP2nnYj7-7N8NWNm', 'w-m-1MtivCOqjAD1DEVPY', 1672897041000.0, 'I had one last thought here -- if we''re only sending pokes every 4 frame, we can batch together all 4 pokes destined for one client into a single websocket message. I believe this gives us an effective safe rate of 8k messages outbound per second?', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('IS-h7a4GF3rCUGlQ-zMqG', 'w-m-1MtivCOqjAD1DEVPY', 1672897415000.0, 'Wait, in that case couldn''t we do the same trick with the poke-per-mutation? If the cost is the syscalls to send messages, and we know we can do 2k 4kb messages per second.

Say we have 10 clients moving continuously at 60fps. So we need to send `60*10*10` ~= 6k pokes per second. But actually we can group all pokes that need to go to a single client together into a single message every 4 frames.

So every four frames, each client will receive 10 pokes for each of the four frames as one socket message. So the DO is really only sending `6000/40` ~= 150 messages / second (!?)

If the average poke really is 500b, this means in this scenarios, these socket messages will be fat!: 500b*40 = 20kb! But I think that''s probably fine, maybe even better than what we''re doing. Even with 30 clients moving continuously the messages are 60kb each. And we can always optimize the messages if that starts to become an issue.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('wU0swyTqxk-ho15Vh_nPe', 'w-m-1MtivCOqjAD1DEVPY', 1672898563000.0, '> couldn''t we do the same trick with the poke-per-mutation

i think yes, the implication is that we should do that or something like that. which is why this task feels to me like rewriting the game loop, as opposed to a fundamentally different thing. but whatever about the nomenclature it seems like it should work at the scale we are talking about (even in the extreme example you give which we know is pushing the perf edge like a vercel conf experience). will be interesting to see how the client replay logic pans out, there are a lot of interesting edge cases (delayed source client sends followed by realtime sends, delayed receipt by the receiving client followed by bursts of realtime mutations, etc). Server authoratativeness ensuring causal consistency is a really nice bedrock to build on for this. ', 'phritz');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('x2oenhixcSXWs2_rYJhuE', 'w-m-1MtivCOqjAD1DEVPY', 1673293929000.0, '@aboodman a few questions about things you said in https://rocicorp.slack.com/archives/C013XFG80JC/p1672866903559879?thread_ts=1672866826.489229&cid=C013XFG80JC, you said:
> - Move FF to connect
>   - will have to put lock around it
>   - this will fix that FF bug
> - Rip out the whole process\* heirarchy
>   - put a cache in front of storage (SortedMap)
>   - immediately execute changes against cache
>   - every n ms
>     - if there are changes:
>       - processdisconnect
>       - flush
> - remove `baseCookie` from `ClientRecord`. It shouldn''t be there.
> - overlap turns
> - add source timestamps
> - implement buffering

Questions:
1. Re: `remove baseCookie from ClientRecord. It shouldn''t be there.`, is that because it should be part of connection state (i.e. ClientState)?
2. What do you mean by `overlap turns`?



', 'grgbkr');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('o1Fyeb5xy05PXNrQfF0Gw', 'w-m-1MtivCOqjAD1DEVPY', 1673295234000.0, 'Yeah I can''t remember what the reason was we were storing `baseCookie`, but it seems pretty clear that is connection-specific state. Client says "hello client 42 here, connecting from state X, wassup".', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('_wccrGWmsE3aGawwV-Fqw', 'w-m-1MtivCOqjAD1DEVPY', 1673295301000.0, 'By _overlap turns_ what I mean is that there is no reason to just sit there and do nothing while we are waiting for the persist to happen. We can begin processing the next batch while the IO is outstanding.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('LbR0MVg8gWJGgeohlB_Nk', 'w-m-1MtivCOqjAD1DEVPY', 1681147166000.0, 'ðŸ¤¯', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('Uvw9M38r1Q8qjRPJ8GtcW', '_7MfUNR1OlSWhzplIW2EY', 1663600490000.0, 'Here is something strange:

```
â–¶ npm run perf -- --run "populate 1024x1000 \\(clean, indexes"

> replicache@11.2.1 perf
> npm run build-perf && node perf/runner.js


> replicache@11.2.1 build-perf
> node tool/build.mjs --perf

Running 6 benchmarks on Chromium...
populate 1024x1000 (clean, indexes: 0) x 7.33 MB/s Â±43.4% (7 runs sampled)
populate 1024x1000 (clean, indexes: 1) x 43.99 MB/s Â±14.5% (19 runs sampled)
populate 1024x1000 (clean, indexes: 2) x 32.55 MB/s Â±24.2% (14 runs sampled)
populate 1024x1000 (clean, indexes: 3) x 26.39 MB/s Â±25.9% (11 runs sampled)
populate 1024x1000 (clean, indexes: 4) x 21.99 MB/s Â±9.8% (9 runs sampled)
populate 1024x1000 (clean, indexes: 5) x 19.53 MB/s Â±8.7% (8 runs sampled)
Done!
```

As you can see, no indexes is a ~5x slower than one index. Something is fishy! Adding a `noop()` mutator and calling that before the call to `populate` gives a more predictable (expected) result:

```
â–¶ npm run perf -- --run "populate 1024x1000 \\(clean, indexes"

> replicache@11.2.1 perf
> npm run build-perf && node perf/runner.js


> replicache@11.2.1 build-perf
> node tool/build.mjs --perf

Running 6 benchmarks on Chromium...
populate 1024x1000 (clean, indexes: 0) x 76.89 MB/s Â±5.2% (19 runs sampled)
populate 1024x1000 (clean, indexes: 1) x 44.59 MB/s Â±6.2% (19 runs sampled)
populate 1024x1000 (clean, indexes: 2) x 32.34 MB/s Â±8.0% (15 runs sampled)
populate 1024x1000 (clean, indexes: 3) x 26.04 MB/s Â±13.3% (11 runs sampled)
populate 1024x1000 (clean, indexes: 4) x 22.30 MB/s Â±27.7% (9 runs sampled)
populate 1024x1000 (clean, indexes: 5) x 18.57 MB/s Â±8.6% (8 runs sampled)
Done!
```', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('xvZ2dS_ehRnS7Hl-R0_GW', '_7MfUNR1OlSWhzplIW2EY', 1663680176000.0, 'One more data point. Instead of a `noop` mutator, we can add a `sleep(100)` before measuring. This makes it clear that we are waiting for some initialization... Changing things to `await rep.clientID;` means we wait for everything to be ready before we start the perf benchmark. We should have done this in a bunch of places throughout this tests.', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('LAqwcmYZK8_tAMinhn7Wz', '_7MfUNR1OlSWhzplIW2EY', 1663680225000.0, 'Next up. Does this mean that there was a perf regression or it was just the above bug being manifested? Will look more later...', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('bQJkYyMnu1SRWrWcwMTKl', 'Q3YfcKImxaRAo9CUO8br9', 1663623425000.0, 'I''m not sure how else we could do it reasonably. Number of keys?

There is a very strong (universal?) pattern of using prefixes to the keys to separate different kinds of data. We don''t enforce or recommend a separator for those keys. But it''s highly likely that keys which share a prefix are going to be similar in size.

Can we exploit that? We don''t need to measure every todo, just a few of them to get an idea how large todos are. Like we could sample 1% of todos or even 0.1%.

I suppose we could even sample a subset of keys independent of prefix on the theory that perf will be dominated by the most common prefixes.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('JA16nds2ffH5qo76yXNXm', 'Q3YfcKImxaRAo9CUO8br9', 1663623649000.0, 'We do tell people that key sizes should be 100B to 10KB: https://doc.replicache.dev/performance#typical-workload. So the mid of that is 1KB. If we want chunks to be 64KB, which I think is what we''re aiming for, then we''re talking about approx 64 keys per chunk. However if the user doesn''t follow our advice that could result in very large chunks.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('9jfGukOH5dNyBV_cuYgJI', 'Q3YfcKImxaRAo9CUO8br9', 1663666102000.0, 'The B+Tree could be based on the number of keys instead.

The LazyStore could also be based on number of chunks instead of the estimated size of a a chunk.

There is always back to square one and use binary ðŸ¤· Maybe worth doing a "spike" for that ', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('WZe133ewKyi4_AP3QnQHQ', 'EbeuhaYtM2Z6OTR2qLYTP', 1663592180000.0, 'I looked at this and a few things stands out.

1. The codesandbox has a bug:

```diff
  mutators: {
    putFeatures: async (tx, updates) => {
      for (let i = 0; i < updates.length; i++) {
-        await tx.put(String(i), updates);
+        await tx.put(String(i), updates[i]);
      }
    }
  }
```

Which means that the array of `25413` items gets inserted `25413` times! Fixing that makes the sandbox behave better and we can look at the perf issues.

2. The click handler does not await the mutator so the numbers being printed has no real significance. The log is also including the download time. Fixing these things gives us `Put time: 773` which is much more inline with our performance metrics.

3. `getSizeOfValue` is a bottle neck here. `getSizeOfValue` is really just an approximation and it is used as a heuristic to determine how to partition the B+Tree as well as to determine how much of the data to cache in memory. One possible short term solution is to compute an average for the N first entries of arrays/objects and use the average of that as the basis for the size of the whole array/object.

4. Once `getSizeOfValue` is changed using averages the big remaining item is `hashOf` which is the native hash function provided by `crypto.subtle`. The good thing is that we are moving away from hashes in an upcoming release.', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('Oy3u5ufBhoVYMDaTMoYOC', 'EbeuhaYtM2Z6OTR2qLYTP', 1663592224000.0, 'And here is the forked code sandbox: https://codesandbox.io/s/angry-wright-qugo6h', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('UYDaHypKTXpdr2munmEnW', 'EbeuhaYtM2Z6OTR2qLYTP', 1663593927000.0, 'Here is a trace with `getSizeOfValue` being replaced by `1`.

[Profile-20220919T152113.json.zip](https://github.com/rocicorp/replicache/files/9599253/Profile-20220919T152113.json.zip)

', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('IlAqOT1OUE21P1g_ewnDq', 'EbeuhaYtM2Z6OTR2qLYTP', 1663641173000.0, '> The codesandbox has a bug:

Whoops.

> Fixing these things gives us Put time: 773 which is much more inline with our performance metrics.

I do not see this on your forked codesandbox. I see numbers closer to 1500.

Chrome:

<img width="622" alt="Screen Shot 2022-09-19 at 4 29 31 PM" src="https://user-images.githubusercontent.com/80388/191154014-126eb547-47c0-4046-b386-f73de5c4a278.png">

Edge:

<img width="564" alt="Screen Shot 2022-09-19 at 4 31 11 PM" src="https://user-images.githubusercontent.com/80388/191154154-29b852a3-2fe4-49e4-989e-04accad00bf5.png">

Firefox:

<img width="646" alt="Screen Shot 2022-09-19 at 4 31 25 PM" src="https://user-images.githubusercontent.com/80388/191154185-651965fd-eec7-4918-872b-32bb300f8e12.png">

Safari:

<img width="524" alt="Screen Shot 2022-09-19 at 4 31 59 PM" src="https://user-images.githubusercontent.com/80388/191154272-6e1b205f-99f0-4eee-b45e-ce08b9d9c3aa.png">

Are you sure you weren''t quoting the number after making the changes to hashing and/or getSizeOfValue?', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('uXYLEn-256k6WJF2o1-bp', 'EbeuhaYtM2Z6OTR2qLYTP', 1663641200000.0, '<img width="248" alt="Screen Shot 2022-09-19 at 4 33 16 PM" src="https://user-images.githubusercontent.com/80388/191154407-99d5bd99-a02d-482b-99e3-b3f2ccc5ada1.png">
', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('xghFfiuFwny0uL7TVjNiu', 'EbeuhaYtM2Z6OTR2qLYTP', 1663643281000.0, 'Looked at the trace, a few interesting things. See video here: https://drive.google.com/file/d/19V6KupZkLPrv-H41-PoOQhCyJX0KLMWO/view?usp=sharing

0. I still see times 2.5-3x slower than expected by perf test. If you are seeing times like 600ms, perhaps it''s because your computer is faster. What times do you see locally for the populate 1024x1000 test? Is it 30ms like the continuous test sees, or something significantly faster?
1. A huge percent of the time in your new trace is in defensive deep clones for the argument to mutate and put.
2. We actually clone the entire dataset *twice* - once to protect mutate, and then again to protect put!
3. We have to find a way to get these defensive copies off for tom. Maybe the right thing is that in release mode we don''t defensive copy ever. We just rely on typescript and documentation. And in dev mode we do the defensive copies. Didn''t we do something like this for read already?
4. Maybe copy isn''t even the right thing -- maybe in dev mode we should do deepFreeze, not deepClone so that user gets an error when they modify something they are not supposed to in read-only.
5. Hash is also a huge cost here. Can we get the uuid change in sooner?
6. Regarding getSizeOfValue() I still see it contributing meaningful to the trace 4-10% depending on what part you''re looking at. How can that be possible if it was returning a constant value?

That''s all. I think overall these are very exciting results as it means there are very easy low hanging fruit to *massively* improve populate. Let''s make some of these changes and make a customer stoked! I think it would be epic for Tom to be able to use 100MB files in his app.

But I think we also need to understand (0) - why is tom''s test case slower than what we''re testing. Perhaps it''s just because the values are more complex.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('76ddgcnTCffiX5H2Tnzgh', 'EbeuhaYtM2Z6OTR2qLYTP', 1663664844000.0, 'Some comments...

## getSizeOfValue

The getSizeOfValue is strange. Must have uploaded the wrong trace? Here is a new one: [Profile-20220920T095422.json.zip](https://github.com/rocicorp/replicache-internal/files/9605333/Profile-20220920T095422.json.zip) I think the trace I uploaded was with getSizeOfValue using an average for arrays and objects.

## deepClone

deepClone is unfortunate. In this case we do a double deepClone which is even more unfortunate. Good catch.

This is a longstanding issue. You''ve argued that you want to be able to mutate the arguments passed into mutators and you''ve also argued that you want to mutate the return values from get/scan in a mutator. To allow that we have two options:
1. Deep clone
2. Proxy to do copy on write. A while back I tried using Immer to get a sense for the performance implications and it was slower than deep clone. Maybe worth looking at something more specific than immer?

I still think we should freeze things. Keep them frozen in debug and skip the freeze in release. One benefit of using freeze is that we can quickly check if something is already frozen to skip the deep freeze. There is no way to similarly check if an object was previously cloned.

### More about the double clone

One clone is needed to clone the argument when we rebase. This is so that the data we store in the LazyStore is not mutated. This clone could be removed when the mutator is called manually. The other clone is for when putting data into the LazyStore.

## Speed 

TMWC''s test: I''m getting 611 ms for 105MB => ~200MB/s. I made sure that I disabled all the asserts (src/config isProd=true)

For the perf tests I''m seeing  ~100MB/s with the stubbed out getSizeOfValue

The difference is probably the shape of the objects but I''m not certain yet.


## Hash

I think we can change to UUIDs now. It is not a format change since we never validate hashes
', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('F2xv8ZO8R3BKAXHaUJuh9', 'EbeuhaYtM2Z6OTR2qLYTP', 1666297964000.0, 'Closing. We did a bunch of things here and we do not know what tmcw is considering to still be slow.', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('3EHETdeph3qH3GwuZM71k', 'cUeAIhPNRUHux1RxsHt5f', 1663895555000.0, 'Argh, pinning a DO to an explicit `jurisdiction` [is only available if we get object ids via `newUniqueID`](https://developers.cloudflare.com/workers/runtime-apis/durable-objects/#restricting-objects-to-a-jurisdiction). Of course we [derive the id from the room name using `idFromName`](https://github.com/rocicorp/reflect-server/blob/350dc90a01654629671af8e51a18c8b552b7180a/src/server/auth-do.ts#L146) instead. In order to close this issue we would probably need to:
- stop deriving the DO id from the room name and instead use random ids via `newUniqueID`. if we want to keep passing the room name in connect as seems desirable, this means we should keep a map from room id to DO id, probably in the auth DO. workers could cache an entry forever once they have seen it.
- stop creating rooms implicitly and instead add an explicit room creation interface or have some mechanism by which the ''create this in the EU'' bit is passed in the first connect(s)', 'phritz');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('GoxCfccvQy2eA-FRBKCau', 'cUeAIhPNRUHux1RxsHt5f', 1667624196000.0, 'Closing in favor of rocicorp/reflect-server#160 ', 'phritz');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('BbhSha9x3e_fzE5RnNCQu', 'KBWekcPn50kLp9V8V3MEo', 1663352576000.0, '> decide whether a room can be re-created. my sense is ''no'' but it''s not a strong feeling

There is an almost fundamental rule of the universe in Replicache that every single time we reuse an identifier (outside of user data, where the sync engine knows how to deal with it), it causes a problem.

At this point without even working it through, I have a strong visceral gut reaction that reusing the room IDs will definitely break something, somewhere. Perhaps multiple things.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('RgYph7JV1pJ3K4bZfINzh', 'KBWekcPn50kLp9V8V3MEo', 1663352664000.0, 'So many parts of Replicache are based on the intuitions flowing from an immutable append-only DAG. Every single time we make something that doesn''t follow that pattern we end up regretting it.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('2VskrBFIucLJJIXjcdwPe', 'KBWekcPn50kLp9V8V3MEo', 1663352854000.0, '> if ''no'' then we need to decide the mechanism by which a deleted room is prevented from being recreated, as well as say what should happen when a client tries to connect to a deleted room. this last part is related to the question of whether we need to delete room data from all clients.

I guess following the pattern, we should tombstone the room.

I don''t think we have a concept in the protocol of "the server you are trying to talk to has been deleted / doesn''t exist", but maybe we should add one analogous to `ClientNotFound`. Then the client could use this to delete its state too.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('lDOg_TqqZJ4r2EJxih4A-', 'KBWekcPn50kLp9V8V3MEo', 1663353191000.0, '> I guess following the pattern, we should tombstone the room.

Maybe we model this as a map from room name to status stored in the authdo, expecting that we''ll find other uses for this kind of info in the future. 

> I don''t think we have a concept in the protocol of "the server you are trying to talk to has been deleted / doesn''t exist", but maybe we should add one analogous to ClientNotFound. Then the client could use this to delete its state too.

Assuming that noam tells us that this is required. If he doesn''t say so, do we do it anyway? Seems prudent, but perhaps beyond what one might expect, and any time we can not do something it means we can do something else more important...', 'phritz');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('sm2_iNfw2y3rHIo13RIzd', 'KBWekcPn50kLp9V8V3MEo', 1663353349000.0, '> Assuming that noam tells us that this is required. If he doesn''t say so, do we do it anyway? Seems prudent, but perhaps beyond what one might expect, and any time we can not do something it means we can do something else more important...

Agreed on spirit of doing min we can get away with. I think we can''t have the clients re-creating rooms accidentally as it would be very confusing, incorrect, and could even violate spirit of this feature request.

Right now `reflect-server` implicitly creates room on connection so I think something minimally has to be done about this path. Like right this second if you delete a room and a client is connected, I believe the clients will reconnect and then other bad things will happen (#152).

If all that happens is clients get disconnected and can''t reconnect / get errors, I think that''s fine for v1 of this feature (modulo noam saying it''s not fine).', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('MiY__O0lv5XA_PpuDbrh-', 'KBWekcPn50kLp9V8V3MEo', 1663353440000.0, 'In most of the Replicache servers we have converged on having an explicit `createSpace()` path - connection doesn''t implicitly create. This was useful for many reasons. Perhaps we need the same here.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('vshBNTldCALDLhPeQ8SKx', 'KBWekcPn50kLp9V8V3MEo', 1663354120000.0, '> I think we can''t have the clients re-creating rooms accidentally

Yeah agree we 100% need to prevent room re-creation. I was referring to deletion of deleted room data from clients. If we can skip doing that, that seems like less work. 

As scoped, we are saying that when we get the call to delete a room we transactionally:
- log all users out of that room
- tell the room to delete all its data, and ensure it completes
- remove all connection records for the room
- record that the room is deleted

As you say we''ll have to add a check at room creation time to enforce not re-creating rooms. Will leave it up to whoever works on this if we add the explicit creation step, which seems sensible to me.', 'phritz');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('IuT9oxV9hirtJVuadurv9', 'KBWekcPn50kLp9V8V3MEo', 1663354471000.0, '> ensure it completes

I think we could also get away with reporting an error if any of this fails and letting customer re-call?', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('14jiEt_XcpjW3X2LNkjmj', 'KBWekcPn50kLp9V8V3MEo', 1663355213000.0, '> I think we could also get away with reporting an error if any of this fails and letting customer re-call?

Sure, but it''s specifically called out in the docs that `storage. deleteAll` might not complete in a single call for rooms that store a lot of data, and should be called again and it will pick up deleting where it left off the previous time. If deleteAll doesn''t complete we could have the customer could re-call the api but that leaves a window where the room still exists but is in a totally broken state with some but not all its data deleted, and clients can still connect to it. Seems better to just kill the room transactionally by repeating the call to deleteAll if it doesn''t complete, and if that turns out to be a perf problem in reality then we can deal with it. ', 'phritz');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('VQNkGmINdc8GmjW1PQMXN', 'KBWekcPn50kLp9V8V3MEo', 1663361215000.0, '@phritz there is no other metadata I''m aware of that would need to be cleaned up.

Note the auth do currently only stores information about open connections (and periodically gcs them).  With the idea of it keeping tombstones for deleted rooms, I think we should likely have it store records about all rooms.   ', 'grgbkr');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('3EZuCHEMAcwnNzm4q-A0c', 'KBWekcPn50kLp9V8V3MEo', 1667422192000.0, 'Most recent feedback from noam: https://www.notion.so/replicache/Monday-Priorities-shared-ce186403a079408abcdbb6aa123c48f8#efec9142bcdd406799d9098ba2df3658', 'phritz');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('VzRcEGss4LM486C6AHCdz', 'KBWekcPn50kLp9V8V3MEo', 1667437173000.0, 'I took a look at what''s required here and the only thing not already covered is that we need to switch how we derive DO object IDs (the things you pass to the namespace to get a stub). We currently use `roomDO.idFromName(roomName)` and we need to move to `roomDO.newUniqueId()` because only `newUniqueId` supports creating DOs that stay in the EU. So we''ll need to keep a mapping from room name to object ID so that we can take a roomName that is passed into `connect` or whatever and look up its object ID, which is required to get a stub. 

In order to support existing rooms that monday created with `idFromName` we have a choice. Option 1 is have monday call an endpoint that creates `room name => object ID` records for existing rooms on a one time basis. Option 2 is to overload how object ID is derived from a roomName, either by lookup (for new rooms) or via `idFromName` (for old rooms). This involves a helper that can distinguish between those two kinds of rooms. I have done things like (2) in the past because it is easier and I''m pretty sure I regretted it each time. So I''m probably going to do (1). 

---
So in summary what needs to happen here is:
- [ ] in the authDO introduce a `roomRecordMap` which maps from `roomName (string) => RoomRecord` where `RoomRecord` holds the `objectID` and a `status` bit (open, closed, deleted).
- [ ] add a `createRoom(roomName: string)` endpoint that adds an entry to `roomRecordMap` and remove implicit creation from `connect`. Ensure `createRoom` errors if a record for the given `roomName` already exists.
- [ ] add `createRoom` call to reflect client
- [ ]  add a migration endpoint that [enumerates existing DO instances via this api](https://api.cloudflare.com/#durable-objects-namespace-list-objects) (namespace id gotten from [this api](https://api.cloudflare.com/#durable-objects-namespace-list-namespaces)) and then creates a `RoomRecord`s for each, mapping room name to `idFromName`-derived object id. This call will need to take the customer''s CF api token and account id as parameters.

(at this point we could release the breaking change; what comes after could come in a second release that enables data deletion)

- [ ] extend creating a room to support rooms that need to stay in the EU `createRoom(roomName: string, jurisdiction: undefined|''eu'')`, pass that bit into DO instantiation, and keep it in the `RoomRecord`.
-  [ ] add an endpoint to "close" a room by logging everyone out of it and changing its `RoomRecord` status to closed. Have `connect` only accept connections to rooms that are open.
-  [ ] add an endpoint to delete a room. The room must first be closed. When we delete a room we delete all its data and then mark the `RoomRecord` status as deleted. 
-  [ ] probably tweak how the version string in the worker path works, [currently only internal apis have versions](https://github.com/rocicorp/reflect-server/blob/1954e4e842b9b398003390b2c05493c03d9a3c15/src/server/dispatch.ts#L28). I''m inclined to add a version string to the externally accessible paths (eg, `/api/v1/create`) and remove the version string from the internal apis because  we never have different versions of room and auth dos trying to talk to each other. We deploy the room and auth do together, even in RaaS. Or i dunno, maybe just keep it @grgbkr ?
- [ ] integrate with replidraw-do and anything else using reflect-server
- [ ] update docs
- [ ] release
- [ ] get users to switch

---
List of things the customer is going to have to do (updating this list as we go):
- (probably) return the "EU-only" bit as part of UserData in the auth call
- make an explicit `createRoom` call from the client instead of relying on `connect` to create room implicitly
- handle new `connect` error case: the room is closed or deleted
- ack that the auth call covers both room creation and connection 
- (TBD how) run the room record migration once
- to delete a user''s data, call `close` on the room, then `delete`
', 'phritz');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('22w3AH6E-ApCkA-fJRZOk', 'KBWekcPn50kLp9V8V3MEo', 1667687254000.0, 'Design sketch for the migration step: https://www.notion.so/replicache/authDO-storage-migration-design-sketch-c2a15c2eb26149bc9dbe0c207fdc0851', 'phritz');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('xUEBPf_-x-wLDqkOp-ARH', 'KBWekcPn50kLp9V8V3MEo', 1672740123000.0, 'I think this is done right @phritz ?', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('Bb2rcYhs5pkqHL1qWng5p', 'KBWekcPn50kLp9V8V3MEo', 1672775365000.0, 'Done in the sense of the code is there and it is available in 0.19. I don''t think anyone is using it yet. I''m fine closing this bug and just treating arising issues as follow ups.', 'phritz');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('PbtGhxjsfvo3CpegpsVku', 'g1J-K1Wj1lyepl3m7KIE0', 1663289152000.0, 'Random test from @phritz on Android/pixel 4a (circa 2020):

https://user-images.githubusercontent.com/80388/190532866-0f8b053e-4c73-40ba-a743-823c5534d381.mp4

So either customer''s phone or their network conditions.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('TjeoJtJNyWVNxlxJqoa3g', 'g1J-K1Wj1lyepl3m7KIE0', 1672740230000.0, 'I think the task here is simply to test on a variety of old phones and makes sure performance looks good. Without a repo of what went wrong in original case, not sure what else we can do for beta (I guess there could conceivably be metrics, but that seems overkill).', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('q7gbRbNu79j9wdUgChgBu', 'gdMJp5JtT1YYdkWKJ-d9D', 1663382641000.0, 'OK got some clarification on this from aaron. This issue is about ensuring reflect client sends mutations individually. It already [does not wait for mutations to accumulate before sending them](https://github.com/rocicorp/reflect/blob/b12a12df9ea4ac511649eeff5eaeed1da42133ef/src/client/reflect.ts#L95), but that doesn''t preclude multiple mutations from having landed by the time we actually crawl the memdag to get the pending mutations. For this issue we want to NOT [bundle all the mutations together into one push request](https://github.com/rocicorp/replicache-internal/blob/4cddcfdb1aeac8fac1b92b0d1fa9ecd6683462bc/src/sync/push.ts#L140) but instead `callPusher` on them individually. Unclear if we should keep using the PushRequest to wrap these mutations. On one hand it''s there and works. On the other hand it''s got unnecessary stuff in it.

More generally all the connection loop stuff that''s in replicache might be doing us a disservice in reflect because reflect is just a different situation (eg, replicache protocol is stateless). As part of this issue maybe take a look and see if we should maybe use a totally different push path in reflect than what''s in replicache. Eg can ws reconnect happen automatically in reflect in a way that doesn''t make any sense in replicache?', 'phritz');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('McvFVMF02OIggJVxaLwix', 'K7XngRCzZgJJVEZiQ6JX0', 1663900779000.0, 'Might we get this automatically as part of rocicorp/reflect-server#149 ?', 'phritz');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('1i8rmSSzi9hMr7pYVJlhU', 'K7XngRCzZgJJVEZiQ6JX0', 1663900993000.0, 'I''m not sure. Have to think how it interacts with the socket interface. I don''t think "for free", but for cheap probably.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('4-KrmH_jueY7WagSHxu08', 'K7XngRCzZgJJVEZiQ6JX0', 1672740312000.0, 'Related to rocicorp/mono#218 ', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('u-pi07xpwXNOpJP2o2saq', 'K7XngRCzZgJJVEZiQ6JX0', 1677080646000.0, 'We do report this error to the client but we do not deal with it in any way att the moment', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('DEEhMc38dK_zQtb9cXktM', 'p5Fif_PwXzinwCvOJucOV', 1663310988000.0, 'Fixed by https://github.com/rocicorp/reflect-server/commit/593f9ceec8351761c692c55924dce57676e51bf1', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('FL8li4nzGHSxLTghbuG80', '9QHgQYv9VgCJGaikLDNlo', 1677784897000.0, 'Actually not sure about this one it might be more subtle than it looks. Let''s talk at the meeting. Relatedly I think maybe we should get rid of `createReflectServerWithoutAuthDO`.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('UpdvHvCEmP_eS1H0hOwJe', '9QHgQYv9VgCJGaikLDNlo', 1678220922000.0, 'Delete `createReflectServerWithoutAuthDO` and related paths also as part of this.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('m-0ok3s4fhGbzn4FAo4gn', 'EOZMl2CohCGenk9DOmvi2', 1663777672000.0, 'For clarification, you have to set `allowUnconfirmedWrites = false` to enable the output gate.

https://github.com/rocicorp/reflect-server/blob/350dc90a01654629671af8e51a18c8b552b7180a/src/server/reflect.ts#L24-L32', 'ingar');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('JsQiDH6yPVp5VzjXhjRZk', 'EOZMl2CohCGenk9DOmvi2', 1663908360000.0, 'closing in favor of https://github.com/rocicorp/mono/issues/318', 'phritz');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('MKXAgzs4N_b46GJOtB7S4', 'vNwsE_q89dNPC9vl3HeZP', 1672928968000.0, 'I''ve previously mentioned the size (and performance) of zod being a problem.

After looking at this again, I think we should go with `simple-runtypes`. It seems to have a good trade off between performance and compile size. It is not using a chaining API so tree shaking is good as well.

https://moltar.github.io/typescript-runtime-type-benchmarks/', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('G9GVt8fJuk8tLItm0uyeh', 'vNwsE_q89dNPC9vl3HeZP', 1672939253000.0, 'Is there any way we can just not do anything about this right now? It seems far less important than so many things we have to do that solve more acute problems we or our customers are experiencing. ', 'phritz');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('-kjfloRGbMujLDA4BGIKY', 'vNwsE_q89dNPC9vl3HeZP', 1672960143000.0, '*edit* - I remembered the current endpoints are already validated with superstruct.

It came up because of routing.  I wanted to have validation of all routes happen mechanically, and we use zod everywhere else.

It''s really trivial to switch from one of these to the other. They all have about the same API. And there''s not really a cost to doing it, the cost is in implementing router support, not in annotating the call sites with one schema system or the other.

So I could just keep using superstruct as part of routing, that''s easy, but it''s also easy to choose a different one. I think it makes sense to make this choice as part of the routing work. 

For my part, from the benchmark Erik, there are a bunch that are way faster in "assertion" mode than parsing mode. We don''t need parsing mode. The "strict assertion" mode would work fine for us. maybe we could even enable in production which would be amaze! Should we use one of those instead? Maybe since this a server and we dont'' care about the size as much we should choose the fastest one as a starter. Like I said, since they all have equivalent APIs we could change it easy enough later.

~It came up because of routing, the reason I wanted to work on routing was to add validation.~

~I chose zod because we already have it everywhere else and have experience with it. The size concerns don''t matter because it''s on the server, and I believe perf has increased significantly since the benchmark erik points to.~

~I don''t think there is a way to avoid sorting this out soonish, it''s a piece of low-level infrastructure, not a nice-to-have. We need to report errors to users. If we don''t have a system for this we will end up adding one-off error checking to every entrypoint and unit testing those. The work will be greater.~

~OTOH, I don''t think it''s a big deal to just pick one and go with it. They all have basically the same API.~

', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('fiUB1Yq6TH6UerrCTaiLh', 'vNwsE_q89dNPC9vl3HeZP', 1677703455000.0, 'We decided not to do this.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('wd6LWJrhTAq53N66Yk5kx', 'XVKywqA9fFHLhfQumEKvg', 1663278318000.0, 'This should not be possible if client and server are correct, but currently server is *not* correct (output gate) so this does happen. And anyway we should complain loudly if either side is being incorrect and stop doing wrong things.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('8Brei6Pr1RJ4AeVWSYGcL', 'XVKywqA9fFHLhfQumEKvg', 1679304620000.0, 'https://github.com/rocicorp/mono/blob/f68a76fa5e4f84dceda0ebcf25ee7d4f59f4bb77/packages/reflect-server/src/server/connect.ts#L94-L108

https://github.com/rocicorp/mono/blob/a1bc9996470aa52517cff9361f72078f9d08a896/packages/reflect-server/src/server/connect.ts#L110-L120

We also have tests for both of these.

The client logs `InvalidConnectionRequest`s as `error`. https://github.com/rocicorp/mono/blob/a1bc9996470aa52517cff9361f72078f9d08a896/packages/reflect/src/client/reflect.test.ts#L1172 ', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('UHvHbhd9YXNwa487ZgH0A', 'hJLcOo1hA-3z3yKOg0fQR', 1663287321000.0, 'If we update the auth interfaces which I think we should (there is no sense in doing this halfway) then we should get customers to move to the new interface.', 'phritz');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('oUsloOwtO0Z0dVVtB5X2t', 'hJLcOo1hA-3z3yKOg0fQR', 1671588821000.0, 'Now as convinced we should do this now. Deprioritizing.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('nfSGXxbEuB6qjIFFEo92_', 'hGFLxlfWmXMgHCyrkUWbq', 1677704703000.0, 'We''re in a monorepo now. thanks @cesara !', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('0LK9UOYpsyi3N2i9iFbok', '0-Jc6iqbtwPqVIFXmJ3OW', 1663242236000.0, 'I have temporarily pinned replidraw-do to the correct version but this is wonky.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('fhKMTj6Z1L-7F_MEfMfUO', '0-Jc6iqbtwPqVIFXmJ3OW', 1672740424000.0, 'I think the task here is just to update to latest wrangler.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('Ae4R1JQiP6426f3CjZ6lq', '0-Jc6iqbtwPqVIFXmJ3OW', 1675936495000.0, 'Thanks @arv ', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('Mwl3g5tdupIIcb1Z9tlS2', 'r9Z7YnNl4Yastlbp90SQG', 1663706572000.0, 'It wasn''t working for me -- I had mistakenly had my websocket URI pointing at a production CF worker.

But we''ve figured out what this is, there are a bug in wrangler2 which, once fixed, I think will resolve our issues:

https://github.com/cloudflare/wrangler2/issues/1767', 'ingar');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('dYAIEJtYQCQnzGIstbW_C', 'r9Z7YnNl4Yastlbp90SQG', 1663724214000.0, 'I think there are lots of advantages to having dev be as close to prod as possible. However while I see these wrangler/miniflare bugs I don''t think we should necessarily switch away from local because of them. So removing from beta list for now and we will keep an eye on it. ', 'phritz');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('pSYLXbLkqhETbO8tqrzGm', 'r9Z7YnNl4Yastlbp90SQG', 1672740466000.0, 'Well, we did switch away, so closing this.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('UiG9gJxhxGk717vkQufIO', 'C2HQKc4ef_wrkOW4gnoqm', 1663901014000.0, 'When we close rocicorp/mono#243 we hope the new solution will not have this problem, but will wait to close this issue until we can verify that it doesn''t. cc @ingar ', 'phritz');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('Xh24-ft_MDX8DeG9oY8Dq', 'C2HQKc4ef_wrkOW4gnoqm', 1677605173000.0, 'once this bug is fixed we should remove the empty init from the scaffold,create-reflect, and reflect-todo apps', 'cesara');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('g5MkKQE74j-TD8EvYAKv-', 'C2HQKc4ef_wrkOW4gnoqm', 1677698713000.0, 'Adding comment from @grgbkr from duplicate bug #176:

> My current thinking is to keep fastforward in turn processing (its efficient to do it as a batch when a DO restarts, and has a bunch of clients reconnecting). I think new connections should cause a turn to run (just as mutations and disconnects do). This structure can allow onConnect (https://github.com/rocicorp/mono/issues/175 ) to be implemented in basically the same way onDisconnect is today', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('2NNT9AMJm74Rr2ifa1FMd', 'C2HQKc4ef_wrkOW4gnoqm', 1679687455000.0, 'Fixed in https://github.com/rocicorp/mono/commit/6e5c5eb93c4bb78a84de7136dba45ed9f77c42ce', 'grgbkr');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('coLhKIvJy3khOoS0C44KV', 'ifZtIP406pHKav23mtYmL', 1663125916000.0, 'You should be able to write code to process a push (and pull) request using the exported types from Replicache.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('tJZYtsDgGK069RPzYowo0', 'ifZtIP406pHKav23mtYmL', 1663231371000.0, 'Can you expand on the use case? I''m not understanding why we should expose this?', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('hIihTgA1F30BMgIvYbyVD', 'ifZtIP406pHKav23mtYmL', 1663235545000.0, 'We expose it now. The reason we expose it is because it is common to write servers in typescript and it''s useful to not have to rewrite/copy the types from the docs.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('52mTL49ULLe__bbt8hpNI', '3aPdgUKvOirJqqDGWZCpk', 1663231665000.0, 'This is intentional: https://github.com/rocicorp/replicache/releases/tag/v10.0.0#:~:text=%F0%9F%8E%81-,Features,-Introduce%20the%20concept

We intentionally keep `process.env.NODE_ENV` in there to allow people to hit some of our asserts in their **debug** builds.

I have this "task"/thought that we should publish a **release** and a **debug** version instead.', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('sdwyybe9bUqwjzkeDHfQw', '3aPdgUKvOirJqqDGWZCpk', 1663233994000.0, 'ðŸ‘ that''d be really helpful for the "try this out quickly" scenario imo. 

Another pattern I see pretty commonly is to have a ./dist folder with production (and maybe debug) builds in them. 

So the file that a build tool would ingest would be `./replicache.js` or ./src/replicache.js (which could be listed as the entry point in package.json). 

Then for pre-built files:
- `./dist/replicache.js` (unminified) 
- `./dist/replicache.min.js` (minified prod build)
- `./dist/replicache.debug.js` (unminified debug build)

^ Could also include `.mjs` esm versions. ', 'jamischarles');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('BVRUv-ZS5K1BRdi_0djDQ', '3aPdgUKvOirJqqDGWZCpk', 1663234416000.0, 'We don''t want to release the unminified build so that would be:

```
./out/replicache.debug.js
./out/replicache.release.js
./out/replicache.debug.mjs
./out/replicache.release.mjs
```

With package.json something like:

```json
"exports": {
    ".": {
      "module": "./out/replicache.release.mjs",
      "require": "./out/replicache.release.js",
      "default": "./out/replicache.release.mjs"
    },
    "debug": {
      "module": "./out/replicache.debug.mjs",
      "require": "./out/replicache.debug.js",
      "default": "./out/replicache.debug.mjs"
    },
    "release": {
      "module": "./out/replicache.release.mjs",
      "require": "./out/replicache.release.js",
      "default": "./out/replicache.release.mjs"
    },
},
```

This also means that we would strip the `process.env.NODE_ENV` completely from all build artifacts.', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('rzzDb8Rvkmm0JCXmCcS1S', '3aPdgUKvOirJqqDGWZCpk', 1663234744000.0, 'ðŸ”¥ðŸ”¥ðŸ‘ðŸ‘ðŸ’¥ðŸ’¥ðŸ’¯', 'jamischarles');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('cfIxWZ9vdZvWvVp1yGAOe', 'SOlle2-ebTy4xp3_1OMwn', 1663015345000.0, 'WIP PR here: https://github.com/rocicorp/reflect-server/pull/131

The end result is to implement the not-implemented here: https://github.com/rocicorp/reflect-server/blob/main/src/storage/replicache-transaction.ts#L85

See https://github.com/rocicorp/replicache-express/blob/main/src/backend/replicache-transaction.ts#L73 for a working example in a different repo.

Some things to be careful implementing this:
1. The semantics of scan are that keys come out in a specific order (the order of the JavaScript sort() method). I am not sure if the built-in order of durable object''s list() method are the same. (That''s why the PR sorts, defensively)
2. The scan() method needs to scan over the union of pending changes and the stored data. We have a helper function in Replicache makeScanResult() that helps with this.
3. The scan() method is lazy (it''s an async iterator), so it would be nice if we didn''t read the entire keyspace into memory to implement this, but if the answer to (1) is unfavorable there might be no choice.
4. There are parameters to scan (startAt, endAt, limit) etc which could at least reduce the amount of data we read into memory from DO, but this fledgling PR doesn''t use them.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('jx0dwjIoAyN8Api86kXwC', 'SOlle2-ebTy4xp3_1OMwn', 1663015372000.0, 'There is also a valid answer here where we do something inefficient to get scan() working then circle back and do it more efficiently later.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('s8XReJ74bNteILTE71KAl', 'iiz4kdlkl-0uxT-tWoOPF', 1662365047000.0, 'Now that I spell this out it seems simpler to just have the developer create normal DD31 Replicache instance on the worker and have them provide identical definitions.', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('FtcGwIFDdLogZesJDKsTS', 'CuPT9CKpVUFRsq07M51wn', 1677782679000.0, 'It seems like the right thing is to put *another* layer of cache between? Wheee. Send another nested cache into `ReplicacheTransaction` and flush it in the success case.

@cesara I think you can take this one.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('WIdx6ZIiJRjKH3v6EwvtK', 'CuPT9CKpVUFRsq07M51wn', 1677782721000.0, 'This nested `EntryCache` abstraction is the gift that keeps on giving.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('BpJu8jYgOV0UU6k79Uwwf', 'pvgwRLWBQcmtbXQydIcUl', 1661765771000.0, 'Wont fix.

`allowEmpty` was added later so it needs to be kept optional so we might as well keep things optional everywhere.', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('_kmOkOEbvMiuQ1PA_WTRM', 'vaga-cTK9yTB98XShYA5x', 1661535663000.0, 'Low prio, but would be nice.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('JK_P1maZvE_fsgctO96_k', 'TpxU6KF2rQJcgQpTd2SDT', 1661535624000.0, 'This already exists: undocumented `enableLicensing` field on `ReplicacheOptions` (https://github.com/rocicorp/replicache-internal/blame/main/src/replicache-options.ts#L227)', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('e9gb9dbmQBPKxLSIAOStA', 'GojkNTEGE0yuF5O5rx7WU', 1659479616000.0, 'Then when people search "svelte offline" we can buy that keyword and send em right to the corresponding sample! same with "solidjs offline", "react native offline", etc.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('Qr07SVKtLlVxDfl0i3gk6', 'GojkNTEGE0yuF5O5rx7WU', 1659495704000.0, '> No express, no nothing.

I guess on second thought I don''t feel so strongly about this. As this server is meant to be more a reusable thingy and less a learning tool it''s OK if it has deps that make it a little more "professional". But we shouldn''t use anything too obscure or fancy as people will want to look at this and understand it.

It definitely shouldn''t use Next, just because Next is so focused on the client-side and client/server integration and what we''re going for here is a plain API server.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('90VMy6cakbnGVchVuNCjL', 'GojkNTEGE0yuF5O5rx7WU', 1666298180000.0, 'Is this done? @cesara @aboodman ', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('cB_xCSnA5gP8dAq0qhwRa', 'GojkNTEGE0yuF5O5rx7WU', 1666315958000.0, '@arv yes, factored out replicache-express and replicache-nextjs', 'cesara');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('rR0eMNkyc6dsWge4ifEzr', 'w11aPgBD5OzWd_wfpEYax', 1658472387000.0, 'The internal values change somehow manages to break Repliear too. Verified both replicache-todo and repliear work again with the disable PR.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('1s1Kz7UGyc0VkFsQ-z91q', 'w11aPgBD5OzWd_wfpEYax', 1658472404000.0, 'Don''t know if same underlying cause though, please confirm when you fix this @arv .', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('Es31fmYKujthXVCNoJ8Y1', 'w11aPgBD5OzWd_wfpEYax', 1667404336000.0, 'When I first saw this I thought it made sense and that there was a case I missed related to `makeScanResult` but I don''t see it any more.

I will try to repro this with replicache-todo.', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('5VNxrezH98LZLMdgtK9LZ', 'hah-xpgyGUYzoFWzjU9f1', 1657167825000.0, 'The Internal api for the database of databases is here https://github.com/rocicorp/replicache-internal/blob/main/src/persist/idb-databases-store.ts

I think this is a good addition and straightforward to add.  ', 'grgbkr');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('qTckc9I0MiVsyJS2xOzA1', 'hah-xpgyGUYzoFWzjU9f1', 1657306511000.0, 'The reflect client also uses the `replicache-dbs-v0` database.  Just wanted to confirm that we want to delete reflect''s IDBs also?  I guess it''s technically replicache also.

<img width="588" alt="image" src="https://user-images.githubusercontent.com/85998/178053548-1f1fe62b-79a7-4316-92f1-af5a784b4f47.png">
', 'ingar');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('TzvY6Lac6dNy-xkAkWQcr', 'hah-xpgyGUYzoFWzjU9f1', 1657311763000.0, 'Hm, it''s kind of an academic question since I don''t expect them to ever be
used together in reality, but sure, let''s delete everything.

On Fri, Jul 8, 2022 at 8:55 AM Ingar Shu ***@***.***> wrote:

> The reflect client also uses the replicache-dbs-v0 database. Just wanted
> to confirm that we want to delete reflect''s IDBs also? I guess it''s
> technically replicache also.
>
> [image: image]
> <https://user-images.githubusercontent.com/85998/178053548-1f1fe62b-79a7-4316-92f1-af5a784b4f47.png>
>
> â€”
> Reply to this email directly, view it on GitHub
> <https://github.com/rocicorp/mono/issues/89>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AAATUBCAJQMWJHQOZ2OGQD3VTB2RVANCNFSM5234KE6Q>
> .
> You are receiving this because you authored the thread.Message ID:
> ***@***.***>
>
', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('0edTCkTLHiHjOFBwO6slD', 'Zyp6cKng_eCdyX2pZi4IM', 1659295205000.0, 'Decided this doesn''t need to be a task in itself or something we need to prioritize.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('JT3BKH8Rfk1UAE1uDB6IM', 'a2QYIcY8XOP56qUNkoLcr', 1656407396000.0, 'Please take care of this', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('SEStlekALnaP6ZBaF7Cj9', 'NhOEDbtZQav19y4Cestl2', 1656407365000.0, 'Please followup on this', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('kVMvbGO4ccazlnk5qhJMX', 'p2DS_Ww4_P7n5OlZlTPY3', 1655692608000.0, 'I guess another way this could work is by (ab)using `schemaVersion` mechanism:

* When we setup the postgres schema we generate a random `instanceID` and store it in the database persistently. This identifies the "instance" of this particular postgres schema.
* The schema version system in the postgres setup works as today and is separate from this.
* As part of `[id].tsx` we read the instanceID from postgres and embed it in the page.
* When we construct Replicache we set the Replicache `schemaVersion` to `${instanceID}:${replicacheSchemaVersion}` (where $replicacheSchemaVersion is currently zero).

This is a bit confusing because there are two "schemas" floating around:

- The postgres schema, which is not (necessarily) visible to the client
- The replicache schema version, the schema of the data stored in Replicache

The two interact but are not the same thing.

', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('j8sXElNT_kOJyGWRYTEfX', 'p2DS_Ww4_P7n5OlZlTPY3', 1655712736000.0, '`ServerNotFound` -> `ServerStateNotFound`', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('Jc7Yu6iVaK6GXeutX1z4x', 'p2DS_Ww4_P7n5OlZlTPY3', 1655755208000.0, 'I thought about that, seemed weird for some reason I can''t quite explain. Will try it.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('STcKZPfEpWpqtt2jT3G8Y', 'p2DS_Ww4_P7n5OlZlTPY3', 1655755509000.0, '> I guess another way this could work is by (ab)using schemaVersion mechanism

This would work if the entire database (all spaces) is deleted. But do we want to handle the case where just one space is deleted? It seems like this mechanism should indeed handle it.

In that case you could put the "instanceID" in the space row instead of globally in the database. But now the question is: why not just the spaceID which we already have, and the problem there is that reloading the page wouldn''t get you a new spaceID (nor would you want it to).

Also I kind of don''t like this mechanism because it creates work for the developer. Now every app that cares about being able to delete server-side spaces has to have this mechanism to communicate the schema/instance to the client. It would be nicer to wrap this up into the Replicache protocol as https://github.com/rocicorp/mono/issues/91 proposes, but that is so much more work.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('sfLTWRCrODJYFBIUJv6WW', 'p2DS_Ww4_P7n5OlZlTPY3', 1655860681000.0, 'Actually I do not think `ServerStateNotFound` as described https://github.com/rocicorp/mono/issues/91 works. Here''s why:

- Client A loads replicache-todo space S1
- Client A pushes first mutation which implicitly creates S1 (in replicache-todo)
- Client A pulls cookie 1 for space S1
- Now server state is deleted
- Client B loads replicache-todo space S1 (perhaps the URL was shared)
- Client B pushes first mutation which implicitly re-creates S1
- Now Client A pulls from cookie 1 for space S1
- Server returns nop patch, client A has wrong state.

Basically the ServerStateNotFound error tells a client that a particular server is not known, but because we share the server IDs among clients there is a chance the server can get recreated before a particular client pulls again and finds out it is deleted.

It seems like we have to ensure with these sample apps that use spaces that spaceIDs are not reused.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('TwFCLjMMvl7rjuZ0TB0zk', 'p2DS_Ww4_P7n5OlZlTPY3', 1655861500000.0, 'OMG I think the solution is waaaaay easier than any of this. Problem is fundamentally that (a) by using an in-memory database we are basically deleting all the spaces, and (b) we implicitly create spaces on push if they don''t exist.

(a) and (b) together mean that old clients that are referring to spaces created before a delete will recreate the space on the server, but find themselves in an incompatible state.

Easiest solution: stop doing (b). It doesn''t reflect what real apps would do anyway -- documents aren''t created implicitly by visiting a URL, they are created by tapping a "create document" button. We can create the space programmatically in https://github.com/rocicorp/replicache-todo/blob/main/pages/index.tsx before redirecting to it. Then we take out the code that implicitly creates in push. Then change push and pull to 500 if referring to a space that doesn''t exist.

The effect will be:

- if you delete a replicache-todo database while a client is running, push and pull will both start failing because they refer to a space that no longer exists
- even if another client visits the `/d/<spaceid>` URL because it was shared, the space won''t be recreated. The only way a space gets created is by visiting `/` and that creates a random new space, so spaces will never be reused.

No code changes in replicache at all.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('QdxLh74sFwH8W30OdF_8q', 'p2DS_Ww4_P7n5OlZlTPY3', 1655861871000.0, 'Or from Replicache''s pov, the resolution here is as it was before I opened this bug:

- It''s not valid for a server-side database to go backward in time.
- Deleting a database and reusing its "namespace" is the same as going backward. Don''t do that.
- If you must support deleting database, then make sure that it''s not possible for their namespaces to get reused.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('2pW4WDVVbMSJIawUjVXki', 'p2DS_Ww4_P7n5OlZlTPY3', 1655890996000.0, 'Close as "working as intended" then?', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('E8U6_-TMU0ek8gSL7u0y2', 'p2DS_Ww4_P7n5OlZlTPY3', 1689319347000.0, 'Now that we''re not using spaces so much and recommending other diff strategies for users, this is coming up again. I think we should fix it.

See also: https://github.com/rocicorp/mono/issues/92 and https://github.com/rocicorp/mono/issues/232', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('QfHOOTS0n-pzPiSC1Rq_H', 'c0121ZvhhZvz7do4IEUWl', 1655388813000.0, 'One unsatisfying solution is to remove the read lock and only have a write lock. In that case the scan will only show what the tree looked like at startup. But at least it will not dead-lock.', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('-LF1-IoPlN2l-WmlFFFwr', 'c0121ZvhhZvz7do4IEUWl', 1655391622000.0, 'One solution is to keep track of the `rootHash` as we `scan`. If the `rootHash` changes we go back to the root and continue the iteration from the new root.

WIP PR coming...', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('1z6n6wrAwHwFYgHkCG_Mu', '_7mrJDQchBc4gXQsuwJji', 1654801111000.0, 'Another option is planetscale: https://planetscale.com/. This is intriguing because they say, publicly, "planetscale doesn''t believe in localhost". They have a forking/deploy model for upgrading the db built right into the product. So you''d just start online in dev mode from the beginning.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('EEVxjoKlLwH5fzkl0GPxy', '_7mrJDQchBc4gXQsuwJji', 1656007415000.0, 'This is live!', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('bVfJKizh2RI0UqgFwFEzF', 'cxWMStEbXVwNwPSYiyRxO', 1654063918000.0, 'Replidraw is kinda a pita to run locally right now tho. Directions aren''t correct. Will attempt to fix.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('arl94XcDCaAuNhxqSmlGR', 'cxWMStEbXVwNwPSYiyRxO', 1654074115000.0, 'I believe the setup instructions for Replidraw are fixed now: https://github.com/rocicorp/replidraw/blob/main/README.md', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('EwAFq8ejmIZ1qkt_JyClY', 'cxWMStEbXVwNwPSYiyRxO', 1655110902000.0, 'Fixed', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('YHF2G-UJo_SCzAdRI0o-v', 'akrNUuHFpt68Mwmqz1_-9', 1653898968000.0, 'A few comments in no specific order:

- Don''t you think people use `subscribe` without `scan`? I feel like it is useful to watch a single or a set of keys
- The callback to watch seems to imply a single diff operation.
   - Would it make more sense to have it as an iterator/stream then?
   - This makes it hard to know when to start/end batch updates. Maybe it is better to use an array of diff ops?
- `map` makes the diff computation harder. We would now have to diff the values produced by `map` and keep old values around at each `map` "layer".', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('VnKlQN2uG6uZYux8aO5dW', 'akrNUuHFpt68Mwmqz1_-9', 1654360969000.0, '> Don''t you think people use subscribe without scan? 

I''m not aware of anyone using for anything except getting a single key or getting a contiguous set. Definitely those two use cases are overwhelmingly the most common from my observation.

We don''t *need* to deprecate subscribe but we might want to if watch() can basically cover it as having both adds API complexity and bundle size (presumably?). Also if subscribe() can be implemented in terms of watch that''s a good reason to move it out of the core.

> The callback to watch seems to imply a single diff operation.

Yeah good point. The callback should receive an array of diffOps so you can apply them all atomically to receivers. I think it wants to be a callback rather than a stream API because almost always people are going to hook this up to something like `useEffect()`. I feel like the async iterator would just create boilerplate.

> map makes the diff computation harder. 

Good point. I can''t think of a clear use case for `map()` so let''s leave it out until we have some.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('K510q4M-cwCOrbtu_laSN', 'akrNUuHFpt68Mwmqz1_-9', 1654361877000.0, 'Some of my own observations:

* Having `watch()` as a method of `ScanResult()` doesn''t make sense after all because `ScanResult` is something that is scoped to a single transaction, whereas watch by definition spans transactions. So this seems to mean that watch should be a method of Replicache not of `ReadTransaction`, more similar to how `subscribe()` is today.
* It''s common to want to monitor a single key for changes but this is less elegant in this API (have to `limit: 1`, and get first item from result).

Putting these two together I''m currently thinking something like:

```ts
rep.watchMany({prefix, startAt, limit})
    .filter(entry => ...)
    .sort((e1, e2) => ...)

rep.watch(id)
```

Open questions:

* Where does the callback go? With the API coming off `scan()` it was elegant to put it as the last method - `tx.scan().filter().sort().watch()`. Now that doesn''t make sense.
  * Should the callback go in the `rep.watch()` method as a formal param or field of the options param? That''s awkward to type with the chain after.
  * Or should we skip the chain and put `filter` and `sort` as optional fields on `WatchOptions`. That''s less useful because you can''t do multiple filters but maybe also less footgunny because you can''t do silly things like have multiple sorts.
* It seems like ideally:
  * callback is last thing you type
  * should be possible to have zero or more filters
  * should be possible to have zero or one sorts
  * should be enforced that sort happens after filter
* The base use case is to receive through the callback a stream of arrays of diff ops. This would be used by e.g., solid, react+mobx (, and maybe svelte? need to investigate). But as a convenience for React and VanillaJS I think it would also be good to have `entries()`, `keys()`, `values()` that return an array of keys and/or values. The array should change identity each time there''s a change, but the entries inside should only change identity when they change. This makes it easy to use with React and `memo()`. There''s an argument here that maybe that should be in replicache-react, but it seems like something more generally useful.
* I feel like it would be useful to have `filter()` and `sort()` on `ReadTransaction.scan()` too. Could be a separate task but we should keep in mind we might go that direction.

@arv any ideas on these questions/points API-wise? I''ll keep thinking about it too.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('l4j5dZBGThZjXhN3dC2ff', 'akrNUuHFpt68Mwmqz1_-9', 1654362556000.0, 'Maybe it''s `rep.watch(details).with(callback)`.

Or maybe `rep.watch(details, [callback])` and if you pass the callback the return type is `void` but if you don''t pass it the return type is the chainable interface.

We can achieve the restriction on having multiple sorts and order of filter vs sort by factoring the return interfaces:

```ts
class Watchable {
  // `with` is a reserved word in js, but vscode doesn''t seem to complain about this usage.
  // https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Lexical_grammar#reserved_word_usage
  // not sure if we should use it.
  with((patch: DiffOp[]) => void): void;
  async keys(): Promise<string[]>;
  async values(): Promise<ReadonlyJSONValue[]>;
  async entries(): Promise<Entry[]>;
}

class Sortable extends Watchable {
  sort((e1: Entry, e2: Entry) => number): Watchable;
}

class Filterable extends Sortable {
  filter((e: Entry) => boolean): Filterable;
}
```', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('a88FQX6VViRfMOTSDAnC1', 'akrNUuHFpt68Mwmqz1_-9', 1654490460000.0, 'OK thinking about this over the weekend here''s a concrete proposal. I realized that the diff ops have to be in terms of positions, not keys, since there''s sorting involved!

Also I haven''t thought this through at an impl level at all, there could very well be issues. This needs a design doc going into code-level design for sure.

# Overview

```ts
type WatchOptions = ScanOptions;

type Entry = {
  key: string;
  value: ReadonlyJSONValue;
};

type WatchChange = WatchInsert | WatchUpdate | WatchDelete;

type WatchInsert = {
  type: "insert";
  position: number;
  key: string;
  value: ReadonlyJSONValue;
};

type WatchUpdate = {
  type: "update";
  position: number;
  value: ReadonlyJSONValue;
};

type WatchDelete = {
  type: "delete";
  position: number;
};

// Call to cancel an existing watch
type CancelWatch = () => void;

class Replicache {
  ...
  watch(options: WatchOptions): FilterableWatchResult;

  // Just a convenience, really has nothing to do with watch(), can be implemented much more easily.
  watchOne(id: string, (entry: Entry|undefined) => void): CancelWatch;
  ...
}

interface WatchResult {
  // Fires every time one or more watched keys changes. Changes must be processed in order for positions
  // to make sense. All changes for a particular mutation are passed atomically to `changes()`. However,
  // multiple mutations may be reflected in same call to `changes()` (i.e., if one frame had many mutations).
  changes((changes: WatchChange[]) => void): CancelWatch;

  // Fires every time changes would, but passes an array of all current entries matching the watch.
  // The identity of the array does *not* change across calls, nor do the identities of unchanged values.
  // However the identity of changed values does change. This is intended to be used with e.g., React.memo().
  entries((entries: Entry[]) => void): CancelWatch;

  // Same as entries, but only returns the keys.
  keys((keys: string[]) => void): CancelWatch;

  // Same as entries, but only returns the values.
  values((values: ReadonlyJSONValue[]) => void): CancelWatch;
};

interface SortableWatchResult extends WatchResult {
  sort((e1: Entry, e2: Entry) => number): WatchResult;
};

interface FilterableWatchResult extends SortableWatchResult {
  filter((e: Entry) => boolean): FilterableWatchResult;
};
```

# First Result

When user first calls `watch()` their callback gets fired with a diff that is all `WatchInsert` representing the current state. If they call `entries()`, `keys()`, `values()`, their callback fires with an array matching current state.

If there are multiple open watches that need there first result (for example during page load) it is possible to collapse their watched key ranges and do only one iteration over the Replicache keyspace. Unclear whether this is a win, needs a test.

# Incremental Results

As the keyspace changes, Replicache checks changes against open watches. If they match, they are passed through the filter / sort chain incrementally, without re-scanning Replicache.
', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('F3Tg5_lvNtJQA8F6BDK-4', 'akrNUuHFpt68Mwmqz1_-9', 1654848969000.0, 'A few things:

- I would like to include the key in the entry as well.
- I assume the position is all about updating an in memory array? I don''t know if it is useful? If the filter changes the output array then the positions change. The only time I think the position can be useful is if there is no filter and no sort.

One option when designing the API is to realize that multiple filters can always be folded into one filter. And we only allow a single sort. Given that, maybe "chaining" isn''t the way to go? Instead we could try an option bag:

```ts
watch(options: {
  prefix?: string,
  filter?: (e: Entry) => boolean,
  sort?: (a: Entry, b: Entry) => number,
  indexName?: string, 
  start?: ...
}): WatchResult;
```


', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('Y-BY1vpo5tfjDy_o_iZfl', 'akrNUuHFpt68Mwmqz1_-9', 1655753864000.0, 'Sorry I forgot to reply to this.

> I would like to include the key in the entry as well.

I''m confused. The `Entry` type proposed here does include the key.

> I assume the position is all about updating an in memory array? I don''t know if it is useful? If the filter changes the output array then the positions change. The only time I think the position can be useful is if there is no filter and no sort.

Right, the position represented in the callback would be adjusted. What''s happening is that the output of a `watch()` is a list of key/value pairs sorted by some criteria (the `sort()` criteria). So the incremental updates have to be index-based, not key-based. Alternately you can think of it as outputting a set of splices. But since each change event will have arbitrary number of splices (because each transaction can touch arbitrary items) there doesn''t seem to be any advantage to introducing a real splice concept and instead I just went with simpler delete(pos), insertAt(pos), update(pos).

You could actually get away with just delete and insertAt obvs. Maybe we should do that.

Put another way, the output of watch is a patch, but a patch to a list, not a patch to a dictionary.

> One option when designing the API is to realize that multiple filters can always be folded into one filter.

I thought about this, it just feels less ergonomic? If you feel strongly about it I''m OK limiting to one filter to start.

> And we only allow a single sort. Given that, maybe "chaining" isn''t the way to go? Instead we could try an option bag:

Yeah, this also felt non-ergonomic to me. I guess I don''t feel super strongly here but do have an aesthetic preference for the chained API. I''m OK trying the non-chained API on for size, I don''t think there''s any functional difference.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('8oUflKKOSP0-ZnJQQujYq', 'akrNUuHFpt68Mwmqz1_-9', 1655754095000.0, 'Certainly the non-chained API is easier to implement and probably lower code weight?', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('xGQSNjVGw-HTF4kd0X5HP', 'akrNUuHFpt68Mwmqz1_-9', 1655800179000.0, 'My initial reaction to this was that it was great. At this point I feel like the semantics (and implementation) is a bit unclear and given that I feel less excited about it.

Can we try to nail down the semantics a bit more and maybe things fall into place after that?', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('8PjyicCbveIxQcCreud0e', 'akrNUuHFpt68Mwmqz1_-9', 1656726042000.0, 'New new new proposal, taking into account @arv''s online and offline feedback:

https://www.notion.so/replicache/RFP-watch-cf3110a59db446a59848ea40f48b799b
', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('S3u3q59XVWv2HgGoqk6s6', 'akrNUuHFpt68Mwmqz1_-9', 1658175675000.0, 'm0c from lazerfocus has an interesting use case involving a join ([discord message](
https://discord.com/channels/830183651022471199/830183651022471202/998568464241397910)).  
<img width="962" alt="image" src="https://user-images.githubusercontent.com/19158916/179610038-417b4b8f-a661-4df2-90d5-1316df277194.png">

I''m not sure how you would do a join using watch (it is possible with subscribe).  


', 'grgbkr');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('-ngIfPZueBXCI315CK67S', '5gsbQgpFjeyn3M8Ylogf-', 1663282072000.0, 'I think we should nt do this since we plan to re-merge, and will get it for free with that. See: rocicorp/mono#290 ', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('5G2hBC-PTwKZGp8G-OrlF', 'gPhqCx-aYMqYuEYjVCm3J', 1653672027000.0, '@aboodman do you have thoughts on what the API should be?  ', 'grgbkr');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('r9arwQhG9bJ6VkPK2Ky8I', 'gPhqCx-aYMqYuEYjVCm3J', 1653673282000.0, 'The Replicache API seems reasonable?', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('rtHLQ30o3V5QHFEFfJB-y', 'gPhqCx-aYMqYuEYjVCm3J', 1653677953000.0, 'To be more decisive: the current Replicache API is good with me.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('kBdMLZq4Q6yMGSrRYHQZn', '2wGcLhvqKsJE4NhB_rKg2', 1652995659000.0, '[image: image.png]

Some random thoughts

0. I think we want the DO (as in the single in-memory running instance).
The DO is critical to our whole design here -- it''s the key bit. I don''t
think SQLite changes that.
1. This embedded compute *sounds* like something we want, but I bet in
practice it is not. Because we are using the persistent storage more for
backup, not for complex calculations that need to run near to the db.
2. If we succeed in moving persistence more off the critical path then
SQLite becomes more viable!

On Thu, May 19, 2022 at 10:19 AM Greg Baker ***@***.***>
wrote:

> Evaluate if this will meet our goals and provide customer''s with better
> visibility / tooling for their data store (i.e. it is currently very hard
> to see what is in your DO storage).
>
> https://blog.cloudflare.com/introducing-d1/
>
> â€”
> Reply to this email directly, view it on GitHub
> <https://github.com/rocicorp/mono/issues/250>, or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AAATUBAGIV7A4RYEE3VDSV3VK2O6HANCNFSM5WNLDFIA>
> .
> You are receiving this because you are subscribed to this thread.Message
> ID: ***@***.***>
>
', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('b-KzxRPxTGrkwfSJrPJNm', '2wGcLhvqKsJE4NhB_rKg2', 1672740639000.0, 'I do not think that D1 meets our needs for a few reasons:

1. It doesn''t exist yet
2. Because we are still persisting quite frequently, we need storage to be nearby. The design of D1 appears to be that storage might be distant from the DO (it''s shared)
3. The way I think we''d want to use D1 ideally (from a dx perspective) is to have a single DB that all DO''s write to, so you can do selects across it, etc. multitenancy basically. But that would serialize writes across all rooms which we don''t want.

Closing this for now.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('Cbv9DvP2UCi10ohN_VkEZ', 'JQtzzPWBxB1AeLUIV5vJs', 1653323249000.0, 'We can certainly migrate what replidraw-do currently runs on vercel to cloudflare pages (https://developers.cloudflare.com/pages/migrations/migrating-from-vercel/).

The question is how much of the worker and dos (room and auth) of replidraw-do can/should be migrated to pages.

1. Currently the DOs cannot be deployed using Pages, there just isn''t support.
2. The worker can be deployed using Pages, worker deployment support is called "Functions", and is currently in Beta https://developers.cloudflare.com/pages/platform/functions/

Since we have to publish the DOs, and the worker gets published as part of the same command, I don''t see any advantage to moving the worker to Pages.

cc @aboodman ', 'grgbkr');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('VU3dv52LFublvZv2WboAl', 'JQtzzPWBxB1AeLUIV5vJs', 1653330752000.0, 'This conflicts with something I was told by a CF employee in their discord. Let me find the reference.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('cGfD6uX8ByUUZj5Xl9dTb', 'JQtzzPWBxB1AeLUIV5vJs', 1653331335000.0, 'Nevermind, it seems consistent: https://discord.com/channels/595317990191398933/779390076219686943/955606582471819294', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('n9uSgBr0GIwDwZQOws4Xf', 'JQtzzPWBxB1AeLUIV5vJs', 1653336098000.0, 'Well it would be nice to figure out how to do preview deploys of Replidraw somehow, including the DO, since this will be a common request from our users. I don''t think it''s critical for the next milestone, however.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('Xszjb9UudJCJBK-wquGHr', 'JQtzzPWBxB1AeLUIV5vJs', 1653336190000.0, 'Sorry for chain-of-comments here, but does it makes sense to move the UI to pages just so we can deploy everything using the same tools and the user only has to deal with one service?', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('sFqh_IOwrSUttJfeUZMKe', 'JQtzzPWBxB1AeLUIV5vJs', 1663282010000.0, 'Whoops duplicate of rocicorp/mono#288 ', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('8oegO3xIrYWGCSF5nnbGf', 'qKdxI5MsM8FzdWz7jCd1x', 1663492333000.0, 'External bug report: https://github.com/rocicorp/replicache/issues/1026', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('cLO9s2MKnpfvFFzUymO0h', '850WT8f7wSQtZ2Wmjv17n', 1690343278000.0, 'Well it shows up in the docs now, but it''s not described.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('XErmCf3ql-22ABmpBjidq', 'K7GhpXmlZM-bMHbJf02Mk', 1677704820000.0, 'Super old.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('qyVuoVc9HoAIfHr04xQNG', '1pMwgmRZq30yxR8yWenLd', 1667310994000.0, 'I think we should make this blocking v12 because with DD31 the type of the request json changed and without "fixing" this TS will not capture errors there.', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('rM2yZs1Ju5ZFlwATKlMgs', '1pMwgmRZq30yxR8yWenLd', 1667311053000.0, 'label:DD31 because DD31 changes the type of the json request body', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('KMoFIv_zXN6XmtS5_DNbj', '1pMwgmRZq30yxR8yWenLd', 1670612679000.0, 'See: https://github.com/rocicorp/replicache-internal/pull/331#discussion_r1009743025 for more details on how this relates to DD31 and mutation recovery in particular.', 'grgbkr');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('nh-ZyMO2Q-9qJY7o5EyvO', '6eZ-NkNCr5Kg2hWBD3W5K', 1651675885000.0, 'I had this idea that we should be able to do a comparison of two JS strings using the same semantics as if we first encoded those strings as UTF8 and then did a byte-wise comparison.

Basically we could achieve this semantics:

```js
function compareUTF8(a, b) {
  const encoder = new TextEncoder();
  const aBytes = encoder.encode(a);
  const bBytes = encoder.encode(b);
  return compareArrays(a, b);
}
```

without having to allocate a buffer for the whole string. We could do a character by character comparison and when we hit a character that is not the same in UTF16 and UTF8 we then encode that character and compare that etc.', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('V78Z0yu3ZksuOgN7DP1pS', '6eZ-NkNCr5Kg2hWBD3W5K', 1652942889000.0, '> I had this idea that we should be able to do a comparison of two JS strings using the same semantics as if we first encoded those strings as UTF8 and then did a byte-wise comparison.

This is a neat solution, "neat" as in "clean" or "tidy". To make sure I understand the implications (and cc @aboodman) I think having key sort order be according to code point means that:
- to compare keys on the server, if it has UTF-16 strings it must to use our comparison function and not whatever is native; and, if it does not have UTF-16 strings, it needs to do a bytewise comparison on the UTF8 encoding of the string (or some other equivalent way to sort by code point).
- locale-aware key sort order is not supported by Replicache. The code point sort order is what you get, even though it''s not what is natural in non-english locales. So if you wanted to for example implement an in-order scannable dictionary in german you need to keep a secondary data structure with the appropriate order, or have a strategy to map from the actual key to a key with the right sort order.
- replicache doesn''t do any kind of canonicalization; if this matters for the customer they need to do it before handing us a string.

Yes?

Second question: can you help me understand the advantage of the proposal above over the seemingly equivalent strategy of defining keys as UTF8 strings? Is it transcoding cost? If so it seems like most keys are generated by the server and will be received by the client as UTF8, so it seems like transcoding overhead from UTF16 to UTF8 might just be for strings that are created on the client. So, relatively small?

Thanks!', 'phritz');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('lRTO3i_Criuo_IwxcWCHr', '6eZ-NkNCr5Kg2hWBD3W5K', 1652944723000.0, '> Second question: can you help me understand the advantage of the proposal above over the seemingly equivalent strategy of defining keys as UTF8 strings? 

I talked with aaron a bit about this and he pointed out a couple of things:
1. we don''t currently have a way to directly access the keys in the pull response as a UTF8 string or bytes. When we decode the response we get UTF16 strings, so we''d have to translate them back into UTF8 to do this. Presumably that is too costly. Or we could switch to a decoder that gave us direct access to the bytes while decoding, if such a thing exists.
2. it might be less convenient to browse keys in the web inspector. right now you can read them as strings and that is very useful. if they were displayed as bytes or similar that''s a lot less useful.', 'phritz');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('f-hI9ytRVpOJuAlu_po02', '6eZ-NkNCr5Kg2hWBD3W5K', 1652948544000.0, '@phritz This all sounds right to me.

Another thing to remember is that the keys that gets passed into put, get, has, scan are all JS strings (utf16).

In the past when we used `Uint8Arrays` as keys we saw a lot of time being spent in `TextEncoder` and `TextDecoder`. Logically it should not be expensive to use these but these are not part of V8 and a lot of optimizations are not done.

Another thing to remember is that V8 (and other engines too) internally use ASCII strings whenever possible and this is the common case and these are very efficient.

I would be willing to do an experiment with using `Uint8Arrays` again but I cannot imagine it being faster.

', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('RLpgEH72T9ZlPiJilfl9F', '6eZ-NkNCr5Kg2hWBD3W5K', 1652987399000.0, '@arv can we make part of closing this issue out adding an item to HOWTO > Launch to Production (or similar spot in docs) that covers key sort order and what they have to do on the server? 

@aboodman when you get a sec can you ack that won''t support locale-aware key collation? Seems OK to me as I think about keys more as identifiers and less as content anyway.

Re:

> I would be willing to do an experiment with using Uint8Arrays again but I cannot imagine it being faster.

I do not think it is worthwhile having byte arrays as keys for efficiency''s sake. I think it would be worthwhile from a *usability/understandability* point of view. If we had byte string keys it would be super clear how to sort the keys on both client and server, there is no opportunity for using the wrong locale or sorting function, and there''s no "missing feature" of having locale-aware key sorting (because nobody expects that of byte arrays).', 'phritz');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('SXfP-EdwBiTPjsonnQSv1', '6eZ-NkNCr5Kg2hWBD3W5K', 1652995262000.0, '> @aboodman when you get a sec can you ack that won''t support locale-aware key collation? Seems OK to me as I think about keys more as identifiers and less as content anyway.

Yes, I agree that is how we should think of the keys.

> If we had byte string keys it would be super clear how to sort the keys on both client and server, there is no opportunity for using the wrong locale or sorting function, and there''s no "missing feature" of having locale-aware key sorting (because nobody expects that of byte arrays).

I agree but it''s hard to implement with the rest of our system because:

1. The pull response is JSON. JSON doesn''t have a byte array type for the keys. It would have to be some kind of encoded string.
2. Our target audience is JS developers. JS doesn''t have good support for byte arrays.

It basically just very un-ergonomic to work with byte arrays in JavaScript. I think overall the simplicity/understandability is better if we say they are strings and specify the sort to be bytewise of utf-8 encoding.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('QrWCkihcFufwBdaa3c8tN', '6eZ-NkNCr5Kg2hWBD3W5K', 1666298396000.0, 'Done', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('_5Ii4uvUzIx_17VfQwb-C', 'RSiJp12d7Cp0_-sbdh3xx', 1652363436000.0, 'Done by @aboodman in e70bef9a35343b4e285ce5134c12ba7892a4c620', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('-jY4oZB7JhF7mrMaVvjI6', 'Fj7117VgpCh38LCESNK0O', 1653529529000.0, 'Moving internal discussion internal. I think this external bug is a good example of why we should have an internal repo and an external one :). We gain little by airing our dirty laundry.

Anyway: With some space, I don''t want to go overboard with the options for this silly little API.

I agree with Tom that it''s typical in database systems to be able to say whether a foreign key permits nulls or not. If it permits nulls, then obviously there should be no message at all and just skip the row. If it does *not* permit nulls, then I agree with everyone who has said that ideally the transaction should not commit in the first place (i.e., the behavior should be `throw`, not `skip`). I don''t know that there is any real use for the behavior `skip-and-log`, which is what we have now.

However, if we make the default `throw` now that would be a breaking change. So what I would like to propose is:

1. Add an `allowNull` flag to `CreateIndexOptions` which defaults to `false` which changes the behavior to silently allow nulls. This is a non-breaking change so can go out right away.
2. As a separate commit, change the default (`allowNull = false`) behavior to:
  - throw on null index values if `allowNull` is false (this is a breaking change)
  - put the better validation on json paths suggested in https://github.com/rocicorp/replicache/issues/913#issuecomment-1136730132 (also a breaking change)

We can do a point release from trunk after 1 is landed. Nothing else on trunk is a breaking change currently.

Separately, I think we should do:

3. Guard the changes from 2 behind a runtime flag. This would be good because it would mean that we could still do dot releases of 10.x after (2) has landed. We have never used this "always shippable" strategy before on Replicache, but it''s common at Google, and we''ve talked about it being a good idea for Replicache in the past. I will file a separate bug for this however.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('phkkYyxIv2WZLs0_TfdhK', 'Fj7117VgpCh38LCESNK0O', 1653531129000.0, 'Discussion for part 3 here, but can be totally separate from this bug: https://www.notion.so/replicache/Runtime-Flags-1f38820f4d4b4ea18905fb62dc9ecb4e', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('85SgPVt_hG7ymJEwm-kr1', 'Fj7117VgpCh38LCESNK0O', 1653635505000.0, '@aboodman `allowNull` is too hand-wavey. What does it mean to allow null?

- Does it cover a present value of `null`?
- What about missing missing properties?
- Then there is the case of invalid array indexing.
- Invalid path syntax

All of these were silently ignored before.

Now we are adding a flag that covers one of these cases. Which one is not clear?', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('EF-OzLmxluZN0pdupgJqK', 'Fj7117VgpCh38LCESNK0O', 1653638355000.0, 'I think the right approach is:

1. Make path syntax errors early errors in `createIndex`

The current behavior is to abort indexing when an error occurs. It does not revert the keys and we end up with an incomplete index map ðŸ˜¢

I don''t know if we can really throw. indexing happens in `createIndex` as well as mutations and pull. If there is an error indexing we must not abort the mutation or pull.

2. Make sure we do not write incomplete index trees when there is an error

3. Decrease verbosity of logging the error **or make it optional** (using `LogLevel`)
    1. If optional my suggested option name is `errorLogLevel`', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('2RRBhaw_nTZX6fchpNCes', 'Fj7117VgpCh38LCESNK0O', 1653644826000.0, 'Sorry ingar :-/.

I think this is going to be hard to solve when none of us are online at the same time. Big picture I was trying to suggest separating out something simple that addressed user complaint from the "right thing".

Stepping back further nobody is even asking us for the current behavior of treating null/undefined fields as an error. The only reason we log when encountering null/undefined is because we only know how to index string, and I felt it was confusing to silently skip other types. But it''s silly to keep trying to work around such a speculative feature. Let''s just remove it.

I''m now in favor of just deleting the log line in the case the value is null/undefined on trunk and forgetting a `allowNull` or similar field entirely. The rest of this can be separate and might take awhile to asynchronously work through. Ingar could move onto other tasks in the meantime.

===

> Make path syntax errors early errors in createIndex

We agree. I was just trying to do this separate from this review since it''s a breaking change.

> The current behavior is to abort indexing when an error occurs. It does not revert the keys and we end up with an incomplete index map ðŸ˜¢

That sounds like an existing problem not introduced by this PR? Can be addressed separately.

> If there is an error indexing we must not abort the mutation or pull.

I can see both sides of this. We do abort mutations and pulls for other reasons btw that are dev-controlled. So it''s not breaking precedent. And you could say that if the user said allowEmpty=false it should be an exception to write such a value! That all said nobody is asking us to do anything if empty values are present so let''s not drive ourselves crazy. We can just remove this error case until we have more information from users.

> Decrease verbosity of logging the error or make it optional (using LogLevel)

I don''t want to add a bunch of API for such a silly feature that nobody is asking for.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('1aNbZQBz4W-49icQvpvEF', 'Fj7117VgpCh38LCESNK0O', 1653644888000.0, 'Basically if we can please do something simple and non-breaking to address user complaint of log spew let''s do that and treat the rest of this separately and potentially lower priority.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('I-ttabMWq85bwc-NMLggF', 'Fj7117VgpCh38LCESNK0O', 1653654350000.0, 'Right now we log using `info` which is the default. If we switch go `debug` then the logging will be off by default.', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('zcaRzUihh9H7lVjHXErIU', 'Fj7117VgpCh38LCESNK0O', 1653678533000.0, 'I''ve gone back and forth about this and I see what you mean, but I think the current solution has some things to like:

1. The first time I (and many) people use `createIndex` they get something about the syntax wrong. If the system doesn''t complain loudly, it''s hard to know whether it''s working, what the problem is, etc. Silent failure for the default is a bad dx. If we change the log level to `debug` people won''t see this output because people don''t typically leave `debug` on.

2. But once people know the system and are seeing this message and don''t want it, they can turn it off manually.

I agree the API around indexes in general is wonky and needs rethink, as well as some near-term better error handling, but I think what was just landed is a good first step. Are you good to ship it? (Think of this as API review).', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('EfF8hz8072my2vOiZePXy', '1WwWxmJrHtPBz49v4K9sl', 1651268263000.0, 'See https://github.com/rocicorp/licensing/blob/main/api-versioning.md for how to add things to the active ping request.', 'phritz');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('ygCFdTYwGwzcmXPM00Lr4', '1WwWxmJrHtPBz49v4K9sl', 1651567645000.0, '`version` was added in cbb2e6ef85dfdfc53686f1783b5d17da0753793d', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('vE_PIeHZeJ0C-RfBVgTrT', 'B1bTC9qD6ERL5E0wh4y_a', 1651388010000.0, 'The new website says five and five is what weâ€™ve said elsewhere. Any reason to not do five?', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('WwAX7ZBSmKJ9_7evdXSeL', 'B1bTC9qD6ERL5E0wh4y_a', 1651388358000.0, 'I dunno bro, the bug says 10. Who can we trust?', 'phritz');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('vy7KhRy_X3OFAPbXeQ4vN', 'RzLiAkwI-mbB_IYlWSqDu', 1650984708000.0, 'I guess we should change the script to output two files (no need to run the perf tests (twice)', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('MNMYsgcea5034P-RtlslC', 'RzLiAkwI-mbB_IYlWSqDu', 1655480167000.0, 'Fixed with 8f06229a3d4ebc90051d60f99732aa41cdeb1f6e and 91da6166062f6d4e00ee71cb76303517d7a37018', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('boLHJY_WlOj-69_Ut3CUu', 'jxq8YIb0gEwLTUWn1XfbW', 1652362900000.0, 'Seems fine according to https://www.skypack.dev/view/replicache

<img width="322" alt="Screen Shot 2022-05-12 at 15 41 25" src="https://user-images.githubusercontent.com/45845/168088757-6895a836-e5f3-4bc2-815e-5a4c8c04c76f.png">


', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('AYvg9zIt-jMyY4fR68NDg', 'jxq8YIb0gEwLTUWn1XfbW', 1652362968000.0, 'rocicorp/mono#102 for keywords', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('0MWV8kAvPwp58n-sbtYiB', '6bnD-KvMSKUP59KqjkxZs', 1650913873000.0, 'I see a data point for https://github.com/rocicorp/replicache-internal/commit/184321fef9c4db86aa94e45fdac68e827f4da983 and that has a failure due to the perf regression

<img width="504" alt="Screen Shot 2022-04-25 at 21 10 03" src="https://user-images.githubusercontent.com/45845/165157525-a8daa2d6-8f22-469b-860e-4c1f575b2b98.png">
<img width="791" alt="Screen Shot 2022-04-25 at 21 10 38" src="https://user-images.githubusercontent.com/45845/165157613-d452f0fb-bc77-4f9b-bb18-fe520ea243e6.png">
 ', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('_Q2jgVileJ1abCvNGQ5P0', '6bnD-KvMSKUP59KqjkxZs', 1650923582000.0, 'We seem to be getting data points now but there is a discontinuity between `558d93c` and `1c6460f`. Guess we are just prepared to say ðŸ¤· to what was going on? That''s fine with me I guess. @arv if you concur feel free to close this one.', 'phritz');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('sv__oU618NDp8bEaoGtgY', '6bnD-KvMSKUP59KqjkxZs', 1650981322000.0, 'I think it is working now... Keeping my eyes on it a little bit longer', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('PsqH_I2bLGVpRvf9_WinT', '6bnD-KvMSKUP59KqjkxZs', 1651133428000.0, 'Closing. Works now', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('frAlq8GzL6mqVzi-zwq2X', 'JneOa0-sxjHCP_CWn9MKx', 1648666850000.0, 'Starting on this.', 'grgbkr');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('EywA7-oW6ENnCUbDWYotj', 'JneOa0-sxjHCP_CWn9MKx', 1648668810000.0, 'This is generally speaking to enable *customers* to send their users'' logs to the *customer''s* datadog, correct? We of course can default our sample apps to sending to our datadog, and let existing customers send to our datadog until we shake the bugs out. But longer term the idea is not that all customers'' client logs come to us, correct?', 'phritz');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('2EQR8C-4pwhL5gd1D0_5d', 'JneOa0-sxjHCP_CWn9MKx', 1648670491000.0, 'Itâ€™s only to enable customers to send logs to their own DataDog. Nobodyâ€™s sending logs to us except us.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('vDMl3onikUH4qRgPug8Em', 'JneOa0-sxjHCP_CWn9MKx', 1648671851000.0, 'See https://github.com/rocicorp/replicache/pull/907', 'grgbkr');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('GH53vNG3OfKiBSr-n9kol', 'JneOa0-sxjHCP_CWn9MKx', 1649098969000.0, 'It looks like replidraw-do needs to be updated to take advantage of this still? (But I assume you will do that after npm/api cleanup).', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('avI4niPPjMBEzHa_LYhTJ', 'RNWQCv_4hbaVYMIRSFVjo', 1673279505000.0, 'If I remember correctly, the reason for this to be a function is that it needed access to the DO env and that is not available at startup creation of the server... Actually, it was the logger/logSink that needed the Env.

https://github.com/rocicorp/reflect-server/blob/0b1e163f5204e3621874623c21a365526df31d15/src/server/reflect.ts#L17-L18

For consistency the `logLevel` should also be a function. It is very reasonable to have the logLevel be a function of the Env.

CC @grgbkr ', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('uuJ1F7nJrrdQffxF5VEuK', 'RNWQCv_4hbaVYMIRSFVjo', 1673292761000.0, 'OK that actually does make sense. Apologies for the noise.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('9DYDOdL4HRv3oYzWqy1hV', 'pm8HuooXYxbgDWDJK4pwG', 1673615807000.0, 'I really do not know what to do here. Are you talking about `reflect`, `reflect-server`, `@rociciro/logger` and/or `replicache-do`? 

Remember that reflect/reflect-server cannot use `DataDogLogSink` by default. It needs a datadog client token. Also, it seems plausible that our customers wants to use Sentry or some other logging service.

For reflect-server I think it is fine to always log things to the console, but for the client that does not seem like a good idea to do by default.', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('4SwY9XiM8JkZpwDL07F68', 'pm8HuooXYxbgDWDJK4pwG', 1673630524000.0, 'I think @aboodman is talking about @rocicorp/logger, which might have implications that trickle out to its consumers. I think the suggestion is that if you are using logger then consoleLogger is enabled by default. And then if they want to pass an _additional_ logger they can and it gets tee''d. That''s the suggestion as I read it. I think you might provide an answer to aaron''s question "I''m struggling to imagine a case where one would not want console logging enabled", which is "for the client that does not seem like a good idea to do by default."

So I think we should wait to hear from aaron.', 'phritz');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('Xo58UX2ChSmB4zLfzPNWu', 'pm8HuooXYxbgDWDJK4pwG', 1673634702000.0, 'This bug was fixed since it was filed. I wanted to not have to setup the
tee logger manually (as replidraw does) and instead pass in an array of log
sinks. This has been done.

There is a separate much smaller question of whether to assume the user
always wants console logging. I can see Erikâ€™s pov that itâ€™s nice to be
able to disable it (ie for tests).

So this bug can be closed.

On Fri, Jan 13, 2023 at 7:22 AM Phritz ***@***.***> wrote:

> I think @aboodman <https://github.com/aboodman> is talking about
> @rocicorp/logger, which might have implications that trickle out to its
> consumers. I think the suggestion is that if you are using logger then
> consoleLogger is enabled by default. And then if they want to pass an
> *additional* logger they can and it gets tee''d. That''s the suggestion as
> I read it. I think you might provide an answer to aaron''s question "I''m
> struggling to imagine a case where one would not want console logging
> enabled", which is "for the client that does not seem like a good idea to
> do by default."
>
> So I think we should wait to hear from aaron.
>
> â€”
> Reply to this email directly, view it on GitHub
> <https://github.com/rocicorp/mono/issues/259>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AAATUBDKXRELG2NMZ73DNSLWSGFMRANCNFSM5RDTJ52A>
> .
> You are receiving this because you were mentioned.Message ID:
> ***@***.***>
>
-- 
a (phone)
', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('Ueu5GzIqQ7i35kAN_wIBs', 'WvmdaY5OczICNTBRd4xPe', 1647373649000.0, 'I can do this one.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('Mc-UBAcRD1tfl8-90hwFX', '1h3HiF7eonal9C7FKRR3o', 1673262939000.0, 'I think this issue is stale.

What does this mean? When the server starts there is no `roomID`. The roomID is created by the  REST endpoint `/createRoom`. The `roomID` is then later used as part of the URL of the web socket. On the client we also include the `roomID` in the LogContext.', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('SgZAg-ubAXPSZltEIS-rt', '1h3HiF7eonal9C7FKRR3o', 1673288408000.0, 'I think this means printing the room ID in a log line when the room DOl starts. I''m on mobile so not sure if stale. I suspect the reason we wanted this is so that we can see when room dos are restarting', 'phritz');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('Soz51l9jdzZb_BmxlzAFb', '1h3HiF7eonal9C7FKRR3o', 1673290471000.0, 'I was using loose language when I created the issue. I meant printing the roomID early on when the DO starts.

I think this means moving this branch: https://github.com/rocicorp/reflect-server/blob/main/src/server/room-do.ts#L104 into the one above and then printing out a "initializing room" or something from the lc. I think it should be at info level (counts as "significant state change" to use Fritz''s language.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('qc-AONUNoHNyKaLIiU8OF', 'XgIcGIjAf8FTov8V44Ycs', 1648671942000.0, 'As a first step adding a optional LogSink ReplicacheOption https://github.com/rocicorp/replicache/pull/907', 'grgbkr');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('v6iwpCDKMUyPiER0QpC-6', 'XgIcGIjAf8FTov8V44Ycs', 1649695897000.0, 'Available in @rocicorp/reflect@0.4.0', 'grgbkr');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('phqw_6tXuSuTNJs7vG4QF', 'K1h3em6-Yvc0lz-8TvEvB', 1647373610000.0, 'I created a bug for client-side logs (https://github.com/rocicorp/reflect-client/issues/12) and added it to the internal monday priorities list: https://www.notion.so/replicache/Monday-com-Priorities-internal-3fd7351956cd4e1baf1e5617f0ee8498.

I also created bugs for the others, but they aren''t as high priority and don''t need to block other Reflect work.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('Ly6crpZX37k7GAh7dBcAG', 'jHMfshAirFdTUkTyPfdTH', 1646772765000.0, 'Thanks @arv ', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('32Xmr04L1_baYB0R24_aX', 'cOnsigzGN2MaN6wZhlpxs', 1675935866000.0, 'Waiting on @aboodman to review pr.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('lclFohnx_aSixQi_f7Dal', '3GlW0ZTQFt_dSJ1jVJhXD', 1684383883000.0, 'https://www.notion.so/replicache/Fast-er-Forward-62a96385bd0d4931b5db868e172049cd', 'darkgnotic');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('cJ3TnsqIHXF-tBh5zK0aK', 'GMRD7IaJH_4V0DRLP6hsE', 1663351170000.0, 'related: 
- https://www.notion.so/replicache/Requirements-fb82ffc6c695496aadd59875fa03acfb
- https://www.notion.so/replicache/WIP-Streaming-Replicache-4acd7513121949f5898f7eeeeeaef96f#e2dbf4dee6574e2c81d266bd57d2fc77
- https://www.notion.so/replicache/Reflect-Alpha-a5369ac380d247b98a0170bb1688804d#49be32958b174b458a43e1b2bca39f04
- typical change is a mouse move:
    - 16 byte client ID
    - 8 byte timestamp
    - two 8 byte coordinates
    - 100 bytes', 'phritz');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('lM-b3YuXe5YOediMQ7pso', 'W5zvTqsrw-A_hT6R6Y2uI', 1672740936000.0, 'Duplicate of rocicorp/mono#316 ', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('qKcmDgG0_UJR4SCHn74rp', 'PE4kaK4nCKkHwH24WZopE', 1663280923000.0, 'For beta: We need to setup basic framework for monitoring and alerting so that we can move fast when something comes up.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('TM62KyLE9sdy4Ya3ctZQy', 'PE4kaK4nCKkHwH24WZopE', 1673557440000.0, 'To make this a little more concrete I think the scope of this issue is potentially quite broad, seems like it will spin other issues out as we get to them. The ''monitoring and alerting'' umbrella I think could potentially cover at the very least (feel free to edit):
- monitoring
  - have an app-agnostic dashboard graphing key metrics from reflect client and server
  - have at least one sample app running this dashboard
  - have a way for customers to import the dashboard config so they don''t have to build it themselves
  - include environment (prod etc) and reflect/server version in metrics via tags
- log analysis
  - teach datadog to parse out our custom attributes (doID, etc) if it doesn''t already know (via pipline)
  - have a way for customers to import the pipeline 
  - include environment (prod, etc) and reflect version in logged lines and teach datadog to filter by it
- alerting
  - for the sample that has the dashboard, configure data dog to alert on errors (after https://github.com/rocicorp/mono/issues/195)
  - determine what metrics we should have alerts for and implement those
  - have a way for customers to import our alert rules
', 'phritz');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('BVqeFqXBHmCGuye59aHUg', '-cYdezEfY1tZU2II3tN8M', 1663280579000.0, 'CF hard restarts a DO when exceptions goes to top level so we def want to catch.', 'phritz');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('hwLzuKDPEKq9Zvs8S13od', '-cYdezEfY1tZU2II3tN8M', 1672740841000.0, 'I think that probably rocicorp/mono#212 fixes this, but we should try it and confirm.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('odsYZh1htnkP7bV4UtrrC', '-cYdezEfY1tZU2II3tN8M', 1672972264000.0, 'Kinda bigger picture, we want to ensure that we see when DOs are restarting for expected (code update) and unexpected (uncaught exception, OOM, etc) reasons. Part of this story is ensuring we hear about these events via logs, current logging buffers non-errors for 10s and it''s likely we don''t hear about OOMs and similar conditions that just outright kill the DO. Theory is that CF logpush can help. But the other part of this story is that we need metrics around (re)starting so we are not relying on logging for this eg count of DO starts in a given period as well as some kind of rapid restart or flapping detection (a given roomDO quits and starts in rapid succession). This second aspect is probably part of https://github.com/rocicorp/mono/issues/201 which requires fleshing out. ', 'phritz');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('kiz-R7e5htXZiWiTQjZKb', '-cYdezEfY1tZU2II3tN8M', 1675936034000.0, '@arv this is basically a dupe of rocicorp/mono#212 but I guess there''s a chance logpush doesn''t work out for us (which would be odd).', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('oqjvos7y4W_MtQZ0Lj3yg', '-cYdezEfY1tZU2II3tN8M', 1677704985000.0, 'This has been fixed by #22 ', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('vq4pckRBWLgvaT-a0UrFh', 'OeqGbyKoZyURoaRcUCYQJ', 1663280530000.0, 'I think it should actually use same exact code as Replicache. The reconnect/backoff options would work prefect for how often to try to reconnect the socket.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('_BwZnkN6oLftFn5kecIyh', 'OeqGbyKoZyURoaRcUCYQJ', 1675129559000.0, 'superseded by rocicorp/mono#200 ', 'phritz');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('4Yu48skOx_86_XI8ojInv', 'tYL-OzLT2Nbd3wup_H5Hl', 1649695986000.0, 'Published at [@rocicorp/reflect ](https://www.npmjs.com/package/@rocicorp/reflect) and [@rocicorp/reflect-server](https://www.npmjs.com/package/@rocicorp/reflect-server).', 'grgbkr');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('JXniYA5zmzz_NBerUmyVa', '5AH5f4_WUUkjUBv1p08yS', 1647498293000.0, 'Specifically, we should end up with one `Reflect` class which is the client which has an API which is roughly:

```
Reflect = Replicache
- stateless http protocol
- createIndex and friends (just no need for it yet, let''s wait for more info)
+ stateful socket protocol
```', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('-dMkAAb0Qnl99viMbJ1N7', 'wnEQdT-7FW2WRTImdrBly', 1646940883000.0, '# What to replace Zod with?

I used superstruct but it turns out that I misread the benchmarks It is slower than zod

Some quick notes based on the benchmarks at https://moltar.github.io/typescript-runtime-type-benchmarks/

- `ajv` is too large
- `ts-json-validator` depends on `ajv` and is too large
- `suretype` depends on `ajv` and is too large
- `valita` has no runtime deps... Let me try', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('gfHQC_tbtzZKbL2-hWKtb', 'wnEQdT-7FW2WRTImdrBly', 1646942690000.0, 'Here is a working valita example:

```ts
import * as v from ''@badrap/valita'';

type JSONValue =
  | string
  | number
  | boolean
  | { [key: string]: JSONValue | undefined }
  | JSONValue[];

const jsonValueSchema: v.Type<JSONValue> = v.lazy(() =>
  v.union(
    v.literal(null),
    v.string(),
    v.boolean(),
    v.number(),
    v.array(jsonValueSchema),
    v.record(v.union(jsonValueSchema, v.undefined())),
  ),
);

const o = { a: ''a'', b: 1, c: true } as unknown;
const o2 = jsonValueSchema.parse(o);
console.log(o2);

// test extra fields
const s2 = v.object({
  a: v.string(),
});
console.log(s2.parse({ a: ''s'', b: ''extra'' }, { mode: ''passthrough'' }));
```

esbuild minimized:

```
  index.js  12.2kb
```', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('D3Ce9KrGHQ_8fiM_R-eYQ', 'wnEQdT-7FW2WRTImdrBly', 1678376843000.0, 'I know I''ve been going back and forth on this for too many times to count... But I''m reopening this with some new insights.

## Problems:

1. All existing runtime type validators are too slow to validate JSON. Especially large JSON structures that we have seen in the wild (i.e. Placemark)
2. Some validators clone the data at all times (i.e. zod)
3. Some validators have large code size and do not allow dead code elimination.
4. Some validators have bad error messages (i.e. superstruct)

## What I''m suggesting 

Use a validator that allows custom validation and use that for the JSON type. That way we can short circuit the runtime validation with our own that is much faster. We can even completely disable it in release mode.

After another stab at this I''m leaning towards [@badrap/valita](https://github.com/badrap/valita):
1. It has a way to do custom validation using `v.unknown().chain()` so we can use our own json validation function
2. Valita does not clone when doing `parse` (when `strict` or `passthrough` parsing)
3. Relatively small code size [bundlephobia](https://bundlephobia.com/package/@badrap/valita@0.2.0)
4. OK error messages. We can wrap these if we want', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('Qr0y1TI-wsiShoxxD1NV2', 'cIU1_Ipxb_LvoWwjS7Eni', 1646733539000.0, 'What does this mean? License pings etc?', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('ZiGwib-XRPVB4lxlJ3bWT', 'cIU1_Ipxb_LvoWwjS7Eni', 1646759796000.0, 'Yes potentially checks and pings but also anything required on the backend: a new license type, whatever billing view we need on reflect licenses, updating any visualizations to include, etc.', 'phritz');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('7NL8sbFYpJ4lUbulAEKeg', 'cIU1_Ipxb_LvoWwjS7Eni', 1646760551000.0, 'Updated description. Sorry for lack of detail.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('0HCFT7kAjFce98fvPXNnV', 'cIU1_Ipxb_LvoWwjS7Eni', 1663280497000.0, 'Needs product/pricing design. Closing for now. Also kinda dupe of rocicorp/mono#23 .', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('1iDPXWlzEoATz_--PD3YT', 'Qbc1-oTPHc1ZBHdmYunQA', 1677698531000.0, 'This idea was abandonded: https://rocicorp.slack.com/archives/C013XFG80JC/p1677695514288939', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('JG5d5mdTKXZqAmkmSAgcg', '0sQUzcq3Xlc9ZhjX4_3av', 1672741332000.0, 'Note this ideally includes as a dependency the new roci.dev webpage :-/.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('bAFSmII_bHEbn6aoEkgXL', '0sQUzcq3Xlc9ZhjX4_3av', 1679346200000.0, 'latest review here: https://rocicorp.slack.com/archives/C013XFG80JC/p1679345082508799', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('6feD0RVJ6vDmixuDgGvr4', '0sQUzcq3Xlc9ZhjX4_3av', 1681146777000.0, 'I''m going to mark this done - now into ongoing maintenance.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('s0iIsDzpVQ7eLO_mCH2Iu', 'nqDMl_RchEnwVwJ2iqLWx', 1647909210000.0, 'I think this is complete right @grgbkr ?', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('7hx9c2yARi4wEThrGLu-h', 'nqDMl_RchEnwVwJ2iqLWx', 1647972008000.0, '@aboodman There is one follow up that really needs to be done.   We need to garbage collect connections from the AuthDO (right now they will grow unbounded).  

The GC follow up is the only must do, there are also these other potential improvements:
1. re-auth connections every N minutes.
2. use finer grain locking in AuthDO (requires adding userID as param to connect requests)', 'grgbkr');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('klBpGPKiPPXO43readS2c', 'g3ldLQv0tRPXdIxcjW7nW', 1646149867000.0, 'Lots of updates here:

1. Noam wasn''t able to add us to cf for security reasons, but he did invite us to datadog. You should have receivied an invite at greg@roci.dev. Once you accept, you need to login and you will have ability to select a different org in datadog here:

<img width="423" alt="Screen Shot 2022-03-01 at 5 40 08 AM" src="https://user-images.githubusercontent.com/80388/156199850-b279d3ba-6b07-4477-97fb-2efc445ed627.png">

2. Noam says it is relatively easy to reproduce this bug. He says is happens ~everytime he draws "intensely".

3. Noam says that when the bug happens the symptom visible to source user is typically this client-side error message: 

<img width="1498" alt="Screen_Shot_2022-03-01_at_16 45 47" src="https://user-images.githubusercontent.com/80388/156202030-2587e3e3-b60d-4c37-9e97-a360a5e1a5a3.png">

4. Noam captured client-side and server-side logs (at info level) from one of these sessions:
[logs.zip](https://github.com/rocicorp/reflect/files/8162834/logs.zip). The zip file also contains a heap profile but I''m not sure if that''s from the same session. Noam not able to reproduce error at debug log level so far.

Thoughts scanning through these logs real quick... it looks like the "client not found" is the immediate cause. It does make sense that if a client wasn''t found on server then symptom would be as Noam describes: mutations would pile up client side, drawing would appear to work, but when you refresh drawing not saved.

I do see the client-not-found error on server too. It appears this happens after two disconnect/reconnect cycles on client. Appears that somehow server state gets confused as to whether client is present.

5. (Not sure if related) Noam says that when he refreshed the session this occurs in he gets this error immediately on refresh:

<img width="1512" alt="Screen_Shot_2022-03-01_at_16 59 08" src="https://user-images.githubusercontent.com/80388/156202528-1f387e04-4e0a-4c8f-8519-d6167c925847.png">', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('rEDzvel4bPNmgNJYS8hBS', 'g3ldLQv0tRPXdIxcjW7nW', 1646150841000.0, '> It appears this happens after two disconnect/reconnect cycles on client.

An underlying question is: why do we disconnect? I do see the server restarted right before this happened, but there''s no indication why.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('3zxjAT2OPURBVYNmhbGh8', 'g3ldLQv0tRPXdIxcjW7nW', 1646152140000.0, '> but there''s no indication why.

Two thoughts:

- What does a durable object do when an unhandled exception happens directly inside a request handler? What about outside a request handler? You''d expect such unhandled error to make it to wrangler log, but not surprising it doesn''t make it to datadog.

- We already know of one case where the server fails silently -- large upload. Is there some way that we could have gotten into a situation where we have a large 1MB upload?', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('moKLoAAaV9I0Q-cTfWZND', 'g3ldLQv0tRPXdIxcjW7nW', 1646153028000.0, '> What does a durable object do when an unhandled exception happens directly inside a request handler? What about outside a request handler? You''d expect such unhandled error to make it to wrangler log, but not surprising it doesn''t make it to datadog.

I tested this. The error doesn''t make it to datadog :(. But it also doesn''t restart the server. The exception is caught by CF at top of event loop and logged to wrangler output. So it doesn''t explain the server restarts in noam''s log.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('YK9DjLhjz5hRZI_-EAyhC', 'g3ldLQv0tRPXdIxcjW7nW', 1646156634000.0, 'Lots of useful debugging info here.  Thanks Noam!

A few updates.

1. I am able to successfully access Monday''s datadog logs.   
2. I have not been able to reproduce the bug myself despite intensely scribbling for 4 mins (now my hand is tired :)).
3. I do see others hitting this "client not found" in the server log.  These clients are then wedged and try to keep pushing over the same web socket connection with the same client id, resulting in this same error over and over again.  I have not yet found the root cause for why the client is not found.  I have identified one change we should make that will prevent clients from becoming wedged when "client not found" occurs.  The connection should be closed by the server, as no messages over that connection will succeed.  Then the client can reconnect, and after reconnection should be unwedged.

I''m continuing to try to find the root cause of "client not found".



 
', 'grgbkr');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('RtlirDOEeenEWRf07pL7N', 'g3ldLQv0tRPXdIxcjW7nW', 1646156977000.0, 'If a client receives "client not found" (for some reason) and user keeps scribbling, they will soon hit 1MB upload limit, right?', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('jkOwRvBIzxbzsGbibo2ym', 'g3ldLQv0tRPXdIxcjW7nW', 1646157993000.0, '> If a client receives "client not found" (for some reason) and user keeps scribbling, they will soon hit 1MB upload limit, right?

I think it would take about 10 minutes of drawing to get to a 1MB push.  (based on going offline and scribbling hard for 3 mins led to 300KB push on reconnect.)', 'grgbkr');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('81uqYQdypmlOag1USg9ZW', 'g3ldLQv0tRPXdIxcjW7nW', 1646161588000.0, 'I found one bug in connection management that results in the "client not found" error.  If a client tries to reconnect, while the previous connection is still open on the server, a race condition occurs and we end up deleting the new connections entry in the client map (when we meant to delete the entry for the previous connection).  I am fixing this race now.  Next why... why is the client trying to reconnect?', 'grgbkr');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('sx-9_39qq-FPwVCvU5SmO', '5Gws1JQqvka70AE11H8ov', 1646660742000.0, 'It would also be interesting to know if this limit applies to binary web socket messages too?', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('Rk2cNVeCI_LUx9W32GcmQ', 'Er2XPJ-7DeyGmFpX7Wo0-', 1645666346000.0, 'Here is a video of the bug. I can reproduce this easily in canvas:

https://drive.google.com/file/d/1Bf3rUjcsoFuOAA1VhOl2Zdq8_xuIorOI/view?usp=sharing', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('Kh24VZXAy7alhuD_vIVwR', 'Er2XPJ-7DeyGmFpX7Wo0-', 1645666449000.0, 'I think there are two questions here:

1. Why does the server crash? This doesn''t seem like sufficiently many mutations (by a long shot) to exceed the 128MB of memory workers are allotted.

2. I get that if a particular push is going to crash the server, it''s going to happen when we try to recover mutations too. But in that case, how come it doesn''t keep happening forever? ðŸ˜¬ Did we do something smart to only try to recover mutations for a little bit?', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('htAsvCqvmSqs4eqU4AsVK', 'Er2XPJ-7DeyGmFpX7Wo0-', 1645668739000.0, 'I created a PR in replidraw-do that replicates this issue I believe: https://github.com/rocicorp/replidraw-do/pull/32.

If you press the "duplicate all" button enough times the server crashes -- I assume for the same reason as canvas.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('dB39Qwi2k25Ouj74xG9tx', 'Er2XPJ-7DeyGmFpX7Wo0-', 1645669209000.0, 'Here is a video repo of the stress test PR:

https://user-images.githubusercontent.com/80388/155444911-45d5edc1-9379-4395-a179-4515325cd819.mov

', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('KOXzSaTrfNjmykS4A3RY6', 'Er2XPJ-7DeyGmFpX7Wo0-', 1645722843000.0, 'I did a CPU and memory profile of the worker using the `wrangler --inspect` option (super easy!) and found something very interesting:

1. It definitely doesn''t appear to be CPU bound. The worker isn''t doing anything according to the profile for all those seconds. The processing only takes a matter of a few hundred ms.

2. I don''t see any massive memory allocations either. It reports to only be using like 5MB or something.

If I run the worker in logLevel=debug mode, I do see an OOM crash that occurs. But it typically happens on the 500KB push, not the 1MB one, so not sure if it''s the same reason we crash in logLevel=info mode. In info mode, I don''t get any such message, the worker just reboots. Need to check the cloudflare dashboard and see if better messages there. Or maybe if you run it under inspector and have it pause on exceptions you can catch why it''s rebooting?

Simplifying: the current question is: why does the worker reboot at ~1MB push using the stress test under logLevel=info. Also (and presumably related) why is the poke response from the 1MB push so slow.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('JeZz7zBFvXP17EiQxkH6h', 'Er2XPJ-7DeyGmFpX7Wo0-', 1645749216000.0, '@grgbkr isolated this down to an apparently undocumented limit on upstream message size in workers. If we send more than 1100000 (~1MB) bytes upstream to a worker, it restarts. In the downstream directly we haven''t discovered the limit yet - up to ~50MB appears to work.

We are asking our contacts at CF to confirm this, but assuming this is a limit issue and it mainly applies in the upstream direction then changing this mutator to a `copyAndPaste(ids)` shape should alleviate the issue.

We have also confirmed that the actual amount of time spent processing this message isn''t an issue -- in our tests only a hundred ms or so is spent processing this large simulated copy/paste on the server.

As for the data loss - this isn''t unexpected given above:
- If the push fails due to size limits, then the client will reconnect and try to push again, and that push will also fail.
- If you reload, the client will again try to send the push and it will fail.

The thing that *is* unexpected is that the client seems to somehow get *past* this large push and carry on before reload. We would expect the client to be permanently stuck trying to send this large push until the page is reloaded, but we don''t see that.

Finally, there is a separate issue of _preparing_ the push to be sent to the server getting progressively slower on the client-side and stalling the UI. Unclear if that is something that needs to be fixed or if `copyAndPaste(ids)` would also fix that.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('nuld1b5UKvDNqAiJdFOh0', 'Er2XPJ-7DeyGmFpX7Wo0-', 1645773092000.0, 'Proposal: Enforce a new restriction that individual mutations cannot be bigger than, say, 1MB. Then we automatically break the push into multiple messages as necessary to stay under some per-push configurable limit.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('BK2PmYoYcsHsMocRWkByu', 'Er2XPJ-7DeyGmFpX7Wo0-', 1646074653000.0, '> 
> As for the data loss - this isn''t unexpected given above:
> 
> * If the push fails due to size limits, then the client will reconnect and try to push again, and that push will also fail.
> * If you reload, the client will again try to send the push and it will fail.
> 
> The thing that _is_ unexpected is that the client seems to somehow get _past_ this large push and carry on before reload. We would expect the client to be permanently stuck trying to send this large push until the page is reloaded, but we don''t see that.

@aboodman where do you see the client get past the large push?  In my testing with your aa/stress-test branch, the client is behaving as expected.  After the large push fails and the connection is closed, the client reconnects, and on next mutation (you need a mutation to trigger the push) it will try to do the large push again.  One potential improvement is the client could auto retry the push (with backoffs and some cap on retries).

<img width="870" alt="image" src="https://user-images.githubusercontent.com/19158916/156041572-f0744ade-a345-4e35-8fdd-3630eb2222f0.png">

<img width="870" alt="image" src="https://user-images.githubusercontent.com/19158916/156041694-9f0c8bd4-0201-456c-93c1-d2e7a9d55ed5.png">


', 'grgbkr');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('XpsQcmTaw3_aOdhvB1oJJ', 'Er2XPJ-7DeyGmFpX7Wo0-', 1646077024000.0, 'I''m not sure I saw this on the stress test. I only saw it when drawing with canvas.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('c9NkuBjiMFYknbmN80MOy', 'Er2XPJ-7DeyGmFpX7Wo0-', 1663279494000.0, 'The change to push changes as they occur should fix the issues with a batch > 1MB. We don''t have to catch individual mutations > 1mb for beta.', 'phritz');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('dz5Ud6jB8HUrOHw0A_F8h', 'w9YeAsQ-Yy9RQ3MJizBXs', 1645473382000.0, 'Actually that approach in https://github.com/rocicorp/reps-do/pull/31 is not right, because we don''t have env.DATADOG_API_KEY available at the time we `createWorker`.

', 'grgbkr');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('JyjFGF3WBFM2IFPskaW3k', 't4tXK_VgdyN1C8RlVFImQ', 1663279195000.0, 'We should also measure the perf cost of doing this (e.g., in Replidraw) and consider having it enabled even in release.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('2VOFBUQR_LS_0krWCKD10', 't4tXK_VgdyN1C8RlVFImQ', 1677705089000.0, 'I think this is actually already done but verify and/or do.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('nY92S4Fy3Y64DezPNDMxy', 't4tXK_VgdyN1C8RlVFImQ', 1678358831000.0, 'Related to #216 ', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('L_qdO9pufYxsBxGx1SiEs', 't4tXK_VgdyN1C8RlVFImQ', 1679067974000.0, 'This still doesn''t check the json in release mode. Leaving open.', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('4ctvDC55oNSyY9AdaOL0y', 't4tXK_VgdyN1C8RlVFImQ', 1679431139000.0, 'We decided that we should check these in release mode for now and have an "escape-hatch" that can be used for customers that want things super fast.', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('DOMfiV7iEtdz7brHVTTLt', 't4tXK_VgdyN1C8RlVFImQ', 1680637349000.0, 'Left to do:
* perf test
* escape hatch', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('Greq0fpVsdbiJy2p69N1i', '2qrR0ZzEEJz8yFy5UnUqo', 1644314355000.0, 'See: https://github.com/rocicorp/replidraw-do/blob/main/README.md#how-to-list-the-rooms-for-your-reps-server', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('MAIr0kI9ZNpOohms4eVpQ', '2qrR0ZzEEJz8yFy5UnUqo', 1672783293000.0, 'https://github.com/rocicorp/reflect-todo/blob/main/doc/server-api.md#get-room-records', 'phritz');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('KTJcTdxilb0QPc-NgDfzb', 'BCiVEZ_YISmgEI1cjHifD', 1644304766000.0, 'If this is solved by doing something inside `reps-do` then I could imagine:

* some webpage baked into `reps-do` which is a datagrid
* datagrid backed by Replicache, of course :)
* some mutators baked into the DO like `_reps_editEntry()`', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('N_alDEDpSq3SzYZAMa7xk', 'BCiVEZ_YISmgEI1cjHifD', 1645829018000.0, 'Per discussion it might be nice/easier if we could just start with a way to view and leave the editing until we absolutely need it.', 'phritz');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('KyNv6En93wVRAzRWYEDl5', 'HMBd-5guc9dw6qwe1CaUk', 1645828742000.0, 'Additional notes so we don''t forget: 
- websocket output gate now supposedly works, so we should turn that on as part of this issue. (We should probably have a metric for how much headroom we have in a frame, latency-wise, so we can see when we get close or get behind.)
- I believe we also need to ensure that we are doing our own caching here for cost reasons.
- this batching needs to gracefully accommodate slow up and down websocket connections (maybe this happens automatically if so yay, but it''s an important requirement)', 'phritz');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('mJYP1Z2uZQqMdv8ueHjdd', 'HMBd-5guc9dw6qwe1CaUk', 1663908350000.0, 'see also https://github.com/rocicorp/mono/issues/285', 'phritz');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('NShK_wYZ2jrPgBZAzRohG', 'HMBd-5guc9dw6qwe1CaUk', 1675936152000.0, 'https://github.com/rocicorp/mono/issues/243', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('wC8qncX5vmko35hGhJrv5', 'uW-teUPdWTHn35Tt9DHJ3', 1645472581000.0, 'https://www.notion.so/replicache/WebSocket-Authentication-Authorization-and-Security-bf2b1a2e31844efca73c4aa453fdecdb', 'grgbkr');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('oGkPI-dHGYhlWA5G66rny', 'rLXGEUaPaMah2pqe8BgYG', 1663246427000.0, 'Replaced with rocicorp/mono#290 ', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('6M9J19HoqdJkLQTcBtSsH', 'Dkl2ie-SjYwyN3ma-vGSd', 1644861653000.0, 'If the next mutation queued for a client doesn''t match the next expected mutation the server will iloop (it keeps trying to run the next frame because there is a pending mutation, but then find its can''t make progress. next frame same thing happens).

I guess there are a number of tasks here:

1. Perhaps the initial connection should additionally send its lmid in the querystring (https://github.com/rocicorp/reps-do/blob/main/src/server/connect.ts#L92). If the client already exists but the lmid is not as expected, then the connection is invalid and the client can never make progress. We should send an error message (https://github.com/rocicorp/reps-do/blob/main/src/protocol/down.ts#L10) then close the socket.
2. Later, we can interpret the error message from (1) above similarly to rocicorp/mono#179, that we should nuke client state and start over because this client is toast.
3. Because web sockets are ordered, if (1) prevents initial OOO connection from being made, then it should also be impossible for a push message to be received OOO. In the case we do receive one, I think we should drop the socket and make the client reconnect. Then if the client really is wedged (1) will apply.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('GCoRglTL8j1ViKOA8ovPY', 'Dkl2ie-SjYwyN3ma-vGSd', 1644862147000.0, 'Note it is technically possible for this to happen on production right now since the output gate is not working:

1. Client sends push
2. Server sends poke without waiting for commit
3. Server shuts down uncleanly before commit
4. Client sends next push
5. Server finds that received mutation is from the future', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('J6YweEhdfnjKnbTwieweB', 'Dkl2ie-SjYwyN3ma-vGSd', 1644868132000.0, '>  output gate is not working:

Not following closely but "the server correctly implements the protocol" seems so fundamental that we should probably hack our own output gate until it properly works. I can imagine mysterious behavior and wasted time due to assumed confirmed but actually unconfirmed mutations.', 'phritz');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('0HaUM1DhZHfssaufJRsOs', 'Dkl2ie-SjYwyN3ma-vGSd', 1644873514000.0, 'I am totally in favor of that but I doubt that we have the time before Feb 23 as this would also require implementing batched mutations.

I think it makes sense in any case to put the protection of step 1 above in place since from server''s pov, it needs to protect itself against badly behaved client.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('H9FG_Nw_Wh05I0_o8cpHU', 'Dkl2ie-SjYwyN3ma-vGSd', 1646772373000.0, 'This does not iloop any more but it does raise an exception which is now covered by https://github.com/rocicorp/replicache/issues/335', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('iAwrleL65iSWwOh1FWjfk', '5HahAMOKYo-llCP-RwZR4', 1644348032000.0, 'Why was this closed?', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('AxziCjYUF_O4J8x_vuz41', '5HahAMOKYo-llCP-RwZR4', 1644383901000.0, 'I thought you completed it.  Was just trying to get a handle on whatâ€™s left
for v9
On Tue, Feb 8, 2022 at 12:20 PM Erik Arvidsson ***@***.***>
wrote:

> Why was this closed?
>
> â€”
> Reply to this email directly, view it on GitHub
> <rocicorp/replicache-internal#83>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AESFPBDC46V3LZ5NQUFBFZDU2FUIXANCNFSM5MX47VOQ>
> .
> Triage notifications on the go with GitHub Mobile for iOS
> <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>
> or Android
> <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>.
>
> You are receiving this because you modified the open/close state.Message
> ID: ***@***.***>
>
', 'grgbkr');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('ngqZ6FUQU1ExFya4TSzwp', '5HahAMOKYo-llCP-RwZR4', 1644401426000.0, 'The pr is still not done. The issue with pullVersion/pushVersion is not resolved.

I could just remove that from the doc for now?', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('j8Ryso0jG8Zv3s9RasdQc', 'dJX4mqJ0gbmXUEBcIE2ek', 1641496200000.0, 'I''m surprised, I would have thought that the oldHeads path would have decremented the count and the newHeads path would increment it, leaving it unchanged.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('YbW4ZyYt9rlfVML7oi2iW', 'dJX4mqJ0gbmXUEBcIE2ek', 1641499157000.0, 'Aaron I think you are correct and since we increment first and then decrement this is guaranteed not to recurse a lot and so should be cheap.  But its a coincidence that the code currently increments before decrementing, if they were swapped we would in certain cases do an expensive deep recursion for no reason.

I think its still a good idea to:
1. write a test cases for this
2. add the check to avoid potential future perf regressions', 'grgbkr');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('U_E6MZfVn8j3wzs7S99RF', 'dJX4mqJ0gbmXUEBcIE2ek', 1644788422000.0, 'Agreed.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('hkacwfWBfJpJmJGiuAuSg', '-bJTkZ0z0hRWQc2uFxYqB', 1709599717000.0, 'This is out of date.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('ylwpdWycpLhmW_T3lrI1Y', 'oDBSiF8TQ-xKJoOX9OX5J', 1637270333000.0, 'OK but let''s be sure that it shows up on some benchmark before complexifying the code.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('e9QydiH-XZub8gmE7ivWQ', 'oDBSiF8TQ-xKJoOX9OX5J', 1637270347000.0, '(the allocs in scan could easily be coming from some other random thing)', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('PuHRdfz_QWyMlBM9lrYjh', 'gDHgP0jMeH1b1ZKPjJg87', 1637276364000.0, 'Very ... "interested" ... to see what effect computing a totally different
bundle and putting it in an embedded string would do to bundle size.

Perhaps it would compress well?

On Thu, Nov 18, 2021 at 11:12 AM Erik Arvidsson ***@***.***>
wrote:

> We can/should run the perdag in the worker. That would allow us to use the
> native hash functions (we can precompute the hash of the chunks in the
> persist operation)
>
> According to this SO post
> <https://stackoverflow.com/questions/10343913/how-to-create-a-web-worker-from-a-string>
> you can create a worker from a string but it is not clear what CSP policies
> this runs under.
>
> â€”
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly, view it on GitHub
> <rocicorp/replicache-internal#49>, or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AAATUBBYQ3XKYRTAZPAFHE3UMVTZ5ANCNFSM5IKTQW7A>
> .
> Triage notifications on the go with GitHub Mobile for iOS
> <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>
> or Android
> <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>.
>
>
', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('_Vq3Yn7XXD1_ORgP-GEFH', 'gDHgP0jMeH1b1ZKPjJg87', 1637287202000.0, 'This might be relevant too:

https://github.com/mitschabaude/esbuild-plugin-inline-worker

Or at least be an inspiration', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('7eoKt08zk3gvwn223tyls', 'e7MUe65zxiBtYx9-CM9ms', 1652805278000.0, 'Is the GC perf here regarding the DAG or the JS runtime?', 'ingar');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('Czq-HM3Z3ODyRlCIp3xMM', 'SlE6AfVubu1fihXYt23Gg', 1636137063000.0, 'I think it''s more than that -- basically I think that customers should *always* name Replicache instances with a user id. Otherwise implementing diff correctly becomes more difficult. ', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('hWlWa_o-B9oyzmSP1W4Qx', '3Lqsy1GR2WqtRXZcEYsFL', 1635372877000.0, 'But maybe we need to start looking at the big picture. We want to achieve more stable output. Should we run until things settle down?', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('wZ5Xbt7Od6sXRtJtx7-Vr', '3Lqsy1GR2WqtRXZcEYsFL', 1635430029000.0, 'We could add a maxRuns component or something.

My guess is that the "variance" is a red herring. I bet that if we print
out the 50/75/90/95 percentiles we will see that it is actually pretty
stable now and that there is just 1 or 2 massive outliers (probably cold
start effects).

I think we should stop printing the variance and instead always print the
50/75/90/95 and then see how it is working. The variance is not that useful.

On Wed, Oct 27, 2021 at 12:14 PM Erik Arvidsson ***@***.***>
wrote:

> But maybe we need to start looking at the big picture. We want to achieve
> more stable output. Should we run until things settle down?
>
> â€”
> You are receiving this because you are subscribed to this thread.
> Reply to this email directly, view it on GitHub
> <rocicorp/replicache-internal#51>,
> or unsubscribe
> <https://github.com/notifications/unsubscribe-auth/AAATUBEQTRH3TBLLDGBVQ7TUJB2VPANCNFSM5G3M4VQA>
> .
> Triage notifications on the go with GitHub Mobile for iOS
> <https://apps.apple.com/app/apple-store/id1477376905?ct=notification-email&mt=8&pt=524675>
> or Android
> <https://play.google.com/store/apps/details?id=com.github.android&referrer=utm_campaign%3Dnotification-email%26utm_medium%3Demail%26utm_source%3Dgithub>.
>
>
', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('Kt7BA4rp0lVqhBYb69g5F', 'PQrfPCAXGBKNWgzrtkUSV', 1651317951000.0, 'It is available in chromium browsers too (and Firefox as well) now', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('lCtXGPYiab2iKNF5Bq15v', 'PQrfPCAXGBKNWgzrtkUSV', 1652363597000.0, 'I did a perf test for this:

```
json deep clone x 41.67 ops/sec Â±6.1% (19 runs sampled)
structured clone x 8.07 ops/sec Â±25.3% (7 runs sampled)
```

Closing', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('A1xd_H_k-G2_P2XXrUGsJ', 'bV6F4SQcaGBW8OC2nrA2H', 1632901959000.0, 'Strawperson:

* There is an npm script in the Replicache package that generates a *license key* which encodes:
  - a unique account id
  - one or more host names
* The license key is signed by a private key known only to Rocicorp
* The license key and signature are provided as arguments to Replicache at startup
* Each time Replicache is constructed it validates the provided license key signature using Rocicorp''s public key (embedded into Replicache) and also that the current host matches one of the allowed hosts
* If the license and usage is valid, Replicache pings a central server with:
  - The hash of the license key
  - The account ID from the license key
  - The client ID Replicache is instantiated in
  - (maybe?) the index of the host in the list of allowed hosts
 * The server records the ping associated with the account and client - the hash of the license key and ordinal of the host could be used for reporting UI for users later, though I admit it is kind of limited utility without mapping back to actual content of license
 * If the account is paid, or is free and within the free tier usage limits, then the ping returns OK. Otherwise it returns an error, and the client throws and does not work.
 * To upgrade from a free to paid account, customers email us and say hi, and we collect their credit card info over the phone, then mark their account paid in our server-side db so that pings return OK.
 * Once a month some other process (read: somebody at Rocicorp runs a SQL query) uses this data to charge customers', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('pV1X5BJxCj9312TtuqzTO', 'bV6F4SQcaGBW8OC2nrA2H', 1638165673000.0, 'Note: since this bug was filed the `clientID` in Replicache has become local to a single tab/session. So we need another permanent `profileID` to track unique profiles as opposed to clients.

Maybe it would be wise to report both `profileID` *and* `clientID` so that we could have flexibility on pricing model in the future.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('Wmz1qQgFOTBo-a16WGNAe', 'bV6F4SQcaGBW8OC2nrA2H', 1638233060000.0, '> we do not want it to be possible to accidentally (or maliciously) use someone else''s account id and charge their credit card ... Therefore, accounts should be somehow tied to domains

I do not think we should have this as a requirement, or at least we should not have this as a requirement right now. This feels like one of those requirements we ultimately did away with in DD like "a snapshot should be useful without first having to hit disk": sure, _ideally_ it''s not possible to accidentally charge someone else''s account, but practically speaking it doesn''t seem worth the effort to implement such a mechanism at this point. We don''t have this problem and adding the mechanism now constrains the way Replicache works in ways potentially annoying to customers (they go to use Replicache on some new thing and it doesn''t work because domain).

I do not think we need to do anything about misuse of keys at the moment, but if we must how about instead of _preventing_ the misuse of keys we instead provide tooling that makes it easy to _detect_ and _recover from_ misuse of keys? There are lots of obvious ways we can do this when the time comes and it seems easier/less annoying than prevention. 

I definitely think we should have some lightweight mechanism that makes accidental misuse of licenses less likely, but I do not think it should be a requirement to _prevent_ it, and I definitely do not think Replicache should stop working if you start using it on an unexpected domain.

(I realize there is risk in proposing to eliminate a requirement without proposing something to go in its place... if this turns into a big discussion I''ll just propose the thing I think we should do instead.)', 'phritz');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('Sj4JwaES-rgpAjS2pjS-c', 'bV6F4SQcaGBW8OC2nrA2H', 1638234304000.0, 'BTW regarding a customer cloning our repo and using whatever key happens to be in there, that''s the kind of accidental misuse that I think we should have a lightweight mechanism to catch, and I think that that lightweight mechanism can be a lot less work to implement and a lot less onerous for customers than what was suggested above.

> I definitely think we should have some lightweight mechanism that makes accidental misuse of licenses less likely, but I do not think it should be a requirement to prevent it, and I definitely do not think Replicache should stop working if you start using it on an unexpected domain.

', 'phritz');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('WLEIObjVO0E7BBhB4NOai', 'bV6F4SQcaGBW8OC2nrA2H', 1638234507000.0, 'Hm maybe. Make a counter-proposal?', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('GMfu-FILeURjvc572V3ka', 'bV6F4SQcaGBW8OC2nrA2H', 1638234629000.0, 'I guess I agree it''s not a *requirement* right now. But it seems like it will quickly become an issue especially with people accidentally copying the sample license key. I''m not sure how we could detect / understand that if we can''t tell where the usage is coming from.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('MuRnuCUjjm6GMenyuISHT', 'bV6F4SQcaGBW8OC2nrA2H', 1638234795000.0, '> I think that that lightweight mechanism can be a lot less work to implement and a lot less onerous for customers than what was suggested above.

Interested to hear what you have in mind!', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('05YWD3aYtrGpKbJOFgHI2', 'bV6F4SQcaGBW8OC2nrA2H', 1638235093000.0, '> I guess I agree it''s not a requirement right now. But it seems like it will quickly become an issue especially with people accidentally copying the sample license key

I don''t think we have to treat those two separate problems as one problem. I certainly agree that we should do something right now to prevent people from using the sample license key for something other than tire-kicking. However I don''t think we need to prevent customers from using other customers keys right now. The first problem can be solved in a variety of simple ways that don''t require elaborate mechanisms, for example if the sample key is used we can check in the client if the URL contains "replidraw", and if so then they are probably tire-kicking. If not we tell them to get a key and stop working after a while.

I will make a proposal.', 'phritz');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('JHtugj7TVJHOgTsGksTA0', 'bV6F4SQcaGBW8OC2nrA2H', 1638236035000.0, '> constrains the way Replicache works in ways potentially annoying to customers (they go to use Replicache on some new thing and it doesn''t work because domain).

Unsurprisingly, I''m sympathetic to this bit.

> if the sample key is used we can check in the client if the URL contains "replidraw"

I think there are other variants of this "sample app problem" -- when people create and share tutorials online on their own blogs, what sample key will they use? Maybe the sample keys should only run for a short time or something, like in minutes, then you have to reload the app?', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('IUsZ7zv1arIVsZlzFLdJI', 'bV6F4SQcaGBW8OC2nrA2H', 1638236077000.0, 'Open to ideas like this as long as people don''t accidentally end up using sample keys and people can still create and share tutorials. (as usually there are probably other requirements/desires in my head that we won''t find until we start discussing alternatives).', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('D15O5MtLrUfHiy0LcQ5Rh', 'bV6F4SQcaGBW8OC2nrA2H', 1638518671000.0, 'Proposal: https://docs.google.com/document/d/1MxPhS55ie57TdjSPfrq8B5xQCug9hJW1GhpKH1tD9lA/edit?usp=sharing

Sorry/not sorry it''s in a doc, we can make public or highlight essential elements in the issue if it is of interest outside Rocicorp.', 'phritz');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('pIC8VJBPMxNZWMnoL_D9N', 'bV6F4SQcaGBW8OC2nrA2H', 1690343135000.0, 'We have the "done done" issue, no need for this to exist anymore.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('c5TnKag4Z8CVtqvC8rdNa', 'rBSmSqQqxdpmP2W45ElO_', 1632896558000.0, '@phritz can you please add targets for the two sync items?', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('asFcCSvll0DnfhwDe81iy', 'rBSmSqQqxdpmP2W45ElO_', 1636502435000.0, '> @1gb: 95% Read first 100kb in < 1000ms

A customer points out that this is never going to make sense for users. Never going to want 1gb in Replicache if it has this effect on startup.

I don''t think we actually expect startup to be influenced by cache size, so should we just say:

> @1gb: 95% Read first 100kb in < 100ms

(e.g., we expect startup to be flat past 100MB)', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('L76uI0x53hU6PJ_AI8Keb', 'rBSmSqQqxdpmP2W45ElO_', 1636502441000.0, '@phritz ', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('WRcclbpJ5rCOJttEVLYgg', 'Lfq4TTkY8hot7brCdnYXi', 1634497187000.0, 'We should also document that serialized transactions are required/recommended server-side and justify why.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('rLBpk_EJFgI6rbzV3dNTZ', 'Lfq4TTkY8hot7brCdnYXi', 1690343025000.0, 'This is scattered in a few places like the BYOB guide and the push/pull reference as well as probably the design doc. But it should be consolidated.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('7BfMyvhyTeD8IzQ_hGeIZ', 'qSROcLgTbB8uqruKO2P02', 1690342957000.0, 'There is now class-level docs for these, but the methods are not documented.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('b089eboYut-BRA-0aEMQw', 'wCvNaE0GWyXeRMZXAJY-O', 1690342899000.0, 'This no longer makes sense to do (and is not often requested by users either).', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('M33JKyn9A6hR0ryUZ4Zdu', 'ifRidiHdNCNChjSOrwaHK', 1632726983000.0, 'Also update https://github.com/rocicorp/replicache-sample-chat', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('Vro3CvbAwla03_AE_wDx1', 'ifRidiHdNCNChjSOrwaHK', 1690342830000.0, 'This has been done.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('fC2ItNHKz1uR3L2At4SsT', 'XLvBCpfRGHsNPqIUCAHJM', 1630886399000.0, 'Yup. I was hoping we could only have, `withRead` and `withWrite` (and no `read`, `write` and `release`). I think it is still doable if I refactor transactions to remove some intermediate abstractions.', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('dKA7a9px7teoRdjqDyEX3', '0Gxp-kEk4sZzEmwXJVnZo', 1643321397000.0, 'chai has been released with loupe, you can pick it up after v4.3.5', 'pcorpet');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('XcBBawDtrW1nlfmjyFKp2', '0Gxp-kEk4sZzEmwXJVnZo', 1643322268000.0, 'Nice! Thanks for the heads up.', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('QrNMm2_wIDJEeM8daRK-S', '0Gxp-kEk4sZzEmwXJVnZo', 1643711641000.0, 'We are using @esm-bundle/chai because we run in a real browser. chaijs/chai is broken so either have to update the @esm-bundle/chai or wait for chaijs/chai to fix their esm version', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('RQ-gMOVuAwQ7iSdtXaXJH', '4W7DAfubDy5JIJiMZ2ru6', 1630833229000.0, 'I realized the other day an easy way to do this is to just read all the keys starting with `c/<hash>`. We want all three of em anyway.

Not sure how much a benefit it is but worth trying!', 'aboodman');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('pWAa0ZxRh7XjnxzJndrbT', '4W7DAfubDy5JIJiMZ2ru6', 1630863699000.0, 'But you suggested that we store these in the same chunk in the future?', 'arv');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('ZKiIhtSvH6S_fr_VNltKj', '-UwGISGEIhOk4tbWCxVlj', 1623419851000.0, 'Maybe something to consider, I am storing the access token in localstorage and I just assume it is working. And `getPushAuth/getPullAuth` call my backend to refresh a token. If they are called initially, I would trigger refreshes even in cases where it is not necessary. ', 'KeKs0r');
INSERT INTO "comment" ("id", "issueID", "created", "body", "creatorID") VALUES ('WltVtxPznv6g-YAPnbGOT', '-UwGISGEIhOk4tbWCxVlj', 1646689072000.0, 'Interesting feedback, thanks @KeKs0r. Removing ''fixit'' label pending more user feedback.', 'aboodman');


-- Inserts for label table
INSERT INTO "label" ("id", "name") VALUES ('41_Ez6STrJoJvHpiwOCzr', 'bug');
INSERT INTO "label" ("id", "name") VALUES ('iFrQEZqd8axffbjCvzbiw', 'connectvity');
INSERT INTO "label" ("id", "name") VALUES ('mJ4d0FUoPh2hX4L7uH1Nx', 'DD31');
INSERT INTO "label" ("id", "name") VALUES ('WzGukSdjEtYenrgo458Bt', 'dependencies');
INSERT INTO "label" ("id", "name") VALUES ('u5Ff9Ven5Hq-IoS6r4Xch', 'Design Needed');
INSERT INTO "label" ("id", "name") VALUES ('wlny3bVLzBFrpuzp1JaMi', 'documentation');
INSERT INTO "label" ("id", "name") VALUES ('65Cozmvv75KoThq5pxiUU', 'duplicate');
INSERT INTO "label" ("id", "name") VALUES ('996Ep13MJftE7OSiexIvl', 'enhancement');
INSERT INTO "label" ("id", "name") VALUES ('PpYv_y9MKXrSIVa877PiN', 'fixit');
INSERT INTO "label" ("id", "name") VALUES ('Tv-6vqzIRnHi2OAfI5u0U', 'Future');
INSERT INTO "label" ("id", "name") VALUES ('x0La7RoaLYbrxmpbhns6k', 'GA');
INSERT INTO "label" ("id", "name") VALUES ('WvzDgJ6vaFnzhSM6d05wO', 'good first issue');
INSERT INTO "label" ("id", "name") VALUES ('4MsmDOP39tLVeIjaMlqTK', 'help wanted');
INSERT INTO "label" ("id", "name") VALUES ('Y6VMxlzF-VdL7evLhpP2Q', 'invalid');
INSERT INTO "label" ("id", "name") VALUES ('9Dc2qkLyCFtg1qWdjDTmN', 'logging/error handling');
INSERT INTO "label" ("id", "name") VALUES ('KoXuiMhlH6Ux2GGF0wz4U', 'mirror');
INSERT INTO "label" ("id", "name") VALUES ('I7Z73GJo4RacGY8oHePa2', 'mono');
INSERT INTO "label" ("id", "name") VALUES ('wdSsPcyI5OARaw4y5_aUA', 'Next Release');
INSERT INTO "label" ("id", "name") VALUES ('RxLyrrnhPjRFiXF1Cb1lG', 'p0');
INSERT INTO "label" ("id", "name") VALUES ('hZOmB3YuLQbKhuOD1B8bc', 'p1');
INSERT INTO "label" ("id", "name") VALUES ('mU64F3QI69020ptdAjzn-', 'p3');
INSERT INTO "label" ("id", "name") VALUES ('LrEbHplJ-f5-NDD0aHF84', 'papercut');
INSERT INTO "label" ("id", "name") VALUES ('IhO8zWjbCSvkN95L_Ujdd', 'query planner');
INSERT INTO "label" ("id", "name") VALUES ('KjmdnHPsGzSfGFS9fJbdo', 'question');
INSERT INTO "label" ("id", "name") VALUES ('1aaxi_QdkHMizllO7aXjB', 'reflect');
INSERT INTO "label" ("id", "name") VALUES ('HILlWIbqMi6deSaR6rNhd', 'replicache');
INSERT INTO "label" ("id", "name") VALUES ('N80mmbKfbGTXdkH0Qw7I_', 'user-reported');
INSERT INTO "label" ("id", "name") VALUES ('nDKYBYHy5v-lvmTW_xAAU', 'wontfix');
INSERT INTO "label" ("id", "name") VALUES ('IADBnttporq54wMfKE-Gn', 'zero-beta');
INSERT INTO "label" ("id", "name") VALUES ('6j-BHbfF-zX66UY0zN4At', 'zql');


-- Inserts for issueLabel table
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('Se8C-8RKu-EUMf9DcPjgM', 'HILlWIbqMi6deSaR6rNhd', 'AZKelPxNxWMM8Tes9AUSr');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('9gLZRqyPxGynTm3lbuFkD', 'HILlWIbqMi6deSaR6rNhd', 'QkF2LYzuk8cu9CdFKm2th');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('UoXHXxN0xKEq02ZBspVj3', '41_Ez6STrJoJvHpiwOCzr', '1pMSyLCAy9OEz-khbYcIE');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('zk_TY9zdrAPqqA3mVSB0n', '1aaxi_QdkHMizllO7aXjB', '1pMSyLCAy9OEz-khbYcIE');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('aa7arMaiHVmSqJce0HRdp', '1aaxi_QdkHMizllO7aXjB', '1MHR9pRvoHHlJ9V8k3RXS');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('xeL6pwuM4Cz1n_rjjFVHF', '6j-BHbfF-zX66UY0zN4At', 'ozDbAz2yztkwQFjKtX1YX');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('3FF_oKgeQeIBmTuM0NXh_', '41_Ez6STrJoJvHpiwOCzr', 'EhVaY0SmM1SFxLt43Xq6l');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('PPVRgz3VkErIGfH9r4rJY', '6j-BHbfF-zX66UY0zN4At', 'EhVaY0SmM1SFxLt43Xq6l');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('lxiW-IMe7cIfoJj35R6f8', '41_Ez6STrJoJvHpiwOCzr', 'clYs8ovjAJjlVSuIGKlL3');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('BaadH0jHooNKP924P6fmF', '6j-BHbfF-zX66UY0zN4At', 'qFXKhttg4GG6UDqNs2Enf');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('crYtqlf9cI1ZtpB6Vbjly', '6j-BHbfF-zX66UY0zN4At', '6AfgoH5ICMJhe8g7Q93Nc');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('KyZMqyA0NB3G-rRhOy7MZ', '6j-BHbfF-zX66UY0zN4At', 'zA09ylXAJ5afxFeiwwXWF');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('Ue0Qyd73TxfSZXBFAHELR', '6j-BHbfF-zX66UY0zN4At', 'M3hzZU1ymOTIVJRRrOiep');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('LhWKHK7ViuNcjtss0i8Fs', '6j-BHbfF-zX66UY0zN4At', '5FhK0qX7Gvsh24jGA0EtP');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('qwPHfZ0mDKysSzGtx3Ovy', '6j-BHbfF-zX66UY0zN4At', 'QkIQSSr1UMxdQ1XvCXRLL');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('LBN3SkymFvXlGl5MwK7u9', '1aaxi_QdkHMizllO7aXjB', 'tx8zFWtt1fTUaUXehja7C');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('FIkb7sNUM88eCceRSk3Lt', 'HILlWIbqMi6deSaR6rNhd', 'fXWBarBsdlyZPdGy6jneN');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('O4zb62eOOv_xgyYYkMxPa', '1aaxi_QdkHMizllO7aXjB', '5KQHlQsv-6j1BOXjh80ft');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('4vs46YeblKBAaVT6tj7nO', '1aaxi_QdkHMizllO7aXjB', 'F0B_SJmTcXhGgGY9hZvk0');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('jsFBaGt6AUUf-RkV4U7Do', '1aaxi_QdkHMizllO7aXjB', 'tfrqvYwlGDe4LVR0BvS63');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('Z9GnVmpqI-PNmSCl3QuAg', '1aaxi_QdkHMizllO7aXjB', 'X1SpFrTBwT5wTa4_x6LNL');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('O-_ghJlntYkldGai1jyJk', 'KoXuiMhlH6Ux2GGF0wz4U', 'PlfYFT5Cf3igeiSPgvlQ9');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('bsDLOyQdcd_wOdt_kTPpX', '1aaxi_QdkHMizllO7aXjB', 'F16yIS2tLzwNqDN4Cznok');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('RyHHfaQA9gH_a0W-yfeqA', 'wlny3bVLzBFrpuzp1JaMi', 'ZfwObbfT8wNRzALaTF39a');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('GvToaEWU_Vl3LQ5r_aroZ', '1aaxi_QdkHMizllO7aXjB', 'ZfwObbfT8wNRzALaTF39a');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('cVPDoL_ark4oRT_7eqxld', '41_Ez6STrJoJvHpiwOCzr', 'tCNDwi5uFtMpq0wNi4snY');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('GWsow29XdP4FMSzyB2cH6', '1aaxi_QdkHMizllO7aXjB', 'tCNDwi5uFtMpq0wNi4snY');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('jxFgGC3osBR44YLp1R5U7', 'KoXuiMhlH6Ux2GGF0wz4U', 'e769BGw1cVWLswvZrrLn9');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('XV1W6GBT02dHJqpsO_njl', 'N80mmbKfbGTXdkH0Qw7I_', '62hTFrMd8ZzbjMXxJW9-5');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('3vyR6bFlO4lAkx7D63Qhi', 'u5Ff9Ven5Hq-IoS6r4Xch', '62hTFrMd8ZzbjMXxJW9-5');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('oyz97SvbrOtBXr-k_kO1J', '1aaxi_QdkHMizllO7aXjB', '62hTFrMd8ZzbjMXxJW9-5');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('Wa9KWOe2G1xtHdw_FBB3o', 'N80mmbKfbGTXdkH0Qw7I_', 'Crni_yCBuJnmTXyO0RAgy');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('wtLkoWV5Dk543-_UM0_Td', '1aaxi_QdkHMizllO7aXjB', 'WAwQvGFjwY_dvk4clQ_vN');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('ptYb88yrkEuKQRr1ICZUL', '1aaxi_QdkHMizllO7aXjB', 'R3POoebZhdEd2uoAXrTjP');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('EGX4xbSmsVJnkrO050b9C', '1aaxi_QdkHMizllO7aXjB', 'PlNsdCMV1KeJ6-jm6Os_d');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('0TVDuuQPJd5_cEIzZQRM9', '1aaxi_QdkHMizllO7aXjB', 'T6xu20G4WytR_lhsAqg55');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('z8XZVf2bRp6F8QSnoQMav', '41_Ez6STrJoJvHpiwOCzr', '9GXlK-CM2benNeZJ4T0Y-');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('4MAumKpZJT84eJmxrHdJx', 'N80mmbKfbGTXdkH0Qw7I_', '9GXlK-CM2benNeZJ4T0Y-');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('gOq6LyqoNKiwdSSQ5DIQ5', 'HILlWIbqMi6deSaR6rNhd', '9GXlK-CM2benNeZJ4T0Y-');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('ssPWIOKu0e_X0x865ny2B', '1aaxi_QdkHMizllO7aXjB', 'Hv6m4oCF_ILqiPYWw5cLL');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('3HNB_SnKRdPNwzMn8UkxK', 'HILlWIbqMi6deSaR6rNhd', 'tTqGbFV7s5es4M19NaRn3');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('p3_lrqo0e7O0gDXJni6y-', '1aaxi_QdkHMizllO7aXjB', 'pKyFIZHzJ5xaEmJJpStDa');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('pTOZT2QmS_bxgUfmrH2E6', '1aaxi_QdkHMizllO7aXjB', '9JroQ9_wROsidpiwLa4zv');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('MSKdMHj2gy9SWtaI-uAPg', '1aaxi_QdkHMizllO7aXjB', 'O1U5xVLfsbRBoIziTaqtt');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('LjRSTh3USxmGC5ymG0PbC', 'HILlWIbqMi6deSaR6rNhd', '8MfQ0wLfkAtOot-3yYlcG');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('yyEprU8gHKU3Z8P38T5ev', '1aaxi_QdkHMizllO7aXjB', 'u5grs7H8DScy9xC2mnQNP');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('wKcQXiUr37bI1oHbM1uuM', 'HILlWIbqMi6deSaR6rNhd', '17YvA8-30cH0GAskVj8_o');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('y5OJIWCc12XicbFUyvGb2', '1aaxi_QdkHMizllO7aXjB', 'g-IYpg6Mtpp0Z2H49-AhV');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('ut-M_jkec4MIAQjHN-0wG', '1aaxi_QdkHMizllO7aXjB', 'DgbQiKj8hqPTh15QQK8P_');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('KAyW_QEXIQaJcQDb-mcA0', '1aaxi_QdkHMizllO7aXjB', '4tzIZz16uaB3EHyaL44w6');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('06xRV2o2TQQghOo8nzc5N', 'HILlWIbqMi6deSaR6rNhd', 'yqygBXzMFKPGv1gk9ByR1');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('Vbm-qRZotapAv3pFiiBrq', 'HILlWIbqMi6deSaR6rNhd', 'Pm9ARiE3x3zpUTVSjzciR');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('pZBVae5A6BGDrZiAkFFqJ', '1aaxi_QdkHMizllO7aXjB', 'mGoIMzuUM0tNhoILcqLsx');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('Xm5KYy4btLrS-GjyZJZKz', 'KoXuiMhlH6Ux2GGF0wz4U', 'pv_64OkkrC2V6b_NSiEdn');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('I2sJR5SxYgMN9bQLKGsC6', '996Ep13MJftE7OSiexIvl', 'tAUqy8xX_VBtcdy7eWPAt');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('YsEy0PgUfzMm2gnX_aDPr', 'WvzDgJ6vaFnzhSM6d05wO', 'tAUqy8xX_VBtcdy7eWPAt');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('Z8HZR7REKfI-AnsBAI3UX', '1aaxi_QdkHMizllO7aXjB', 'tAUqy8xX_VBtcdy7eWPAt');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('2KUSApc4jeNFhb-yfCNOy', 'KoXuiMhlH6Ux2GGF0wz4U', 'NLhqu8WtqteqJE1VeTeWv');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('W1qHlIl6LCz599bgRJiAJ', 'WvzDgJ6vaFnzhSM6d05wO', 'sOyiw5y37HpoZt4VhLydX');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('5Xn9x8oMA80IPk9VJWjI0', 'N80mmbKfbGTXdkH0Qw7I_', 'sOyiw5y37HpoZt4VhLydX');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('RIjUAscoJ7ixigtT-10eN', 'HILlWIbqMi6deSaR6rNhd', 'sOyiw5y37HpoZt4VhLydX');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('v9b8D0tkZZbBasDEsJp2K', 'PpYv_y9MKXrSIVa877PiN', 'LVpMPrrTMZ9SQBqf_3Yeu');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('CzzbRWbtIzGXqA8EYrDX6', '1aaxi_QdkHMizllO7aXjB', 'KlOJuUZboYv6Kf42Eusfv');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('l5Wa5UFr-JpY_ngDmweUw', '1aaxi_QdkHMizllO7aXjB', 'fGYBYI7icLmn87kHdScAk');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('TYorFWMkorObK-9fWEwGp', 'PpYv_y9MKXrSIVa877PiN', 'uR4L2c9D7aoo7a2WeclAH');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('smM5SlfLZconKPwdMFsk1', '1aaxi_QdkHMizllO7aXjB', 'uR4L2c9D7aoo7a2WeclAH');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('pbtgA478YI7VYvVP49_AF', '1aaxi_QdkHMizllO7aXjB', '4Q5SGl4eAHqkV2xCIsjmX');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('puydXv0vYANIzIqaRzFpN', '1aaxi_QdkHMizllO7aXjB', '7gVsEKOmCjXwNBaAwmA_c');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('w9QfciRexaGfaheO6uCw3', '1aaxi_QdkHMizllO7aXjB', 'KkwyzF_g9DQ3AEMx501MW');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('ShxtcBfvncUqzJ_-4kLeL', '1aaxi_QdkHMizllO7aXjB', 'AkKLbzDv6fSdL-NJx0eCL');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('QGab0mCEV3q1-DIHkszHy', 'PpYv_y9MKXrSIVa877PiN', 'LAJ4XiFaHxCFur9JKjurC');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('2qVSCmg8bq6NEGJzuTtNG', '1aaxi_QdkHMizllO7aXjB', 'LAJ4XiFaHxCFur9JKjurC');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('JXIoGFFwLEYaH6EWSWwEc', '1aaxi_QdkHMizllO7aXjB', 'aBvSRgSF6g3ppw7fb_1kl');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('La9fq1lYjkoGWEOgKM3Lv', '1aaxi_QdkHMizllO7aXjB', '-pwHLdQfYmU6LhWG0GqBf');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('2bvNcu7XchpZpi2dnifVo', '1aaxi_QdkHMizllO7aXjB', '7oMtDqYIaFq2FauBqU3nR');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('jeT7eqnighDizoqn0DNNz', '1aaxi_QdkHMizllO7aXjB', 'Jg_Lc8p-Qo5DJARqYAZa-');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('EV1E7tPMArYFVINo9BW3v', '1aaxi_QdkHMizllO7aXjB', 'mcyn0Mc1lRIh88TtaUxgA');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('t36lRfrYZIClUfzf851wH', '1aaxi_QdkHMizllO7aXjB', 'STzcA1Q0z31SUm6sYZMWi');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('sxAK-UR6iLZMnCR_Y30gZ', '1aaxi_QdkHMizllO7aXjB', 'K305J2SgfJzfnolGfEEtt');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('qi62H7vBnc1sPtt847BdI', '1aaxi_QdkHMizllO7aXjB', 'rDkzXqfJWPsX2mLK1iDCU');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('sMdIRjVkpQ4un3rz7ZMhe', '1aaxi_QdkHMizllO7aXjB', 'Dj6LEOA9zuXzoV6c9klaa');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('lAAJpJF2HjOdqSh56npr9', 'hZOmB3YuLQbKhuOD1B8bc', '7rq_RVRKkIXMkuWfAUgQo');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('z64ABDZUpjb5GCmjLohRL', '1aaxi_QdkHMizllO7aXjB', '7rq_RVRKkIXMkuWfAUgQo');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('U9Eo-roGbgqrvvIVEQT2b', 'hZOmB3YuLQbKhuOD1B8bc', 'AkjVslOD_m3m-18wS4cuE');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('tML5hTABtS7huzWFaDexT', '1aaxi_QdkHMizllO7aXjB', 'AkjVslOD_m3m-18wS4cuE');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('YMl5_lkwlQaHgqU2S9O14', '1aaxi_QdkHMizllO7aXjB', 'HUUC4Hxvx78QSMFt13C21');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('PldAxQ159N3d8gmTKybFV', '1aaxi_QdkHMizllO7aXjB', 'Gq69iFiLUO9DqZ0LEMug8');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('JdrcHD3ehFSZeh5ZH5Ois', 'LrEbHplJ-f5-NDD0aHF84', 'Gq69iFiLUO9DqZ0LEMug8');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('99ce75ACgULcYulveyAuZ', 'HILlWIbqMi6deSaR6rNhd', '1A3uLjcAPJnZChekrNVXC');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('XY0EQci1L7a_riBA4ZPBw', '1aaxi_QdkHMizllO7aXjB', '1A3uLjcAPJnZChekrNVXC');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('18nLxtFFYI-XAFYb2EtHX', 'LrEbHplJ-f5-NDD0aHF84', '1A3uLjcAPJnZChekrNVXC');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('0y62lVn8SOuGsKl9lsUpm', 'hZOmB3YuLQbKhuOD1B8bc', '_rW2bAe2HIoIcPhGRuyOL');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('IfKgnk1NhGIYFv7hRmwBP', 'HILlWIbqMi6deSaR6rNhd', '_rW2bAe2HIoIcPhGRuyOL');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('JObGXV0XBvAm2RKhZARr3', 'hZOmB3YuLQbKhuOD1B8bc', 'XlJCP3zurxQelxQz0MLr7');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('XliBO92QaRDOI2IjTy1gh', '1aaxi_QdkHMizllO7aXjB', 'XlJCP3zurxQelxQz0MLr7');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('eRL9nqjWU-SycYLS6PvNz', '1aaxi_QdkHMizllO7aXjB', 'UpKiMRI46OhfsrBm1hSXv');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('xm49RYRM32WOlnwJRStFX', 'hZOmB3YuLQbKhuOD1B8bc', 'D-G0Q7myhKbKFr5E5UPDS');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('SheqX7PtRA2VjI01KFHWI', '1aaxi_QdkHMizllO7aXjB', '64kYNhcr9MRaLzLWAWwRY');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('I43H26ssyeY0hOY2DCHmT', 'hZOmB3YuLQbKhuOD1B8bc', 'swvXBTdDXne3CB_d7L1zi');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('nzYIeOdxMcLafEUWcANsU', 'hZOmB3YuLQbKhuOD1B8bc', 'WMdjOTB7dm6bpaN4gJVND');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('mu0loqku1SHPGa4uasTGu', '1aaxi_QdkHMizllO7aXjB', 'WMdjOTB7dm6bpaN4gJVND');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('Osbs3i7RKJH2a4pr5gAN3', '1aaxi_QdkHMizllO7aXjB', 'EAZ1GpjTKqMcMDuvn_Tcb');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('xfzZA9LpNN7XPgNChcs1Q', 'LrEbHplJ-f5-NDD0aHF84', 'EAZ1GpjTKqMcMDuvn_Tcb');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('GVRxo-DBBtj3mrVLUnVJi', '41_Ez6STrJoJvHpiwOCzr', 's3Pnkr55mei8sd-aBlx-u');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('XvbhREnSNAZjI7eyalJYU', 'hZOmB3YuLQbKhuOD1B8bc', 's3Pnkr55mei8sd-aBlx-u');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('0AwazQ5kdwRZQValsSfVa', '1aaxi_QdkHMizllO7aXjB', 'THdq73qLJ7wht0mX6bx33');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('2Mn-xqi2ARgB53zYMyDaZ', '1aaxi_QdkHMizllO7aXjB', 'WsyA82AUCyjc6Ttf8LqmX');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('OGQ3O7nfoprlSCB81uj9A', 'hZOmB3YuLQbKhuOD1B8bc', '9hFg7azUPgM5Zq-H39ygr');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('B5ar05JXZCk3jvbi4aN6B', '1aaxi_QdkHMizllO7aXjB', '9hFg7azUPgM5Zq-H39ygr');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('sse5gDBBKcaXbBxZWF3FK', 'HILlWIbqMi6deSaR6rNhd', 'LW3sBrJffl51JCGIHx5Lj');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('Kk6zH4-ydDQlvKC6tlWI_', '1aaxi_QdkHMizllO7aXjB', 'LW3sBrJffl51JCGIHx5Lj');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('NAfoGfE9cAIRJ645X9loo', '1aaxi_QdkHMizllO7aXjB', 'ojwQdGdrvD6sxgoe0tXWq');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('5DZESb9XaFl5wyHaJsWzR', 'hZOmB3YuLQbKhuOD1B8bc', 'Nt4PKu_sMVV62EWLGLosW');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('lMrLmeZ3Pl6N8sQBMKAqa', 'HILlWIbqMi6deSaR6rNhd', 'lmadQG9qgjCJ-cH0iAUrC');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('D8DbxSVMxs8SFIJYtKzki', '1aaxi_QdkHMizllO7aXjB', 'XeWRpSJn2BkjvCyC6Bd_6');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('ZIjETg5lqg2JzXPijmGGG', 'HILlWIbqMi6deSaR6rNhd', 'gBu2nDQ12xiPyiROFKuTy');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('NwzKjxVj-DC8GOpB660aQ', 'LrEbHplJ-f5-NDD0aHF84', 'gBu2nDQ12xiPyiROFKuTy');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('JXIjaP8BbtLNbhTly1unq', 'HILlWIbqMi6deSaR6rNhd', 'v6dFwAmRkI7Ik95jkz04l');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('SsSe5IfsqfN_bkksumrf-', 'LrEbHplJ-f5-NDD0aHF84', 'v6dFwAmRkI7Ik95jkz04l');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('Fhk6i4c7ECVPk5ono_0YZ', '1aaxi_QdkHMizllO7aXjB', '-Tn95VNrkaxlmduVtuuCD');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('lpgCDCBC8PXuRk_YXK8AA', '1aaxi_QdkHMizllO7aXjB', '-z7a9Y0Lggy_cMzNNEf7m');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('ZsTeqiXA0IA9GUTRPFX_W', 'hZOmB3YuLQbKhuOD1B8bc', 'gPVhbmVjtQnpYc9lhqqOp');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('ShLiLuM0DG5i_L-0W5ZYa', 'HILlWIbqMi6deSaR6rNhd', 'gPVhbmVjtQnpYc9lhqqOp');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('LZpEFiSIMNEf9mp1O_ASG', '1aaxi_QdkHMizllO7aXjB', 'gPVhbmVjtQnpYc9lhqqOp');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('7v_PdUiz2LMbIaVBsYoyi', '1aaxi_QdkHMizllO7aXjB', 'gMuZ1tweNI0m8O0VTFRnz');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('7V3Zsgo3Ozd6e0iW4oOW-', '1aaxi_QdkHMizllO7aXjB', 'MPdM2gW91IhLM9Riata5f');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('0UsdvyQVMZGoRt_KbxHQX', 'HILlWIbqMi6deSaR6rNhd', '5Lzwt-BLI28FPj8fed01J');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('AwGHdF5dg-hh7SRShUC9V', 'I7Z73GJo4RacGY8oHePa2', '5Lzwt-BLI28FPj8fed01J');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('_bbWL8rg1_blGXIuoccKT', 'RxLyrrnhPjRFiXF1Cb1lG', 'oE5HPVV0KG8ZY0j6h8zM7');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('arVHHI8bbBUh6mmbKZPiM', 'HILlWIbqMi6deSaR6rNhd', 'oE5HPVV0KG8ZY0j6h8zM7');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('ewPmhbDPnZtyHNypBIzu3', '1aaxi_QdkHMizllO7aXjB', '7Tefw3JDgEe7D_nmgQ3x4');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('ipPoD_4EJWVsja4sAG7C7', '1aaxi_QdkHMizllO7aXjB', 'sL61Cb893TvSZQ-FNG-B6');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('p0sH2hm8Kvgp1DIfPEoXu', '1aaxi_QdkHMizllO7aXjB', 'Zo7n_XGcUrs5eU9Z_awvz');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('ouegJtrYEM7HJjQ8CoW5p', 'hZOmB3YuLQbKhuOD1B8bc', 'nuPkE2aUsvOlgl3Plc7F3');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('hH6T48L63E8JPK-r6SKsg', '1aaxi_QdkHMizllO7aXjB', 'nuPkE2aUsvOlgl3Plc7F3');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('7n6bTDw4jJY9LjV3YhvdK', 'hZOmB3YuLQbKhuOD1B8bc', 'a2XvjZLnYV40X-tvhkML0');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('-DyhxgnQaVxOyHIfMwkl-', '1aaxi_QdkHMizllO7aXjB', 'a2XvjZLnYV40X-tvhkML0');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('QDctLXqyhlWJUq1j1WKYh', '1aaxi_QdkHMizllO7aXjB', 'o9q8HM5PE0vLGzu6Y05Fc');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('z9AyAx6NHkCFcQCNnZ7i5', 'HILlWIbqMi6deSaR6rNhd', 'MFfdqRANMgtlQa43HXWQJ');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('xHW-s8roDHGStTqNSqQOv', '1aaxi_QdkHMizllO7aXjB', 'QHzFs8PB_x1gcmvQecn8U');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('GlypKxBi5IjnIkgdm8eL4', '1aaxi_QdkHMizllO7aXjB', 'KjrHIoayJ0qvG09u5iDm7');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('c6nPnKlKddPuNwrzWHx8A', 'hZOmB3YuLQbKhuOD1B8bc', 'pq-diA4zFzhkW7VJsn3Qr');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('K8TVsPdM7KjW0cBMfx4dF', 'HILlWIbqMi6deSaR6rNhd', 'pq-diA4zFzhkW7VJsn3Qr');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('uQiyeCrBEOxLqXhoApXts', '1aaxi_QdkHMizllO7aXjB', '0rJxp4rrxgBHvH_hxZjSm');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('960TETJjfzUUSyctvb_Kt', '1aaxi_QdkHMizllO7aXjB', 'niqzkiznUTO33HDXdZ328');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('uK6wudkXqchCgzykBYOCk', 'hZOmB3YuLQbKhuOD1B8bc', '109oqnpl3p17c9QB6qAHA');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('nLJyr1Zp-_nbCsormge1X', 'I7Z73GJo4RacGY8oHePa2', '109oqnpl3p17c9QB6qAHA');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('ht-K_wxlGdqu_x49_Fa-d', 'hZOmB3YuLQbKhuOD1B8bc', 'GnmhxPQj--yAjshNQR5xj');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('OK6zCLsFc6MdqIsrQxi69', '1aaxi_QdkHMizllO7aXjB', 'GnmhxPQj--yAjshNQR5xj');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('nwjosS4f57jq8SAwvqrGi', '1aaxi_QdkHMizllO7aXjB', 'g-7mAvE-qMLJgg-v0ZzQi');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('O7liuUV3WyA3kZ2kcPbOs', '1aaxi_QdkHMizllO7aXjB', '_Wef2qqAq4jCHbnSGRoaP');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('gNqEHDlyFluXps2RBgbh6', 'I7Z73GJo4RacGY8oHePa2', 'w3k2LCZJxewgS1dARkA8Y');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('-oSC6rfM-jDGVpgBfp9UR', 'I7Z73GJo4RacGY8oHePa2', 'uEXZoER5yNM3vQxWIXg-f');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('hNWl2fuVmCMPFTmyLGqFB', 'I7Z73GJo4RacGY8oHePa2', 'xSEkY1awzyhuB-c0Jc0UV');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('ckZ3ONUTomio7yw9ZYfPo', '1aaxi_QdkHMizllO7aXjB', 'xM_4rGRsys3jKT0CiGJCP');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('SRfkW2_ywbChJCj_WopGH', 'hZOmB3YuLQbKhuOD1B8bc', 'zV0bdg5YJbbylZqmUhrQt');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('6ThiIiyqZHqcfPIW16byl', '1aaxi_QdkHMizllO7aXjB', 'zV0bdg5YJbbylZqmUhrQt');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('JvSa3I1u3zui3FUebb2bO', '1aaxi_QdkHMizllO7aXjB', 'BLUsC86cjEIHYkXuiYnTU');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('fRQcG917rX3kbXMonqnOU', '1aaxi_QdkHMizllO7aXjB', 'qQJ73nkTKuavBoXO4aFti');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('Vsb79qm9r_45miQFx3LL8', '1aaxi_QdkHMizllO7aXjB', 'WrDhErVP1Ef_KNtlUSzjb');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('OsU7d3G5xe8B6XMGMBON-', '1aaxi_QdkHMizllO7aXjB', 'bFaOQIA4nFbMiKJdA2T8L');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('ZMcsIVR9RzJftrsllMaTI', '1aaxi_QdkHMizllO7aXjB', 'Gh2cCtydyJTG7-mm6lA2y');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('US4cop1Cs7UUEyf0ZsOyv', '1aaxi_QdkHMizllO7aXjB', 'gEXJOAFBf35Kt0q7RN5Lh');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('xyxozjGqdT9v6MoLtTTiQ', 'hZOmB3YuLQbKhuOD1B8bc', 'Nq0htvSPDitB4OtUjrm_s');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('ReEYV5Zp051zZIxgX4v27', '1aaxi_QdkHMizllO7aXjB', 'Nq0htvSPDitB4OtUjrm_s');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('r5x0zNCeudEf44EvhUffj', '1aaxi_QdkHMizllO7aXjB', 'TT5ktZwwQo-f-IqKKYoWB');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('bRvyCbvX6AG_4NCOCnyEw', '1aaxi_QdkHMizllO7aXjB', 'OjzdiqxGAf2EdgbkenwD7');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('NBPWZpMz7QsnXOO6HUYz1', '1aaxi_QdkHMizllO7aXjB', 'CfcrJOa7X1DqD87cLDpbh');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('1k9x6l9-30Q4fjJqmbjqO', '1aaxi_QdkHMizllO7aXjB', 'B82TlBRphU8lxQeUwf5dO');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('J3ITQIK7i70QpjPFtWezy', '1aaxi_QdkHMizllO7aXjB', 'qUMrP0S5nCH2HxWaEDsqv');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('AsjbH7kF9J7rRgacJ664h', '1aaxi_QdkHMizllO7aXjB', 'vhsfVoQ6fiiFCxLtFZcNB');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('l9LAwH8A5MK-DJ7wrG0pf', 'hZOmB3YuLQbKhuOD1B8bc', 'EvajFts82v8s3vXjLvwmb');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('VwrwYjX2uaGLxCMd0f9dj', '1aaxi_QdkHMizllO7aXjB', 'EvajFts82v8s3vXjLvwmb');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('fOrW5aKKLeALu8y9a3p_Q', '1aaxi_QdkHMizllO7aXjB', '6U76IyqxlPLwVxDA0zNYa');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('syxG_HFtH_pVVx35r86yq', '1aaxi_QdkHMizllO7aXjB', '5Ma2f_7pZwbyZBnjl-ynV');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('m8OngdSFaw4qeG_AnHakt', 'hZOmB3YuLQbKhuOD1B8bc', '37ERG8iH3VUeHDBRRZ0NZ');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('1-FOzR_1ekjI3so3PKE2k', '1aaxi_QdkHMizllO7aXjB', '37ERG8iH3VUeHDBRRZ0NZ');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('yoC2g7WXh13Hb8Uxm6Iij', '1aaxi_QdkHMizllO7aXjB', '8730DfuqU_eRsNeuVF36-');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('RuHGOaOQWVz1wLqPnxy7I', 'HILlWIbqMi6deSaR6rNhd', 'ksNwVR3DysT5WFxCK3WIa');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('-cqdzQJqExeMi0LMNDe-6', 'hZOmB3YuLQbKhuOD1B8bc', 'hjO4PWlvoCCXj_AlS4CNk');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('ruDXLHZ8v4u-Ar25stQ3r', '1aaxi_QdkHMizllO7aXjB', 'hjO4PWlvoCCXj_AlS4CNk');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('N7hyMnPAnbscBDEG33qHP', 'iFrQEZqd8axffbjCvzbiw', 'hjO4PWlvoCCXj_AlS4CNk');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('Dfm89YMk2-UOx-es8VDAG', '1aaxi_QdkHMizllO7aXjB', 'uKLntOMhEHkSnEv0g33OI');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('reWTOh0vuT2-qPY6LmGth', 'N80mmbKfbGTXdkH0Qw7I_', 'V6GY_WByY1kDHoeefuCtK');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('M3sNcj4jxqN_FWqjGGptK', 'HILlWIbqMi6deSaR6rNhd', 'V6GY_WByY1kDHoeefuCtK');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('NfU9BntMG40-AVxao1JJL', 'hZOmB3YuLQbKhuOD1B8bc', 'eYxcTP-MuuJ36qlHHEXbC');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('n51SW2rvoys6QoPBL0Nyx', '1aaxi_QdkHMizllO7aXjB', 'eYxcTP-MuuJ36qlHHEXbC');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('UjuP9vmu-Z77fc13DUju8', 'iFrQEZqd8axffbjCvzbiw', 'eYxcTP-MuuJ36qlHHEXbC');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('ahyEPrt1CpBJGtb5z1aUi', '1aaxi_QdkHMizllO7aXjB', 'xut4J7fPqhioTXsCGnVc8');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('wh8cT39FT_TPSI8w2719b', '9Dc2qkLyCFtg1qWdjDTmN', 'xut4J7fPqhioTXsCGnVc8');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('A0RF6KZD-1cTheG-Cvjzo', '1aaxi_QdkHMizllO7aXjB', 'QJZsGPRqM-9XZYPkJMrkJ');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('34SXoAfPfXvMDv5P2pmEp', '1aaxi_QdkHMizllO7aXjB', 'uKGqplhXFnm_N7UWmi_mL');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('QGEAPuQ36k8DahYHbBX7i', 'iFrQEZqd8axffbjCvzbiw', 'uKGqplhXFnm_N7UWmi_mL');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('bjNxdgqGIattme8S4siBl', '1aaxi_QdkHMizllO7aXjB', 'Rohqczqcb8_uiuhA568rn');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('vu_l3wnXrqFVO0q54ZpAr', 'Tv-6vqzIRnHi2OAfI5u0U', 'pWlTHTkynqjd5gmljVESD');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('N5NaRgJXl4N1B2AP3VJE3', 'HILlWIbqMi6deSaR6rNhd', 'pWlTHTkynqjd5gmljVESD');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('qFvyx_a76KErp33Szu_df', '1aaxi_QdkHMizllO7aXjB', 'GpU_EDkk_C4jBuucWoRYZ');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('Bt5aQNIrhH6L6lDkf6OwF', '9Dc2qkLyCFtg1qWdjDTmN', 'GpU_EDkk_C4jBuucWoRYZ');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('-ulnOe4HCuZK5WzE3vUuR', 'hZOmB3YuLQbKhuOD1B8bc', 'TApJHYop1jupTX3YsgT10');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('GoBTSVdHAMUUQs9a5Q88p', '1aaxi_QdkHMizllO7aXjB', 'TApJHYop1jupTX3YsgT10');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('t8EyhSDxqZ0ZGcGc9ZoH5', '9Dc2qkLyCFtg1qWdjDTmN', 'TApJHYop1jupTX3YsgT10');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('HdASulnJWmR8dSpeGy33G', '1aaxi_QdkHMizllO7aXjB', '2o486QHPAKSkPKEp09j2c');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('Y7nm-z721v3-0vPJYzVkq', 'RxLyrrnhPjRFiXF1Cb1lG', 'xQmT4UvY5YlFGXX5XfKiW');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('XHptAqLdKaxqvgR8jfb_I', 'HILlWIbqMi6deSaR6rNhd', 'xQmT4UvY5YlFGXX5XfKiW');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('KdBeQshUGh9Q3Q_vEWsZC', '1aaxi_QdkHMizllO7aXjB', 'wRqEqQYUCjqI022sBVO8i');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('ZGLzv3YOs3o1uHGL666x1', 'iFrQEZqd8axffbjCvzbiw', 'wRqEqQYUCjqI022sBVO8i');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('CdUrWPEruWGt4_rsh6hKT', 'hZOmB3YuLQbKhuOD1B8bc', 'hHwAQIHg-U3VXQAU7DJIF');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('4aHYSyEXD3v7Dp1fLhn5D', '1aaxi_QdkHMizllO7aXjB', 'hHwAQIHg-U3VXQAU7DJIF');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('zzaQszFwbaP0sypyn1I0k', '1aaxi_QdkHMizllO7aXjB', 'dVgRDv1EGyv1GI5KJSXhe');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('00OO9wduzgTbFGSJ1MiQG', 'iFrQEZqd8axffbjCvzbiw', 'dVgRDv1EGyv1GI5KJSXhe');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('ETu3tC3P2k4BOuswnDTsH', 'hZOmB3YuLQbKhuOD1B8bc', 'KzIYXTz64Zx0_fyMDywos');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('eJ4gU_Zc1zfk36YQUvJXi', '1aaxi_QdkHMizllO7aXjB', 'KzIYXTz64Zx0_fyMDywos');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('ZsxSobWN9_uIWzNs-YjM9', 'iFrQEZqd8axffbjCvzbiw', 'KzIYXTz64Zx0_fyMDywos');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('AmUofI57gZZDevtuRMHMg', '1aaxi_QdkHMizllO7aXjB', 'nlwrOyWJN0vyNMRf86PQ8');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('FS0Wfpzbu_zuDwRN0dyzM', '9Dc2qkLyCFtg1qWdjDTmN', 'nlwrOyWJN0vyNMRf86PQ8');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('Wthus_-778Eh7qGok877H', 'hZOmB3YuLQbKhuOD1B8bc', '-7BhUMh2YaWRHglsa2qVX');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('xb-BaECvAtZ4YxlDL394V', '1aaxi_QdkHMizllO7aXjB', '-7BhUMh2YaWRHglsa2qVX');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('bjzWh7jOUFwrmDWqzOMqn', '9Dc2qkLyCFtg1qWdjDTmN', '-7BhUMh2YaWRHglsa2qVX');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('A7Id9azgUFV8KsJFYuQOM', 'hZOmB3YuLQbKhuOD1B8bc', 'yi2sfNm8hnpVw5QzGLhhR');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('hY3m0gE4orQtmj22bwuEs', '1aaxi_QdkHMizllO7aXjB', 'yi2sfNm8hnpVw5QzGLhhR');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('tMvR4OAB9J_WCDoIHqOpz', '9Dc2qkLyCFtg1qWdjDTmN', 'yi2sfNm8hnpVw5QzGLhhR');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('EXgt-rSlYZ3rWXPb7sHSH', 'hZOmB3YuLQbKhuOD1B8bc', 'VWXO0N4fdf-KhdXcsXD4T');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('Sg4ARAgG2pa0Rs6A20YuG', '1aaxi_QdkHMizllO7aXjB', 'VWXO0N4fdf-KhdXcsXD4T');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('3B1CYnJQd9fA7xNl6Rb9O', '1aaxi_QdkHMizllO7aXjB', 'f32CMpps9G8iPf1V5jL9e');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('vGA4zdCkEr4uS7WhCeqBo', 'HILlWIbqMi6deSaR6rNhd', 'jnvkd4BC3WfzBxCc9yVD2');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('YDbPUUJJ6iolfla05a7YM', '1aaxi_QdkHMizllO7aXjB', 'ABtgmV-PsHQxnlSahn33p');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('NR9nmd5ZNJwCVGZK05pFr', '1aaxi_QdkHMizllO7aXjB', 'Us6h6NpZMZ5xUJtKVOIaY');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('1Q8-SVe_yHjNuivxwjLJi', '1aaxi_QdkHMizllO7aXjB', 'gv4JD5Tag5mehVMUhGQ09');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('RCKEPgQ8xHA6Rnek4j2hf', 'hZOmB3YuLQbKhuOD1B8bc', 'PHPgfv6OYAOSsNEjhl64x');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('nCedrjx-haaDAg0Cy35HA', '1aaxi_QdkHMizllO7aXjB', 'PHPgfv6OYAOSsNEjhl64x');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('OeJ9XTgHOg5aQv6z8xFR6', 'hZOmB3YuLQbKhuOD1B8bc', 'kyIsazaqxZ0MaMc2BPUNb');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('xHpMESsO3_ZbGcWywx1BS', '1aaxi_QdkHMizllO7aXjB', 'kyIsazaqxZ0MaMc2BPUNb');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('FJq_8YP9h8DP38EdedXsS', '9Dc2qkLyCFtg1qWdjDTmN', 'kyIsazaqxZ0MaMc2BPUNb');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('KgIBPVUZGaun2MM7dfBSI', 'iFrQEZqd8axffbjCvzbiw', 'kyIsazaqxZ0MaMc2BPUNb');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('k7GSGx01Q8JaZ4fi1vJyp', '1aaxi_QdkHMizllO7aXjB', 'ZVMaUpEXC2DMc58HGmCmN');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('F-KgJJ1nO5VMXpaW3Bwm-', 'hZOmB3YuLQbKhuOD1B8bc', 'SdR6HejgAoHeOXZDxNk04');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('NU1X-IeCa2we8grRtzk-L', '1aaxi_QdkHMizllO7aXjB', 'SdR6HejgAoHeOXZDxNk04');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('LDyeaY0HTXA04Y2NkYx_7', '9Dc2qkLyCFtg1qWdjDTmN', 'SdR6HejgAoHeOXZDxNk04');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('XaYNYi7KnZXYA-MRlZhBw', 'HILlWIbqMi6deSaR6rNhd', 'NfGzs2Ek979noQug0u6is');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('voRv4g6GqUjxZr9jnMDua', '1aaxi_QdkHMizllO7aXjB', 'D0bqjZ7ARO4z7oKD-XOav');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('XCnrMVwCUyIKQPA3sPFfY', '9Dc2qkLyCFtg1qWdjDTmN', 'D0bqjZ7ARO4z7oKD-XOav');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('wdoTTCZpl-ddZOJjH1i_e', '1aaxi_QdkHMizllO7aXjB', 'nAUaIawWukZ0ILh81OI5o');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('4R0PFIvn2od6dOi_aaAZ2', 'hZOmB3YuLQbKhuOD1B8bc', 'QJX3WHGHFxj4Jd5lMCV_K');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('-hQP0gDoYVDCqqz8YUjzM', '1aaxi_QdkHMizllO7aXjB', 'QJX3WHGHFxj4Jd5lMCV_K');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('NiB_HputV79lozvhjyINL', '1aaxi_QdkHMizllO7aXjB', 'DoPst6sihTnuPFQJyoOQs');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('8eTVfZp7ASEp5oNaCvEuJ', '1aaxi_QdkHMizllO7aXjB', '4YoRIX20muvgX9UTonFNY');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('R6ammIJWKPNhF5LSeKXzT', '1aaxi_QdkHMizllO7aXjB', 't8JkuDLqepHnD-M9px6pc');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('Rgffs5kXzthXRCo3fitPD', '1aaxi_QdkHMizllO7aXjB', 'hslhpa8d1IpVjc22fWGBs');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('YNNHTdigR1_wtncnxaST-', '1aaxi_QdkHMizllO7aXjB', 'ceHAdNti1cEW8D47EDxDm');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('L68dlXPFR6jb_kITJHSW3', '1aaxi_QdkHMizllO7aXjB', 'jKyY9OC7B6RkQ2sJG9Dah');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('zrYo-bAqlylDikmA6w4Vm', '1aaxi_QdkHMizllO7aXjB', 'CHzWF5_HOZ_9Dr1ejtVuJ');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('3GuQ6OfUzADY_DV33OcKD', '1aaxi_QdkHMizllO7aXjB', 'qjL0v-_SW6cpPNEduJnCi');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('m0Efu8YszEzkcUm_Hj16Q', 'mJ4d0FUoPh2hX4L7uH1Nx', '5Efy5Z6Q4sHNkjon62G0r');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('wTdjiEyHFQUzkHA6-q97z', 'HILlWIbqMi6deSaR6rNhd', '5Efy5Z6Q4sHNkjon62G0r');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('iq_wDscqPAcJ9V7woHoE3', 'mJ4d0FUoPh2hX4L7uH1Nx', 'bX_QF_IyWVB96VWPtvuRC');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('psTTNe1gcWtDPtqGBcYUW', 'HILlWIbqMi6deSaR6rNhd', 'bX_QF_IyWVB96VWPtvuRC');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('qKQGYKCETphYYgu_1SFUl', 'HILlWIbqMi6deSaR6rNhd', 'CDe4S8jYbbE_YjPDjCJJg');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('_ZskCPkrWTMRoOIn6kbZG', 'wdSsPcyI5OARaw4y5_aUA', 'P7VQ12P0oM_wetK1eb8yB');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('U6CDIJHrh1DmUdTMrpI-I', 'HILlWIbqMi6deSaR6rNhd', 'P7VQ12P0oM_wetK1eb8yB');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('Qur-H7iUt9XCkDs6ELlu_', 'wdSsPcyI5OARaw4y5_aUA', 'e3LTAd2Ycn8zk4kKceyEf');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('EJPE5bucXvWLH15oPYI4Q', 'mJ4d0FUoPh2hX4L7uH1Nx', 'e3LTAd2Ycn8zk4kKceyEf');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('UKoLe85KytbReRgiDQlgs', 'HILlWIbqMi6deSaR6rNhd', 'e3LTAd2Ycn8zk4kKceyEf');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('veS6E0uU7Qw_nlO7b0hWD', '1aaxi_QdkHMizllO7aXjB', 'EfDktb1UjMH7V8A-uVj9n');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('KodIIAWuxDkdEdrfbv0az', 'Tv-6vqzIRnHi2OAfI5u0U', 'RynRjG25I_BAM3DJOqDmI');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('KjwWovqD3YGO22p37-QdO', 'HILlWIbqMi6deSaR6rNhd', 'RynRjG25I_BAM3DJOqDmI');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('xvIU1Xa8qsfMlLB1veo6Q', 'mJ4d0FUoPh2hX4L7uH1Nx', '3jz_BefDC360O5zZVF1YS');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('3p42iq6JHFBch6LrGinqY', 'HILlWIbqMi6deSaR6rNhd', '3jz_BefDC360O5zZVF1YS');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('HqjLOJuZ5WzsHthKfWOEa', '1aaxi_QdkHMizllO7aXjB', 'VfuNdbCGYyHDLzfNsIqxU');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('l74pBYIRKMx4cpowNYMX0', '1aaxi_QdkHMizllO7aXjB', 'g-ved13slCh2SylOTH-Eh');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('uNgLc-sQp3fKIP0sDwoQJ', 'hZOmB3YuLQbKhuOD1B8bc', '89oMKPoYWax07t24iwPP9');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('tHpjOs5oDzKHs1DW_CXmS', '1aaxi_QdkHMizllO7aXjB', '89oMKPoYWax07t24iwPP9');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('CcrW5tHQsS24ZNJBZyxOQ', '9Dc2qkLyCFtg1qWdjDTmN', '89oMKPoYWax07t24iwPP9');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('LkCcJuad97VKZ2ch0PSRP', 'hZOmB3YuLQbKhuOD1B8bc', 'deF6IymyIRJjUt_G0bKPa');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('lvy1qaevCU7D_qPfrynOx', '1aaxi_QdkHMizllO7aXjB', 'deF6IymyIRJjUt_G0bKPa');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('T7D_I2rj582Vhz7X3w0aO', '9Dc2qkLyCFtg1qWdjDTmN', 'deF6IymyIRJjUt_G0bKPa');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('4jy-0_WOsvd_Yj6J1xcyw', '1aaxi_QdkHMizllO7aXjB', 'sCZFFkpWjtQRxsQiy9jWN');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('oa9JkxTfFOdyrFt0MQjvm', '1aaxi_QdkHMizllO7aXjB', 'ziAKPSegyw9lpjiFq3z3k');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('VB7F1BaFaSftAGdTQG4dj', 'hZOmB3YuLQbKhuOD1B8bc', 'cq_yNS_2PFGbgkMVfUIGj');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('lBck4jQTKM61RaFS3FlMO', '1aaxi_QdkHMizllO7aXjB', 'cq_yNS_2PFGbgkMVfUIGj');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('ujeHm5DOOR4elKSZa9H06', 'iFrQEZqd8axffbjCvzbiw', 'cq_yNS_2PFGbgkMVfUIGj');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('ftnKAtKYZ1cz4QmlwALbJ', '1aaxi_QdkHMizllO7aXjB', '_wqsHhqrLyNzqzeNwBRxj');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('gfixLemiM5Ozmxh4cGNK3', '9Dc2qkLyCFtg1qWdjDTmN', '_wqsHhqrLyNzqzeNwBRxj');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('DAxeW_-uoS94ech9aMr1V', '1aaxi_QdkHMizllO7aXjB', 'A2g5IUtalmSIsZUT4tq5L');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('afB1pci0BqgS0nv9Q3s6U', '9Dc2qkLyCFtg1qWdjDTmN', 'A2g5IUtalmSIsZUT4tq5L');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('KmC8VGXLUf0DT-kNLjJ8f', 'mJ4d0FUoPh2hX4L7uH1Nx', 'wGxWtCO7rh5qTTOZIjV-Y');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('WF33_ufgYCq-A2__Sr5MG', 'HILlWIbqMi6deSaR6rNhd', 'wGxWtCO7rh5qTTOZIjV-Y');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('ji2ffGKEIHrxMo2NpxjeW', 'mJ4d0FUoPh2hX4L7uH1Nx', 'IIJSH7IsERZHmvlPaVATf');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('mve2Trlt4une22aIhz9KO', 'HILlWIbqMi6deSaR6rNhd', 'IIJSH7IsERZHmvlPaVATf');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('3IrgCiWbPH3ptR-cN7PJ6', 'HILlWIbqMi6deSaR6rNhd', 'OxUUoS8GGuW8tN31kfs36');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('DeVIPnBZfVEWbnE_99dN9', 'HILlWIbqMi6deSaR6rNhd', 'tJP-LuUyEizU4KGVEHfu3');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('5NTbPpQyfwdgWLruZMlv2', 'Tv-6vqzIRnHi2OAfI5u0U', 'WivK6UNoZ8wjsnawtbEf6');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('QX42YaMsQWiS3uZzxVFiI', 'HILlWIbqMi6deSaR6rNhd', 'WivK6UNoZ8wjsnawtbEf6');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('LjNER7zm8v-CRD5BoQ2u5', 'Tv-6vqzIRnHi2OAfI5u0U', '3mkBB9ILsSZ5QhzswwmDZ');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('QoZrbenLGRqqwffgY4dpM', 'HILlWIbqMi6deSaR6rNhd', '3mkBB9ILsSZ5QhzswwmDZ');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('kW9AhH4CXRvNQKZACNey7', 'Tv-6vqzIRnHi2OAfI5u0U', 'PT3jXRcu19PYHh-q7hV3_');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('GQDNmzCcr9iM1Lrqs_zsW', 'HILlWIbqMi6deSaR6rNhd', 'PT3jXRcu19PYHh-q7hV3_');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('3dojRy1OOdtbebUikhvyA', 'Tv-6vqzIRnHi2OAfI5u0U', '3tkiPrWToNgD4RhlwrM2D');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('UexnryM03Y5MiLPiyd0Cx', 'HILlWIbqMi6deSaR6rNhd', '3tkiPrWToNgD4RhlwrM2D');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('yVNv-Nj1wncI3DAHkFaKk', 'HILlWIbqMi6deSaR6rNhd', 'UORH6N6zcrn-pQ8nzqpaE');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('gKgR3AUhq9j2oCupeb1sm', 'hZOmB3YuLQbKhuOD1B8bc', '_s7mPxujoqBtybUn78cKJ');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('tNTnXH204bzNFt6ja7pL7', '1aaxi_QdkHMizllO7aXjB', '_s7mPxujoqBtybUn78cKJ');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('niOQQ2LvP2GOU6BzlmVx6', '9Dc2qkLyCFtg1qWdjDTmN', '_s7mPxujoqBtybUn78cKJ');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('CP_IZOl-W-UtuHPJ2VwGq', 'HILlWIbqMi6deSaR6rNhd', '16bjNPfoHawGEaDZRHPwO');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('SM3DQzuHZPIRUdx_7QEXO', '1aaxi_QdkHMizllO7aXjB', '8P8fGLJ2uldgKFT5ERK1R');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('b-MsiULvuRxiQvfa9Ki3t', 'Tv-6vqzIRnHi2OAfI5u0U', 'tgIGh7Lmke7WUoduo-KP8');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('f8shuHnVA2IpJkjr2Rha4', 'HILlWIbqMi6deSaR6rNhd', 'tgIGh7Lmke7WUoduo-KP8');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('LmL6sk1j3b7GgbUCmFdn9', 'HILlWIbqMi6deSaR6rNhd', '6Xj3jAEB5s2jWQnJWq6rK');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('ypdCS1iGmq3iDn4Jx-Wez', 'HILlWIbqMi6deSaR6rNhd', '63NUk-wOlEz2mX3Ba1XjU');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('SlVaqy0xBhU9jBjvqybCQ', 'HILlWIbqMi6deSaR6rNhd', 'ZffAHdq6EUF36pwBdx7VK');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('45A_oSa5CVEDuBlu-QUn9', '1aaxi_QdkHMizllO7aXjB', 'CpwNCPCX7E2ZqQ3FI5ztD');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('BGb1Y-5E8dCBdw2LX-6fJ', 'x0La7RoaLYbrxmpbhns6k', 'CpwNCPCX7E2ZqQ3FI5ztD');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('eOJzkCigfqcsg2OWzxI7B', 'HILlWIbqMi6deSaR6rNhd', '-sGvypX4I-6NgCjMA7w0r');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('230pp73ow1D-m6U8VkQ4L', 'HILlWIbqMi6deSaR6rNhd', 'eI0jMVHQqf6y-vEGTUXSp');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('aM5olTDtze0gmKp_hx6yh', 'wdSsPcyI5OARaw4y5_aUA', 'msfBFWwz1BHa9a4P0wpxj');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('u-UrJqUTPN12DQ3t0aREv', 'HILlWIbqMi6deSaR6rNhd', 'msfBFWwz1BHa9a4P0wpxj');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('RQZECm3pD8-CMul7_vupL', 'RxLyrrnhPjRFiXF1Cb1lG', '-lyB2jG8ruMO0XLDAcM1-');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('e_ApCi9Vlm4_bQ_FB148P', 'HILlWIbqMi6deSaR6rNhd', '-lyB2jG8ruMO0XLDAcM1-');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('nCb1imKemP4hmUhVsXMGJ', 'HILlWIbqMi6deSaR6rNhd', 'BNyyrMVDWQYoraAdNu2bn');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('MpLqx2d3GLgjWdLWtc1qg', 'HILlWIbqMi6deSaR6rNhd', 'nIg73t7LpImlEz-rwv_eW');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('JSU-xID726J2wqgZelA54', 'wlny3bVLzBFrpuzp1JaMi', 'jef7IlxqYTf2FK_GqujAf');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('wJyo4ozVdcQqnW7UXa0ud', 'Tv-6vqzIRnHi2OAfI5u0U', 'jef7IlxqYTf2FK_GqujAf');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('x06yvi5zUz675NQxyhwn0', 'HILlWIbqMi6deSaR6rNhd', 'jef7IlxqYTf2FK_GqujAf');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('CFzrDNAN06XpKvtnN0JYw', '1aaxi_QdkHMizllO7aXjB', 'r-yUB5GoR9btnEGqzNr30');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('M6uVhZmQ-bSuAy3WZjsVk', 'Tv-6vqzIRnHi2OAfI5u0U', 'SHr0OH-dH5OzMQznNglDj');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('Cf2uOpER3Wxo80kYXM0_p', 'HILlWIbqMi6deSaR6rNhd', 'SHr0OH-dH5OzMQznNglDj');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('Lv7mIwzqwYjW0NxG7VQFx', '1aaxi_QdkHMizllO7aXjB', 'paZ3kPMWit7ec3d077oQk');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('PLl3VDym0MZk6pTfe54-S', 'wdSsPcyI5OARaw4y5_aUA', '_vkx4XL8YLrJdj48myGlu');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('iUpx5d2PhhNN-dQ7Vcj9l', 'mJ4d0FUoPh2hX4L7uH1Nx', '_vkx4XL8YLrJdj48myGlu');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('Sum4pYnOgZV-LOs3lcAG7', 'HILlWIbqMi6deSaR6rNhd', '_vkx4XL8YLrJdj48myGlu');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('5plyRNLpfSe4tp5dPWd41', '1aaxi_QdkHMizllO7aXjB', 'QJS5_XjX-Q2gjWnz8zRW7');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('PUJrf7XqNX2KsFfh1mmnV', 'N80mmbKfbGTXdkH0Qw7I_', 'vStALsmM6QE6DhyDiTFte');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('C3nV-WRnkQ7CIh0qcaSgA', 'wdSsPcyI5OARaw4y5_aUA', 'vStALsmM6QE6DhyDiTFte');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('NXtxyTy84fzUunDBwgwnL', 'HILlWIbqMi6deSaR6rNhd', 'vStALsmM6QE6DhyDiTFte');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('g4rPgHGGFBJPuwsjKKv_T', 'Tv-6vqzIRnHi2OAfI5u0U', '45o4ZNvHvQOD-7lnw_2Ud');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('GxAJtHqJdxdXFEADpfYyA', 'HILlWIbqMi6deSaR6rNhd', '45o4ZNvHvQOD-7lnw_2Ud');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('6aS2o8a5cOe8uwhy1XfRs', '996Ep13MJftE7OSiexIvl', '4CL6b_WYsSrA-CsdtAjFQ');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('8ac5qDtEarA9n3bxXpfs5', 'HILlWIbqMi6deSaR6rNhd', '4CL6b_WYsSrA-CsdtAjFQ');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('dBzkrxIOoepdZnFIYXt-L', '1aaxi_QdkHMizllO7aXjB', '4CL6b_WYsSrA-CsdtAjFQ');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('pUE1KVsJkdQjFPlyxSVcc', 'mU64F3QI69020ptdAjzn-', '4CL6b_WYsSrA-CsdtAjFQ');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('QKH6GgWIZM0wnPSWkbdny', '996Ep13MJftE7OSiexIvl', 'VUhmmw-1HmU-09h035uA9');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('TWj8_LJD92xiC-T4c8ScF', 'WvzDgJ6vaFnzhSM6d05wO', 'VUhmmw-1HmU-09h035uA9');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('NaTTT8hEGjV-4TqVHWH6M', 'PpYv_y9MKXrSIVa877PiN', 'VUhmmw-1HmU-09h035uA9');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('pyjLjXdXjrV2_jvdldthr', 'HILlWIbqMi6deSaR6rNhd', 'VUhmmw-1HmU-09h035uA9');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('YyD6xZMtib73dqD5lsiWb', 'wdSsPcyI5OARaw4y5_aUA', 'RT9OCN_PhnDZgZJKK-VSo');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('ItUx8MrssI6OMZrhb35HA', 'mJ4d0FUoPh2hX4L7uH1Nx', 'RT9OCN_PhnDZgZJKK-VSo');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('M5Ql2lmpj3JGKkZ6uJ4lf', 'HILlWIbqMi6deSaR6rNhd', 'RT9OCN_PhnDZgZJKK-VSo');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('o0ujIfAUnUUb3oU3s02Mr', 'N80mmbKfbGTXdkH0Qw7I_', 'Ag5p9iWOp9-DEap2jg6jy');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('Jz3OJucLwcKlI_8RT5ODE', 'Tv-6vqzIRnHi2OAfI5u0U', 'Ag5p9iWOp9-DEap2jg6jy');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('nLCFZ8IanmoZxKjtxVsOc', 'HILlWIbqMi6deSaR6rNhd', 'Ag5p9iWOp9-DEap2jg6jy');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('BlTtwDRSMdpxNtertlJdU', 'N80mmbKfbGTXdkH0Qw7I_', '1Fv8DCZaPgZHVrHVTuK2o');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('IXRGlYcs3_t3DtpkkQQxs', 'mJ4d0FUoPh2hX4L7uH1Nx', '1Fv8DCZaPgZHVrHVTuK2o');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('q8HHzIwJbG4AWGmB6AmQh', 'HILlWIbqMi6deSaR6rNhd', '1Fv8DCZaPgZHVrHVTuK2o');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('qgWmKczvoONwY3LYLabP7', '1aaxi_QdkHMizllO7aXjB', '1Fv8DCZaPgZHVrHVTuK2o');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('mI9UMCsDnIFVeqiqnCRvG', 'LrEbHplJ-f5-NDD0aHF84', '1Fv8DCZaPgZHVrHVTuK2o');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('BQCihrQEEQ09LZDXl1MoB', 'PpYv_y9MKXrSIVa877PiN', 'XUkqCj2VMbXS8qw2jyjw-');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('u7suoW59aI10g5tg-_HCb', 'mJ4d0FUoPh2hX4L7uH1Nx', 'XUkqCj2VMbXS8qw2jyjw-');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('EEx3edFGDHrEmrOwNc-Lv', 'HILlWIbqMi6deSaR6rNhd', 'XUkqCj2VMbXS8qw2jyjw-');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('T5CY2hEM93aB1rWiE35SH', '1aaxi_QdkHMizllO7aXjB', 'J7PndUQpTfoaX6vujTDPM');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('BKhNWhkiZH0K2UL1mNiWW', 'Tv-6vqzIRnHi2OAfI5u0U', 'WOUeYWWHfkscpp3MMjg9q');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('A81ospQnlt8H2rqD2BRae', 'HILlWIbqMi6deSaR6rNhd', 'WOUeYWWHfkscpp3MMjg9q');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('2U6nA2UUoN478r5bETfdt', 'PpYv_y9MKXrSIVa877PiN', '5j4vJjNL8XvXu8XOAgdZ-');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('8YQPsgYUFIHmH8Z7DLWlL', 'Tv-6vqzIRnHi2OAfI5u0U', '5j4vJjNL8XvXu8XOAgdZ-');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('kBjJfUbWH1p5UC4AVXavn', 'HILlWIbqMi6deSaR6rNhd', '5j4vJjNL8XvXu8XOAgdZ-');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('ngPCICfO4d8YHonzJA4qS', 'HILlWIbqMi6deSaR6rNhd', 'F6tcpFPBlYqFuAAGAF_3C');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('oaxYB-cbHRYZ7-6k6tbEA', 'HILlWIbqMi6deSaR6rNhd', 'zvV2RenfHuqN3CiHcz8_u');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('fB73RxlhqN7BiLXVS8T6X', 'Tv-6vqzIRnHi2OAfI5u0U', 'pWMR6ND8jk3rJSnUisV3O');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('Ag3BoAFJ5LmVhgLrOpuJd', 'HILlWIbqMi6deSaR6rNhd', 'pWMR6ND8jk3rJSnUisV3O');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('SYQ2hPEF1D13Rb24-24S_', 'WvzDgJ6vaFnzhSM6d05wO', 'm1apcipgZBQ6MvY0x-kRW');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('qlHvmjhqCXw4vGIFxotSC', 'Tv-6vqzIRnHi2OAfI5u0U', 'm1apcipgZBQ6MvY0x-kRW');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('fe1nbDir1yjH2Ml42nLSm', 'HILlWIbqMi6deSaR6rNhd', 'm1apcipgZBQ6MvY0x-kRW');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('ZYVGviUi02XW7y_fYlxfz', 'HILlWIbqMi6deSaR6rNhd', 'qNDHq-v9DJi9Q8Phkqi7p');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('cPGSLgjZoA8ZAp8hV2J9D', '1aaxi_QdkHMizllO7aXjB', 'CthUwWD206hFESRP8kDf5');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('56glwJeF4m__g0DxBrBO-', 'HILlWIbqMi6deSaR6rNhd', 'MEtP8zRWMlRBupye-z2cU');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('X4Xncnkb35jlEbhzwKJZ_', '1aaxi_QdkHMizllO7aXjB', '562u6FSuRLUhO1WfLCOKU');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('FWCArWYuZa1p86Cq41ktu', '1aaxi_QdkHMizllO7aXjB', '0OoBGWKQ-7QLSmC3AlQpk');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('OIPalU5iEyfmqyN-wJoZf', 'Tv-6vqzIRnHi2OAfI5u0U', 'RIOpt4qqn1wHFAbbVGLfZ');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('C-STrp-rL1D6SYzzg2OsU', 'HILlWIbqMi6deSaR6rNhd', 'RIOpt4qqn1wHFAbbVGLfZ');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('igLwG75TjsAKbN58FShXL', '1aaxi_QdkHMizllO7aXjB', 'UIjwj9vMRKJWAM6JxYKO5');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('q-1wv5cGFTZXaripLSR1H', 'hZOmB3YuLQbKhuOD1B8bc', 'w-m-1MtivCOqjAD1DEVPY');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('tbLBV35auxotSm7KBOltA', '1aaxi_QdkHMizllO7aXjB', 'w-m-1MtivCOqjAD1DEVPY');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('cgh0HFBS0mHfvm5YvNxn1', 'HILlWIbqMi6deSaR6rNhd', '_7MfUNR1OlSWhzplIW2EY');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('v8Zqj6jWAPE-5o9U0uXip', 'Tv-6vqzIRnHi2OAfI5u0U', 'Q3YfcKImxaRAo9CUO8br9');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('EH2sAuHssq0goNPk9yIrM', 'HILlWIbqMi6deSaR6rNhd', 'Q3YfcKImxaRAo9CUO8br9');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('4UjtDLUqGrlq_RwGo7ysl', 'HILlWIbqMi6deSaR6rNhd', 'TCK1ahMSWldhL0R9h9kj7');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('biDkK4Yh-HVH8Jj9wItWR', 'HILlWIbqMi6deSaR6rNhd', 'EbeuhaYtM2Z6OTR2qLYTP');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('M4tHARBMAr-WBOMzCVbJv', '1aaxi_QdkHMizllO7aXjB', 'cUeAIhPNRUHux1RxsHt5f');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('t4zwAllH0qjkf1Cjru3T9', '1aaxi_QdkHMizllO7aXjB', 'KBWekcPn50kLp9V8V3MEo');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('rHAW2kP4Pt3XRRH56hgKN', '1aaxi_QdkHMizllO7aXjB', 'g1J-K1Wj1lyepl3m7KIE0');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('llyw1IL2KmOpQDdK2Qmax', '1aaxi_QdkHMizllO7aXjB', 'gdMJp5JtT1YYdkWKJ-d9D');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('hKtO6j6vdqgctORA5eNWF', '1aaxi_QdkHMizllO7aXjB', 'K7XngRCzZgJJVEZiQ6JX0');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('XxHwK_ZP02FHfhXc7PvhF', '1aaxi_QdkHMizllO7aXjB', 'p5Fif_PwXzinwCvOJucOV');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('5EtwAo0XSyKK_sx33zTYS', '1aaxi_QdkHMizllO7aXjB', '9QHgQYv9VgCJGaikLDNlo');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('w19xFTQApxROYWda2pq5p', '1aaxi_QdkHMizllO7aXjB', 'EOZMl2CohCGenk9DOmvi2');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('1ybUYMPJB4iW-x73ZvNdt', '1aaxi_QdkHMizllO7aXjB', 'vNwsE_q89dNPC9vl3HeZP');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('mM5ISgnLprr-XuQY7Qlpb', '1aaxi_QdkHMizllO7aXjB', 'XVKywqA9fFHLhfQumEKvg');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('49V06c5Yg01pOEpq2xNZC', '1aaxi_QdkHMizllO7aXjB', 'yE6fo6hZkgpqm1O-W_TYq');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('qdZVM2RcD6uxY5R7bzxLd', '1aaxi_QdkHMizllO7aXjB', 'hJLcOo1hA-3z3yKOg0fQR');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('4KTZWfGT2W6ETxzEkUB2U', '1aaxi_QdkHMizllO7aXjB', 'hGFLxlfWmXMgHCyrkUWbq');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('dADNuU8V7Pi3DD5gE6Ikx', '1aaxi_QdkHMizllO7aXjB', '0-Jc6iqbtwPqVIFXmJ3OW');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('ItPEIk6IFuJ_vAW3iDlt8', '1aaxi_QdkHMizllO7aXjB', 'r9Z7YnNl4Yastlbp90SQG');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('GE54q3LHHUw-7kMCcOq-J', 'hZOmB3YuLQbKhuOD1B8bc', 'C2HQKc4ef_wrkOW4gnoqm');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('GZ7rXrtO2FW5AH45vpvHe', '1aaxi_QdkHMizllO7aXjB', 'C2HQKc4ef_wrkOW4gnoqm');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('Eje8tWWfhyisdzfkZgoYq', 'HILlWIbqMi6deSaR6rNhd', 'Wv6ENlaDy7KBftIR2v7yg');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('v0F95Cenf3CAprFUEzY4l', 'HILlWIbqMi6deSaR6rNhd', '6gSYfSTZo0vCn_PD4Q8si');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('JxLXW11Wf8RX90XJj-o_R', 'PpYv_y9MKXrSIVa877PiN', 'ifZtIP406pHKav23mtYmL');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('tKO2-buFefynKK_Xtpx4f', 'HILlWIbqMi6deSaR6rNhd', 'ifZtIP406pHKav23mtYmL');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('sqKmKDfQM_4iJvHWf2i6o', 'wdSsPcyI5OARaw4y5_aUA', '3aPdgUKvOirJqqDGWZCpk');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('nL1Nv-Zuwj4EVGHhChSQT', 'HILlWIbqMi6deSaR6rNhd', '3aPdgUKvOirJqqDGWZCpk');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('jLet-FRe68ivvnQe4vQOk', '1aaxi_QdkHMizllO7aXjB', 'SOlle2-ebTy4xp3_1OMwn');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('wolBhhE_0js2bn1mmYfjt', 'nDKYBYHy5v-lvmTW_xAAU', 'iiz4kdlkl-0uxT-tWoOPF');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('5lyLMmznXei-PpTO4P3dQ', 'HILlWIbqMi6deSaR6rNhd', 'iiz4kdlkl-0uxT-tWoOPF');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('LR3QanTLjByTtXk8AzGhT', '1aaxi_QdkHMizllO7aXjB', 'CuPT9CKpVUFRsq07M51wn');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('XZk5KehL8DpmvQk0ayMnb', 'HILlWIbqMi6deSaR6rNhd', 'Wgp3eU5OrhcKtJ0LrY4fm');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('-gmPZ7Lza_io5VzsDQhc_', 'HILlWIbqMi6deSaR6rNhd', 'sOcAm32sRs1idyG7L4qHn');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('QPfZKSRNzquttJ48NonU-', 'HILlWIbqMi6deSaR6rNhd', 'pvgwRLWBQcmtbXQydIcUl');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('3-RCwMRA31p2hlP-FpP6w', 'Tv-6vqzIRnHi2OAfI5u0U', 'vaga-cTK9yTB98XShYA5x');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('rYtTtw8vlF0Ry1Hcsl2xz', 'HILlWIbqMi6deSaR6rNhd', 'vaga-cTK9yTB98XShYA5x');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('bkJdsmp8WK1Th3qBMsHts', 'HILlWIbqMi6deSaR6rNhd', 'TpxU6KF2rQJcgQpTd2SDT');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('CYOFe8qaXf8aua_ghBtAM', 'HILlWIbqMi6deSaR6rNhd', 'F4sB9lLVF5Wpk6Pd1yR09');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('abOtd1vOju3KE_KsdUipp', 'Tv-6vqzIRnHi2OAfI5u0U', 'GojkNTEGE0yuF5O5rx7WU');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('tuzLPbVIJZhZDjnC6L6H2', 'HILlWIbqMi6deSaR6rNhd', 'GojkNTEGE0yuF5O5rx7WU');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('OJQXb33lCjY-H-ZqCAVW0', 'Tv-6vqzIRnHi2OAfI5u0U', 'KpVnCc00295evmfce4M-i');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('nTZWoMJtqN5wchHgyJ83C', 'HILlWIbqMi6deSaR6rNhd', 'KpVnCc00295evmfce4M-i');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('XrtfrmxM8cl5rrmIo06Yy', 'HILlWIbqMi6deSaR6rNhd', 'g5yqTgtmjJNBfB8UuT3wZ');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('Zjb98PIiDNxHhzh3bpqAY', 'wdSsPcyI5OARaw4y5_aUA', 'w11aPgBD5OzWd_wfpEYax');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('ABIT-7vjfure3FW5p4Hin', 'HILlWIbqMi6deSaR6rNhd', 'w11aPgBD5OzWd_wfpEYax');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('mf-0pKWDTHCozJ5WAKMVQ', 'HILlWIbqMi6deSaR6rNhd', 'hah-xpgyGUYzoFWzjU9f1');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('GdDyLQLAhvAHb9v7xw0bj', 'HILlWIbqMi6deSaR6rNhd', 'Zyp6cKng_eCdyX2pZi4IM');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('2QXsVmFszJNfEsg4JxwO6', 'HILlWIbqMi6deSaR6rNhd', 'a2QYIcY8XOP56qUNkoLcr');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('jDqzqqjCUsML747KKw92-', 'HILlWIbqMi6deSaR6rNhd', 'NhOEDbtZQav19y4Cestl2');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('oqt7OSE3Yltppxw5GbF-h', 'wdSsPcyI5OARaw4y5_aUA', '7t6SB5rCJST-6CgnCuwnX');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('e7Z5Bvns6u_VJqHpA6Viv', 'mJ4d0FUoPh2hX4L7uH1Nx', '7t6SB5rCJST-6CgnCuwnX');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('tfITIqEBjs7ruFxK5746w', 'HILlWIbqMi6deSaR6rNhd', '7t6SB5rCJST-6CgnCuwnX');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('tit9505Djc-hW3WEs93YC', 'HILlWIbqMi6deSaR6rNhd', 'p2DS_Ww4_P7n5OlZlTPY3');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('3WrjVl-FI7Bhjzww9Odf0', 'N80mmbKfbGTXdkH0Qw7I_', 'c0121ZvhhZvz7do4IEUWl');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('0vM_Tu8_SRPyqfL9deH7x', 'HILlWIbqMi6deSaR6rNhd', 'c0121ZvhhZvz7do4IEUWl');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('Dnug2b7yBFyj_LY577oGo', 'wdSsPcyI5OARaw4y5_aUA', 'gsbadTprMp4-PWu6YEJzx');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('TnlJyqs1zL6-Li6M3Xd7o', 'HILlWIbqMi6deSaR6rNhd', 'gsbadTprMp4-PWu6YEJzx');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('WkcbtXeOc3Bda4R9N7H1w', 'HILlWIbqMi6deSaR6rNhd', 'LR3JYhtUJQa1m9H8d_axg');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('7PuLI06S2yCcXZb2v50Jx', 'HILlWIbqMi6deSaR6rNhd', '_7mrJDQchBc4gXQsuwJji');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('tmY88Xns8WpeVQ9g1N5Yy', 'HILlWIbqMi6deSaR6rNhd', 'nZkhkY6yBwVSMHcFQ5ovP');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('Kk29sedymh1BNYmpiBZBY', 'HILlWIbqMi6deSaR6rNhd', 'cxWMStEbXVwNwPSYiyRxO');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('HuvB6JjvOki1zgQwwyZy0', 'HILlWIbqMi6deSaR6rNhd', 'akrNUuHFpt68Mwmqz1_-9');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('JCYoVpAzWRRDDrST_6IlD', '1aaxi_QdkHMizllO7aXjB', '5gsbQgpFjeyn3M8Ylogf-');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('II-9XW8BSMhKBFfwjex7T', '1aaxi_QdkHMizllO7aXjB', 'gPhqCx-aYMqYuEYjVCm3J');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('pWLRJhKwgmCbo221eOMaO', '1aaxi_QdkHMizllO7aXjB', '2wGcLhvqKsJE4NhB_rKg2');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('vBpfxZYT1bnvmpzK3LXX6', '1aaxi_QdkHMizllO7aXjB', 'JQtzzPWBxB1AeLUIV5vJs');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('MRHtt7slCMDYFl0CRt-A2', 'HILlWIbqMi6deSaR6rNhd', 'faKsp9DnD2o2RSWiwp7zm');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('gsjJ-GNAIFDHjRwsi16ES', '1aaxi_QdkHMizllO7aXjB', 'Wplln6wUPMg-UpncZinnK');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('AV2HrEyPuM7SvVyZ_kooE', '1aaxi_QdkHMizllO7aXjB', 'U-ms75ISN0eWFMsfSPH7W');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('lbtNOr0ZLKZVKc4AlhfnV', 'PpYv_y9MKXrSIVa877PiN', 'qKdxI5MsM8FzdWz7jCd1x');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('9SLP1ExiCcq_U2tBEtqFG', 'HILlWIbqMi6deSaR6rNhd', 'qKdxI5MsM8FzdWz7jCd1x');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('n5Jx4w5kOyP5BjUrpDWTq', 'PpYv_y9MKXrSIVa877PiN', 'S3CqyrqHSO6hfzNZN-VBF');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('ZTh-9CrXqtTH3TuJ6ghCp', 'HILlWIbqMi6deSaR6rNhd', 'S3CqyrqHSO6hfzNZN-VBF');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('hcfDRlIqRHOg-4GoZl7VI', 'HILlWIbqMi6deSaR6rNhd', 'nUhUqD_bTwZiab-oN74OO');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('-MbSCTHJ2AYqylTxHKd2A', 'wlny3bVLzBFrpuzp1JaMi', '850WT8f7wSQtZ2Wmjv17n');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('8g49hfb7mYDQJZQTaiH5z', 'HILlWIbqMi6deSaR6rNhd', '850WT8f7wSQtZ2Wmjv17n');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('s0-fq3mmW0UFZYotgpZvR', 'wlny3bVLzBFrpuzp1JaMi', 'K7GhpXmlZM-bMHbJf02Mk');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('IHIYuzGV0ahaxnyGCjloR', 'HILlWIbqMi6deSaR6rNhd', 'K7GhpXmlZM-bMHbJf02Mk');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('sA3NJXnXWCpYIA04ar2s8', '1aaxi_QdkHMizllO7aXjB', 'kxmFJ2Dkl_jaW4tyDUq3u');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('F5GnH5mdrpPu7ncIyogKG', 'PpYv_y9MKXrSIVa877PiN', '1pMwgmRZq30yxR8yWenLd');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('AZcXv9iRhZExe0hvq_Iun', 'N80mmbKfbGTXdkH0Qw7I_', '1pMwgmRZq30yxR8yWenLd');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('jwhS8CmolWq0oB0mPKhad', 'wdSsPcyI5OARaw4y5_aUA', '1pMwgmRZq30yxR8yWenLd');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('YNxxxgZX-XoKc3ABp7d1G', 'mJ4d0FUoPh2hX4L7uH1Nx', '1pMwgmRZq30yxR8yWenLd');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('mCskWqFzvJ-LGp07Jx6_i', 'HILlWIbqMi6deSaR6rNhd', '1pMwgmRZq30yxR8yWenLd');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('lQG0sSuJM-C0AErk-NWXq', 'N80mmbKfbGTXdkH0Qw7I_', 'LHh6KGh3-JhVj2samTs8-');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('Lc8-RH-Cz12rPkoyXi2PV', 'HILlWIbqMi6deSaR6rNhd', 'LHh6KGh3-JhVj2samTs8-');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('Osqp9EQpU0AE4QYUXAS3m', 'PpYv_y9MKXrSIVa877PiN', '9ngkocA1DYbF7C7291-jL');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('bGGaxbuI7-w_1VeVsS8LT', 'N80mmbKfbGTXdkH0Qw7I_', '9ngkocA1DYbF7C7291-jL');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('8_2zAJAZXfsW4Mc6EZZQ1', 'HILlWIbqMi6deSaR6rNhd', '9ngkocA1DYbF7C7291-jL');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('jD6QTIUPTIAXTaRUX1V16', 'PpYv_y9MKXrSIVa877PiN', 'SlToTgycCsnO07UfH4mBp');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('pwS0GJpY9fF4oQbgkPgGx', 'N80mmbKfbGTXdkH0Qw7I_', 'SlToTgycCsnO07UfH4mBp');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('VkC4TSGVlVWmiZyG19wVW', 'HILlWIbqMi6deSaR6rNhd', 'SlToTgycCsnO07UfH4mBp');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('pnTlz-jBliiMF20EZQDxL', 'WvzDgJ6vaFnzhSM6d05wO', '8q7Jjk_fvu1j3eSi00aD7');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('JfYprYPqeg9TN1fVd6BNx', 'PpYv_y9MKXrSIVa877PiN', '8q7Jjk_fvu1j3eSi00aD7');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('xZVKLweRNvwnxN4cFyrnL', 'HILlWIbqMi6deSaR6rNhd', '8q7Jjk_fvu1j3eSi00aD7');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('0fy-cZI-0m_6bW-RkH_SP', 'HILlWIbqMi6deSaR6rNhd', '6eZ-NkNCr5Kg2hWBD3W5K');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('aEJKdE-HlRay2paZDuM6r', '1aaxi_QdkHMizllO7aXjB', 'PPViYslTtb0Bq1KHyMbJo');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('Sory-gvwdrztGJsYYdkCr', 'HILlWIbqMi6deSaR6rNhd', 'RSiJp12d7Cp0_-sbdh3xx');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('sFJ5cuidqasU5u6lvqCeQ', 'HILlWIbqMi6deSaR6rNhd', 'Fj7117VgpCh38LCESNK0O');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('iaKNdb2dz0VTkgHJLyL3F', 'HILlWIbqMi6deSaR6rNhd', '1WwWxmJrHtPBz49v4K9sl');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('RZ9mYPZB1aTSnvuQMD1WR', 'HILlWIbqMi6deSaR6rNhd', 'B1bTC9qD6ERL5E0wh4y_a');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('RLAnXS-dsCrBkr2yPZT5f', 'HILlWIbqMi6deSaR6rNhd', 'ZBfUbuN4fbheJpOEaVNbI');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('qcBJLlYnIRcC9xYfYbOWf', 'HILlWIbqMi6deSaR6rNhd', 'aSt6EUTfiaiflXux5-ooF');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('NHLFZlQlaGrdU-CAEjMEB', 'HILlWIbqMi6deSaR6rNhd', 'RzLiAkwI-mbB_IYlWSqDu');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('h1aEVfFQQd4WZNRWfW089', 'HILlWIbqMi6deSaR6rNhd', 'jxq8YIb0gEwLTUWn1XfbW');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('NnviMQrAESfDVeeegw9gf', 'HILlWIbqMi6deSaR6rNhd', 'vY4VXqGpUGWhhjEJtMsZH');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('U2AI6tXR_8cF1YmgbqQrM', 'HILlWIbqMi6deSaR6rNhd', '6bnD-KvMSKUP59KqjkxZs');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('Z6QhI7J2qecTOYqq-Q84q', 'WvzDgJ6vaFnzhSM6d05wO', '_CnvrgvrxbYYRKbaSxKYP');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('Ob646hbhuzccttVFJguR7', 'PpYv_y9MKXrSIVa877PiN', '_CnvrgvrxbYYRKbaSxKYP');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('HVqx8z6UXzXn0MdvDKQ8k', 'HILlWIbqMi6deSaR6rNhd', '_CnvrgvrxbYYRKbaSxKYP');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('BsRhGABE6bab2nPjGkYV9', 'PpYv_y9MKXrSIVa877PiN', 'YRgQx77UCybmonAQy-D5L');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('EgozgSPlW5GQzOO522cES', '1aaxi_QdkHMizllO7aXjB', 'YRgQx77UCybmonAQy-D5L');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('xfJcnm0uAJ6AUJR16O-eH', '1aaxi_QdkHMizllO7aXjB', 'c0unLajkmF9jsPqCAsyGr');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('3Qxy6QQu0h8FssckqllZP', '1aaxi_QdkHMizllO7aXjB', 'JneOa0-sxjHCP_CWn9MKx');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('JOnO8fv0p9W8qIU5Mehrl', '1aaxi_QdkHMizllO7aXjB', 'YEyaRdVWx7Ho0FEA42R1_');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('JUTgWboWz_O-goBkdQCbv', 'hZOmB3YuLQbKhuOD1B8bc', 'RNWQCv_4hbaVYMIRSFVjo');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('PgQYgMqrqD1z-cpSywrnC', '1aaxi_QdkHMizllO7aXjB', 'RNWQCv_4hbaVYMIRSFVjo');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('jJllVdr-Fys6ENMido0MM', '9Dc2qkLyCFtg1qWdjDTmN', 'RNWQCv_4hbaVYMIRSFVjo');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('YrBh1FsWnuvYHGRGZ2yYx', 'hZOmB3YuLQbKhuOD1B8bc', 'pm8HuooXYxbgDWDJK4pwG');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('hwmhWP-covXB6pSFOUcdA', '1aaxi_QdkHMizllO7aXjB', 'pm8HuooXYxbgDWDJK4pwG');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('h47Slqjrdehva4posgIBF', '9Dc2qkLyCFtg1qWdjDTmN', 'pm8HuooXYxbgDWDJK4pwG');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('Zt5jlluVdHOHoNoS4hO-g', '1aaxi_QdkHMizllO7aXjB', 'WvmdaY5OczICNTBRd4xPe');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('oZJqhh7FKLlgWq-xQ_Upq', 'hZOmB3YuLQbKhuOD1B8bc', '1h3HiF7eonal9C7FKRR3o');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('KnfFN3amGKcVqikUYb_Vs', '1aaxi_QdkHMizllO7aXjB', '1h3HiF7eonal9C7FKRR3o');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('AbbsG5OCNvXWkgMge5gXV', '9Dc2qkLyCFtg1qWdjDTmN', '1h3HiF7eonal9C7FKRR3o');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('s_G9fL0kyobX6aEYkvHk-', '1aaxi_QdkHMizllO7aXjB', '11ifzmeKegTKSINXyrdE5');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('tJ0dXcN8enC8kXxlVxSs3', '1aaxi_QdkHMizllO7aXjB', 'XgIcGIjAf8FTov8V44Ycs');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('LCaHQPKjewRnGdXBD6qoY', '1aaxi_QdkHMizllO7aXjB', 'K1h3em6-Yvc0lz-8TvEvB');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('QP1N_cABRqTenbnGgrV1e', 'HILlWIbqMi6deSaR6rNhd', 'g0a20gZfDDsfSAWbWoxpe');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('dIu2FTXbh-L8UxfTOebi1', '1aaxi_QdkHMizllO7aXjB', 'L5yNDvtoZEn4Op6MRvGnb');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('h04zLzJzwa94B7gDgfns0', 'HILlWIbqMi6deSaR6rNhd', 'PwnCrgRHPjyUQC4hYvO6J');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('bONTFEuRnwp1CRWz2Z0b4', '1aaxi_QdkHMizllO7aXjB', 'jHMfshAirFdTUkTyPfdTH');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('gLu1Q2J9tfAhIG4j0ousz', 'HILlWIbqMi6deSaR6rNhd', 'Us0-8EbmbQBIYvkDkvQLl');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('uSU4_rwPsY93a789Roo4w', '1aaxi_QdkHMizllO7aXjB', '8pE64EJT-yjuNPfrckhhl');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('rTDSyiItZSFC1YH-T6jub', 'PpYv_y9MKXrSIVa877PiN', 'cOnsigzGN2MaN6wZhlpxs');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('aPR5Crma_KHXfIXi1j87Q', '1aaxi_QdkHMizllO7aXjB', 'cOnsigzGN2MaN6wZhlpxs');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('kKpTJYBYpMtvnssHmems6', '1aaxi_QdkHMizllO7aXjB', '3GlW0ZTQFt_dSJ1jVJhXD');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('JzlYwkhWKv_FZebEPKCU2', '1aaxi_QdkHMizllO7aXjB', 'arDVoFVGcnhEaAxICvuSc');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('ZX_PpAoQDYSn-z1YLaw8t', '1aaxi_QdkHMizllO7aXjB', 'xJIY0eiS82XcikHemKc5g');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('Fk8XIf3wld8ZpHHcqIudw', '1aaxi_QdkHMizllO7aXjB', 'dUwt5mhJVsM0_6dnLIHOe');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('9sAAIe7WhXQfnSReC2JNd', '1aaxi_QdkHMizllO7aXjB', 'fgZfIqf2NKYqI80U1YeAY');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('BerMiMej1_V4MfmmSeTgX', '1aaxi_QdkHMizllO7aXjB', 'hlhhXfhnNVXufEtCjccXA');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('tjiy79JElkTzyoUKaepYE', '1aaxi_QdkHMizllO7aXjB', 'GMRD7IaJH_4V0DRLP6hsE');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('A8Wp5PyXN4CiX66SxQ39V', '1aaxi_QdkHMizllO7aXjB', 'W5zvTqsrw-A_hT6R6Y2uI');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('dIfMydf5lYd0MKTHtD6tZ', '1aaxi_QdkHMizllO7aXjB', 'oB7iJ_cl641lfosZdXvPQ');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('QHcJEkbpagGWUk3PrigzS', 'hZOmB3YuLQbKhuOD1B8bc', 'PE4kaK4nCKkHwH24WZopE');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('xmgETp5zu2VSR5UOjYddA', '1aaxi_QdkHMizllO7aXjB', 'PE4kaK4nCKkHwH24WZopE');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('-E-cOGX617L98MLzqYudO', '1aaxi_QdkHMizllO7aXjB', '-cYdezEfY1tZU2II3tN8M');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('qrePn1vJQHh-3p3T5T7nk', '9Dc2qkLyCFtg1qWdjDTmN', '-cYdezEfY1tZU2II3tN8M');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('nqjiYz7exfyOuuosWtWXh', '1aaxi_QdkHMizllO7aXjB', 'OeqGbyKoZyURoaRcUCYQJ');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('e2Y-XiklBA33E-yMMlCJw', '1aaxi_QdkHMizllO7aXjB', 'tYL-OzLT2Nbd3wup_H5Hl');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('1zXKLWCdUKH3jp6PZGAiH', '1aaxi_QdkHMizllO7aXjB', '5AH5f4_WUUkjUBv1p08yS');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('kLB6jKBq0kqktLMAwHV03', '1aaxi_QdkHMizllO7aXjB', 'wnEQdT-7FW2WRTImdrBly');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('wgU-q43yp7WLaQdUQqCJx', '1aaxi_QdkHMizllO7aXjB', 'cIU1_Ipxb_LvoWwjS7Eni');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('R0NWJM9YS4DpBicaCIZFD', '1aaxi_QdkHMizllO7aXjB', 'Qbc1-oTPHc1ZBHdmYunQA');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('JLpXNxQowdVl7sUavXnwK', '1aaxi_QdkHMizllO7aXjB', '0sQUzcq3Xlc9ZhjX4_3av');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('90EeJHfvpZXBbmCND5Ca1', 'HILlWIbqMi6deSaR6rNhd', '-R3lUiMlJGZOkojj3ifDf');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('PKZ2IMn5ZixsQ4973wKuZ', '1aaxi_QdkHMizllO7aXjB', 'nqDMl_RchEnwVwJ2iqLWx');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('zhKTa2pgVv2vaWvCPagUN', 'N80mmbKfbGTXdkH0Qw7I_', 'VhjxLu-RwPAKzUs6eSvb5');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('CIL92bUao5tgv95IF1OAw', '1aaxi_QdkHMizllO7aXjB', 'VhjxLu-RwPAKzUs6eSvb5');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('dqvUhKMgTX81-xmicwBVg', '1aaxi_QdkHMizllO7aXjB', 'oFLCLE8py_AqJigfqVopj');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('lbM4HsrcsbFjPkRcTTlMX', '1aaxi_QdkHMizllO7aXjB', 'g3ldLQv0tRPXdIxcjW7nW');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('-tbwES0txv_LnxA6aEgDS', '1aaxi_QdkHMizllO7aXjB', 'd3TfT8wEJC0qqy4nnwbYw');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('eWjA1xNP3SB65pMDVOUle', '1aaxi_QdkHMizllO7aXjB', '5Gws1JQqvka70AE11H8ov');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('-oQC1UNq6tr3-Iq7NEVTl', 'N80mmbKfbGTXdkH0Qw7I_', 'Er2XPJ-7DeyGmFpX7Wo0-');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('GTHwF-Vtc5rf1FuBhRJhM', '1aaxi_QdkHMizllO7aXjB', 'Er2XPJ-7DeyGmFpX7Wo0-');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('2kGsIsHdqL74NjoMnCVWL', '1aaxi_QdkHMizllO7aXjB', 'w9YeAsQ-Yy9RQ3MJizBXs');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('M2FHISxoO5kXig1iYlmx0', 'hZOmB3YuLQbKhuOD1B8bc', 't4tXK_VgdyN1C8RlVFImQ');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('Z1_uiSTomdWoLkShitcPG', 'PpYv_y9MKXrSIVa877PiN', 't4tXK_VgdyN1C8RlVFImQ');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('NiA9j860XzfPbTSH_AY8X', 'N80mmbKfbGTXdkH0Qw7I_', 't4tXK_VgdyN1C8RlVFImQ');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('hlQxQJrzISwUyrOhM0vao', '1aaxi_QdkHMizllO7aXjB', 't4tXK_VgdyN1C8RlVFImQ');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('CqaOcDCAykNSsDC8-f-a_', '1aaxi_QdkHMizllO7aXjB', '2qrR0ZzEEJz8yFy5UnUqo');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('4LEvffhah4vUSo-xXu_uC', 'N80mmbKfbGTXdkH0Qw7I_', 'BCiVEZ_YISmgEI1cjHifD');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('THmqDoRAtyhp3_VKlW33J', '1aaxi_QdkHMizllO7aXjB', 'BCiVEZ_YISmgEI1cjHifD');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('q7NowVBGXayUSxOUF3Jnd', 'HILlWIbqMi6deSaR6rNhd', 'XfX-R3W5Cn7Wz7fta5Vxg');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('y0qCYNRGT98ixWCorsKYs', '1aaxi_QdkHMizllO7aXjB', 'DKq0lHnGahPQLCvMBTWnO');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('LN1e7HTtlB7FQgJxcdJ9R', '1aaxi_QdkHMizllO7aXjB', 'HMBd-5guc9dw6qwe1CaUk');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('N5s6K2-Sh__1qmgoEkqZJ', '1aaxi_QdkHMizllO7aXjB', 'uW-teUPdWTHn35Tt9DHJ3');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('13GKmPLQjZlOjzACyi-JT', '1aaxi_QdkHMizllO7aXjB', 'ZmJzch57TWDw_TBZYo8u2');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('x1BahSh7xhNLOAJQEYnzZ', '1aaxi_QdkHMizllO7aXjB', 'rLXGEUaPaMah2pqe8BgYG');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('tYfVfTydul09oEIwG1y0e', '1aaxi_QdkHMizllO7aXjB', 'Dkl2ie-SjYwyN3ma-vGSd');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('AAkh1c8q7StEgkTlcVrVk', 'HILlWIbqMi6deSaR6rNhd', '5HahAMOKYo-llCP-RwZR4');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('45cQ030c9SfTD8tmQjd7L', 'HILlWIbqMi6deSaR6rNhd', 'ipoW51m5K6SD_Dfin2bke');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('7rZt9gnu6_akDa7z9Hesh', 'PpYv_y9MKXrSIVa877PiN', 'dJX4mqJ0gbmXUEBcIE2ek');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('PZ7g6Wa_sNTPK8B9W9Q6L', 'HILlWIbqMi6deSaR6rNhd', 'dJX4mqJ0gbmXUEBcIE2ek');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('JmxN7QuL9ZKlJbDPOSJli', 'HILlWIbqMi6deSaR6rNhd', 'sN3njrG2plDSyO-KGErre');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('UvB554uyr7DwVCtX2JEoF', 'PpYv_y9MKXrSIVa877PiN', '987MObqQpXlQEoi2Rp9hf');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('qDXYm_Wf8ES0s4HTsJBLC', 'HILlWIbqMi6deSaR6rNhd', '987MObqQpXlQEoi2Rp9hf');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('733wrrcawpSzjH1dTdtVb', 'HILlWIbqMi6deSaR6rNhd', '-bJTkZ0z0hRWQc2uFxYqB');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('xz6nX11Zzhe9GJEagPN8x', 'HILlWIbqMi6deSaR6rNhd', '9-kGTzoWbU3ZkGIjsDu6D');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('rh98QBj7EevFPXgv388lr', 'HILlWIbqMi6deSaR6rNhd', 'oDBSiF8TQ-xKJoOX9OX5J');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('KAjxm4HsmllFXAW_YFRZz', 'HILlWIbqMi6deSaR6rNhd', 'gDHgP0jMeH1b1ZKPjJg87');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('O_f_vkv0mROS_cxYM_Sqj', 'HILlWIbqMi6deSaR6rNhd', 'R5MQzNdePV7LxZDudXBcn');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('LIAor_1OiecKrpv8Uj_Sq', 'HILlWIbqMi6deSaR6rNhd', 'e7MUe65zxiBtYx9-CM9ms');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('GIy2jre8I8E-RWXy_wJdW', 'HILlWIbqMi6deSaR6rNhd', 'SlE6AfVubu1fihXYt23Gg');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('090l4bQMJBWLDpRl4uiwa', 'HILlWIbqMi6deSaR6rNhd', '3Lqsy1GR2WqtRXZcEYsFL');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('SuH9SnDZ_BhAQKH-RjMl9', 'HILlWIbqMi6deSaR6rNhd', '4V730VpXj_77aTZ9ChnO_');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('goWH8Z48bylV_cnyVS4mK', 'PpYv_y9MKXrSIVa877PiN', 'PQrfPCAXGBKNWgzrtkUSV');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('sR4kzXgl65ImnHeaTwDBe', 'HILlWIbqMi6deSaR6rNhd', 'PQrfPCAXGBKNWgzrtkUSV');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('zoAzkr9DCWIhpzK1GqqNj', 'HILlWIbqMi6deSaR6rNhd', 'bV6F4SQcaGBW8OC2nrA2H');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('PW3kRDI9QqfaXP5cQF09P', 'HILlWIbqMi6deSaR6rNhd', 'rBSmSqQqxdpmP2W45ElO_');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('TD2FwinTxIlGGM-B_bVin', 'PpYv_y9MKXrSIVa877PiN', 'kBIhj35iwGtK9x7WaKBlX');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('xtpap5pIGgC1H2gVBEQop', 'N80mmbKfbGTXdkH0Qw7I_', 'kBIhj35iwGtK9x7WaKBlX');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('ZtySbdHQKxv70n5O8m9g0', 'HILlWIbqMi6deSaR6rNhd', 'kBIhj35iwGtK9x7WaKBlX');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('xEO7_Ws238lfw46uTqwMt', 'PpYv_y9MKXrSIVa877PiN', 'HpDqR9p-u7MaXLH9amS5o');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('FUvRkeEmFWnGWNcSsrBVi', 'HILlWIbqMi6deSaR6rNhd', 'HpDqR9p-u7MaXLH9amS5o');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('crojJUxZU6LysrQSLQo39', 'PpYv_y9MKXrSIVa877PiN', 'Lfq4TTkY8hot7brCdnYXi');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('JlNjOk1IRWpbIiUWk3U9-', 'HILlWIbqMi6deSaR6rNhd', 'Lfq4TTkY8hot7brCdnYXi');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('pInTLC8KwA2hYXKrgnnYG', 'PpYv_y9MKXrSIVa877PiN', 'qSROcLgTbB8uqruKO2P02');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('qsltZSoPM_UBMyxtq_bOt', 'HILlWIbqMi6deSaR6rNhd', 'qSROcLgTbB8uqruKO2P02');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('x-NNxhErlY7WZdGQbSyq7', 'PpYv_y9MKXrSIVa877PiN', 'wCvNaE0GWyXeRMZXAJY-O');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('2JkO8DtxqHGBlF2SatGvW', 'HILlWIbqMi6deSaR6rNhd', 'wCvNaE0GWyXeRMZXAJY-O');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('qMcAONWRlNTwpfVZAqcsg', 'PpYv_y9MKXrSIVa877PiN', 'ifRidiHdNCNChjSOrwaHK');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('ZnBm503TU66mH8Josy-z9', 'HILlWIbqMi6deSaR6rNhd', 'ifRidiHdNCNChjSOrwaHK');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('TYDNpAsAz58ls6OkpLxb_', 'HILlWIbqMi6deSaR6rNhd', '05dtxzYQLH_XJFMINALtS');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('t4X9Jh4myOovyv0ZgM73D', 'HILlWIbqMi6deSaR6rNhd', 'k-7h0a983HN22wjzjIZFJ');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('yjXlOXI-c0HE9wB1YNHwS', 'PpYv_y9MKXrSIVa877PiN', 'XLvBCpfRGHsNPqIUCAHJM');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('KcZT0ZpcmQ1na8vFVGMLN', 'HILlWIbqMi6deSaR6rNhd', 'XLvBCpfRGHsNPqIUCAHJM');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('xVMApC0qdVhgIN1ESwHOc', 'HILlWIbqMi6deSaR6rNhd', '0Gxp-kEk4sZzEmwXJVnZo');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('9aAvOa_AsYmu0sMI8QDsX', 'HILlWIbqMi6deSaR6rNhd', '4W7DAfubDy5JIJiMZ2ru6');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('Sl0vD4k7af3lhDO-LZkvL', 'HILlWIbqMi6deSaR6rNhd', 'iRNtQBs54QY3paG2NSZb5');
INSERT INTO "issueLabel" ("id", "labelID", "issueID") VALUES ('qDZLJkVR2N8cY61uzKkZr', 'HILlWIbqMi6deSaR6rNhd', '-UwGISGEIhOk4tbWCxVlj');

COMMIT;

SELECT
    *
FROM
    pg_create_logical_replication_slot('zero_slot_r1', 'pgoutput');